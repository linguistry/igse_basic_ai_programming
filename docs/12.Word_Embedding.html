<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.57">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>12&nbsp; Word Embedding and Relational Similarity – Basic AI Programming</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./13.NLP_toolkit.html" rel="next">
<link href="./11.Text_Representation.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./12.Word_Embedding.html"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Word Embedding and Relational Similarity</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Basic AI Programming</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./01.setup.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Setting Up Environment</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./02.intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Constants and Variables in Python</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./03.control.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Control Structures in Python</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./04.function.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Functions and Packages in Python</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./05.pandas.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Data Analysis in Python</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./06.arithmetic.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Basic Arithmetic Operations in Python</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./07.Regular_Expression.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Regular Expressions (RegEx) in Python</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./08.Custom_Corpus.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Building a Custom Corpus</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./09.Preprocessing.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Text Preprocessing</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./10.Word_Cloud.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Visualizing Word Frequencies with Graphs and Word Clouds</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./11.Text_Representation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Text Representation Based on Counts</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./12.Word_Embedding.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Word Embedding and Relational Similarity</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./13.NLP_toolkit.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Word Embedding Activities</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./14.Semantic_Network.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Semantic Network Analysis</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./15.SNA_Example.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Semantic Network Analysis (Examples)</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./16.Topic_Modeling.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Topic Modeling in Python</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./17.Topic_Modeling_Example.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Topic Modeling with R (Example)</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./A.assignment1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Assignment #01</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./B.assignment2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">Assignment #02: Semantic Network Analysis</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#word-embedding" id="toc-word-embedding" class="nav-link active" data-scroll-target="#word-embedding"><span class="header-section-number">12.1</span> Word Embedding</a>
  <ul class="collapse">
  <li><a href="#overview" id="toc-overview" class="nav-link" data-scroll-target="#overview"><span class="header-section-number">12.1.1</span> Overview</a></li>
  <li><a href="#what-is-word2vec" id="toc-what-is-word2vec" class="nav-link" data-scroll-target="#what-is-word2vec"><span class="header-section-number">12.1.2</span> What is Word2Vec?</a></li>
  <li><a href="#example-in-python-using-gensim-word2vec" id="toc-example-in-python-using-gensim-word2vec" class="nav-link" data-scroll-target="#example-in-python-using-gensim-word2vec"><span class="header-section-number">12.1.3</span> Example in Python (Using Gensim Word2Vec)</a></li>
  </ul></li>
  <li><a href="#step-by-step-example-using-gensims-word2vec" id="toc-step-by-step-example-using-gensims-word2vec" class="nav-link" data-scroll-target="#step-by-step-example-using-gensims-word2vec"><span class="header-section-number">12.2</span> Step-by-Step Example Using Gensim’s Word2Vec</a>
  <ul class="collapse">
  <li><a href="#step-1-installing-gensim" id="toc-step-1-installing-gensim" class="nav-link" data-scroll-target="#step-1-installing-gensim"><span class="header-section-number">12.2.1</span> Step 1: Installing Gensim</a></li>
  <li><a href="#step-2-import-required-libraries" id="toc-step-2-import-required-libraries" class="nav-link" data-scroll-target="#step-2-import-required-libraries"><span class="header-section-number">12.2.2</span> Step 2: Import Required Libraries</a></li>
  <li><a href="#step-3-preparing-the-data" id="toc-step-3-preparing-the-data" class="nav-link" data-scroll-target="#step-3-preparing-the-data"><span class="header-section-number">12.2.3</span> Step 3: Preparing the Data</a></li>
  <li><a href="#step-4-training-the-word2vec-model" id="toc-step-4-training-the-word2vec-model" class="nav-link" data-scroll-target="#step-4-training-the-word2vec-model"><span class="header-section-number">12.2.4</span> Step 4: Training the Word2Vec Model</a></li>
  <li><a href="#step-5-exploring-the-word-embeddings" id="toc-step-5-exploring-the-word-embeddings" class="nav-link" data-scroll-target="#step-5-exploring-the-word-embeddings"><span class="header-section-number">12.2.5</span> Step 5: Exploring the Word Embeddings</a></li>
  <li><a href="#step-6-visualizing-word-embeddings-optional" id="toc-step-6-visualizing-word-embeddings-optional" class="nav-link" data-scroll-target="#step-6-visualizing-word-embeddings-optional"><span class="header-section-number">12.2.6</span> Step 6: Visualizing Word Embeddings (Optional)</a></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion"><span class="header-section-number">12.2.7</span> Conclusion</a></li>
  </ul></li>
  <li><a href="#vector-similarity" id="toc-vector-similarity" class="nav-link" data-scroll-target="#vector-similarity"><span class="header-section-number">12.3</span> Vector Similarity</a>
  <ul class="collapse">
  <li><a href="#example-in-python-cosine-similarity" id="toc-example-in-python-cosine-similarity" class="nav-link" data-scroll-target="#example-in-python-cosine-similarity"><span class="header-section-number">12.3.1</span> Example in Python (Cosine Similarity)</a></li>
  </ul></li>
  <li><a href="#using-pre-trained-word-vectors-using-spacy" id="toc-using-pre-trained-word-vectors-using-spacy" class="nav-link" data-scroll-target="#using-pre-trained-word-vectors-using-spacy"><span class="header-section-number">12.4</span> Using Pre-trained Word Vectors (using spaCy)</a>
  <ul class="collapse">
  <li><a href="#install-spacy-and-download-language-model" id="toc-install-spacy-and-download-language-model" class="nav-link" data-scroll-target="#install-spacy-and-download-language-model"><span class="header-section-number">12.4.1</span> Install spaCy and Download Language Model</a></li>
  <li><a href="#example-code-document-similarity-using-spacy" id="toc-example-code-document-similarity-using-spacy" class="nav-link" data-scroll-target="#example-code-document-similarity-using-spacy"><span class="header-section-number">12.4.2</span> Example Code: Document Similarity Using spaCy</a></li>
  <li><a href="#explanation" id="toc-explanation" class="nav-link" data-scroll-target="#explanation"><span class="header-section-number">12.4.3</span> Explanation:</a></li>
  <li><a href="#output" id="toc-output" class="nav-link" data-scroll-target="#output"><span class="header-section-number">12.4.4</span> Output:</a></li>
  <li><a href="#notes" id="toc-notes" class="nav-link" data-scroll-target="#notes"><span class="header-section-number">12.4.5</span> Notes:</a></li>
  <li><a href="#summary" id="toc-summary" class="nav-link" data-scroll-target="#summary"><span class="header-section-number">12.4.6</span> Summary:</a></li>
  </ul></li>
  <li><a href="#spacys-vs.-gensims-word2vec" id="toc-spacys-vs.-gensims-word2vec" class="nav-link" data-scroll-target="#spacys-vs.-gensims-word2vec"><span class="header-section-number">12.5</span> spaCy’s vs.&nbsp;Gensim’s Word2Vec</a></li>
  <li><a href="#relational-similarity" id="toc-relational-similarity" class="nav-link" data-scroll-target="#relational-similarity"><span class="header-section-number">12.6</span> Relational Similarity</a>
  <ul class="collapse">
  <li><a href="#using-gensim-word2vec" id="toc-using-gensim-word2vec" class="nav-link" data-scroll-target="#using-gensim-word2vec"><span class="header-section-number">12.6.1</span> Using Gensim Word2Vec</a></li>
  <li><a href="#using-spacy" id="toc-using-spacy" class="nav-link" data-scroll-target="#using-spacy"><span class="header-section-number">12.6.2</span> Using spaCy</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Word Embedding and Relational Similarity</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<section id="word-embedding" class="level2" data-number="12.1">
<h2 data-number="12.1" class="anchored" data-anchor-id="word-embedding"><span class="header-section-number">12.1</span> Word Embedding</h2>
<section id="overview" class="level3" data-number="12.1.1">
<h3 data-number="12.1.1" class="anchored" data-anchor-id="overview"><span class="header-section-number">12.1.1</span> Overview</h3>
<p><strong>Word Embedding</strong> refers to a technique where words are mapped to vectors of real numbers, typically in a lower-dimensional space. These vectors capture the semantic meaning of words. Unlike TF-IDF, embeddings capture semantic similarity—words with similar meanings will have closer vectors.</p>
<p>Popular models for word embeddings include:</p>
<ul>
<li><p><strong>Word2Vec</strong>: Predicts context words given a target word (CBOW) or predicts the target word given the context (Skip-gram).</p></li>
<li><p><strong>GloVe</strong>: Focuses on matrix factorization, capturing global statistical information about word co-occurrences.</p></li>
<li><p><strong>FastText</strong>: Extends Word2Vec by considering subword information, which helps model morphology.</p></li>
</ul>
</section>
<section id="what-is-word2vec" class="level3" data-number="12.1.2">
<h3 data-number="12.1.2" class="anchored" data-anchor-id="what-is-word2vec"><span class="header-section-number">12.1.2</span> What is Word2Vec?</h3>
<p>Word2Vec is a model used to learn the relationships between words based on their co-occurrence in a corpus. It creates a dense vector representation of words such that words with similar meanings are closer in the vector space.</p>
<p>There are two main architectures for Word2Vec:</p>
<ol type="1">
<li><strong>CBOW (Continuous Bag of Words)</strong>: Predict the current word from surrounding words.</li>
<li><strong>Skip-gram</strong>: Predict surrounding words given the current word.</li>
</ol>
<p><img src="images/clipboard-3597099819.png" class="img-fluid" width="492"></p>
</section>
<section id="example-in-python-using-gensim-word2vec" class="level3" data-number="12.1.3">
<h3 data-number="12.1.3" class="anchored" data-anchor-id="example-in-python-using-gensim-word2vec"><span class="header-section-number">12.1.3</span> Example in Python (Using Gensim Word2Vec)</h3>
<div class="sourceCode" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> gensim</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> gensim.models <span class="im">import</span> Word2Vec</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Sample sentences</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>sentences <span class="op">=</span> [</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>    [<span class="st">'natural'</span>, <span class="st">'language'</span>, <span class="st">'processing'</span>, <span class="st">'is'</span>, <span class="st">'fascinating'</span>],</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>    [<span class="st">'word'</span>, <span class="st">'embedding'</span>, <span class="st">'and'</span>, <span class="st">'tf-idf'</span>, <span class="st">'are'</span>, <span class="st">'techniques'</span>],</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>    [<span class="st">'text'</span>, <span class="st">'similarity'</span>, <span class="st">'can'</span>, <span class="st">'be'</span>, <span class="st">'measured'</span>, <span class="st">'using'</span>, <span class="st">'vector'</span>, <span class="st">'similarity'</span>]</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Train Word2Vec model</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> Word2Vec(sentences, vector_size<span class="op">=</span><span class="dv">100</span>, window<span class="op">=</span><span class="dv">5</span>, min_count<span class="op">=</span><span class="dv">1</span>, workers<span class="op">=</span><span class="dv">4</span>)</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Check vector for a word</span></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>word_vector <span class="op">=</span> model.wv[<span class="st">'similarity'</span>]</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Vector for 'similarity':</span><span class="ch">\n</span><span class="sc">{</span>word_vector<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Find similar words to 'text'</span></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>similar_words <span class="op">=</span> model.wv.most_similar(<span class="st">'text'</span>)</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Words similar to 'text': </span><span class="sc">{</span>similar_words<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><img src="images/clipboard-82208775.png" class="img-fluid"></p>
<p>In this example, a simple Word2Vec model is trained on a small dataset. After training, we can retrieve word vectors and find similar words based on vector proximity.</p>
</section>
</section>
<section id="step-by-step-example-using-gensims-word2vec" class="level2" data-number="12.2">
<h2 data-number="12.2" class="anchored" data-anchor-id="step-by-step-example-using-gensims-word2vec"><span class="header-section-number">12.2</span> Step-by-Step Example Using Gensim’s Word2Vec</h2>
<section id="step-1-installing-gensim" class="level3" data-number="12.2.1">
<h3 data-number="12.2.1" class="anchored" data-anchor-id="step-1-installing-gensim"><span class="header-section-number">12.2.1</span> Step 1: Installing Gensim</h3>
<p>To use Word2Vec in Python, you will need to install Gensim. You can install it using pip:</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="ex">pip</span> install gensim</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="step-2-import-required-libraries" class="level3" data-number="12.2.2">
<h3 data-number="12.2.2" class="anchored" data-anchor-id="step-2-import-required-libraries"><span class="header-section-number">12.2.2</span> Step 2: Import Required Libraries</h3>
<p>We will import the necessary libraries for training the Word2Vec model.</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> gensim</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> gensim.models <span class="im">import</span> Word2Vec</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> nltk</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> nltk.corpus <span class="im">import</span> brown</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>nltk.download(<span class="st">'brown'</span>)  <span class="co"># We will use the Brown corpus for this example</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="step-3-preparing-the-data" class="level3" data-number="12.2.3">
<h3 data-number="12.2.3" class="anchored" data-anchor-id="step-3-preparing-the-data"><span class="header-section-number">12.2.3</span> Step 3: Preparing the Data</h3>
<p>We will use the Brown corpus provided by NLTK, which contains a variety of text genres. First, let’s tokenize the sentences into words.</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Load the Brown corpus sentences</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>sentences <span class="op">=</span> brown.sents()</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Print a sample sentence</span></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(sentences[<span class="dv">0</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="step-4-training-the-word2vec-model" class="level3" data-number="12.2.4">
<h3 data-number="12.2.4" class="anchored" data-anchor-id="step-4-training-the-word2vec-model"><span class="header-section-number">12.2.4</span> Step 4: Training the Word2Vec Model</h3>
<p>Now that we have the tokenized sentences, we can train the Word2Vec model using Gensim. The important parameters are:</p>
<ul>
<li><code>sentences</code>: The training data (list of tokenized sentences).</li>
<li><code>vector_size</code>: The dimensionality of the word vectors.</li>
<li><code>window</code>: The maximum distance between the current and predicted word.</li>
<li><code>min_count</code>: Ignores all words with a total frequency lower than this.</li>
</ul>
<div class="sourceCode" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Train the Word2Vec model</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> Word2Vec(sentences, vector_size<span class="op">=</span><span class="dv">100</span>, window<span class="op">=</span><span class="dv">5</span>, min_count<span class="op">=</span><span class="dv">5</span>, workers<span class="op">=</span><span class="dv">4</span>)</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Save the model</span></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>model.save(<span class="st">"word2vec_brown.model"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="step-5-exploring-the-word-embeddings" class="level3" data-number="12.2.5">
<h3 data-number="12.2.5" class="anchored" data-anchor-id="step-5-exploring-the-word-embeddings"><span class="header-section-number">12.2.5</span> Step 5: Exploring the Word Embeddings</h3>
<p>Once the model is trained, you can explore the word embeddings. You can find words similar to a given word, check the cosine similarity between words, and more.</p>
<section id="finding-the-most-similar-words" class="level4" data-number="12.2.5.1">
<h4 data-number="12.2.5.1" class="anchored" data-anchor-id="finding-the-most-similar-words"><span class="header-section-number">12.2.5.1</span> Finding the Most Similar Words</h4>
<div class="sourceCode" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Load the trained model</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> Word2Vec.load(<span class="st">"word2vec_brown.model"</span>)</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Find words most similar to 'king'</span></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>similar_words <span class="op">=</span> model.wv.most_similar(<span class="st">'king'</span>)</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(similar_words)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="word-vector-representation" class="level4" data-number="12.2.5.2">
<h4 data-number="12.2.5.2" class="anchored" data-anchor-id="word-vector-representation"><span class="header-section-number">12.2.5.2</span> Word Vector Representation</h4>
<p>Each word is represented by a vector. You can see the vector representation of a specific word.</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Get the vector for a specific word</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>vector <span class="op">=</span> model.wv[<span class="st">'king'</span>]</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(vector)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="cosine-similarity-between-words" class="level4" data-number="12.2.5.3">
<h4 data-number="12.2.5.3" class="anchored" data-anchor-id="cosine-similarity-between-words"><span class="header-section-number">12.2.5.3</span> Cosine Similarity Between Words</h4>
<p>Cosine similarity is commonly used to measure the similarity between word vectors.</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate cosine similarity between two words</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>similarity <span class="op">=</span> model.wv.similarity(<span class="st">'king'</span>, <span class="st">'queen'</span>)</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Cosine similarity between 'king' and 'queen': </span><span class="sc">{</span>similarity<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
</section>
<section id="step-6-visualizing-word-embeddings-optional" class="level3" data-number="12.2.6">
<h3 data-number="12.2.6" class="anchored" data-anchor-id="step-6-visualizing-word-embeddings-optional"><span class="header-section-number">12.2.6</span> Step 6: Visualizing Word Embeddings (Optional)</h3>
<p>To visualize word embeddings in a 2D space, we can use dimensionality reduction techniques like PCA (Principal Component Analysis) or t-SNE. Below is an example using PCA.</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.decomposition <span class="im">import</span> PCA</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Select a few words to visualize</span></span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>words <span class="op">=</span> [<span class="st">'king'</span>, <span class="st">'queen'</span>, <span class="st">'man'</span>, <span class="st">'woman'</span>, <span class="st">'dog'</span>, <span class="st">'cat'</span>]</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>word_vectors <span class="op">=</span> [model.wv[word] <span class="cf">for</span> word <span class="kw">in</span> words]</span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Reduce the dimensionality of word vectors using PCA</span></span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a>pca <span class="op">=</span> PCA(n_components<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a>word_vectors_2d <span class="op">=</span> pca.fit_transform(word_vectors)</span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the 2D word vectors</span></span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">5</span>))</span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, word <span class="kw">in</span> <span class="bu">enumerate</span>(words):</span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a>    plt.scatter(word_vectors_2d[i, <span class="dv">0</span>], word_vectors_2d[i, <span class="dv">1</span>])</span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a>    plt.text(word_vectors_2d[i, <span class="dv">0</span>] <span class="op">+</span> <span class="fl">0.05</span>, word_vectors_2d[i, <span class="dv">1</span>] <span class="op">+</span> <span class="fl">0.05</span>, word)</span>
<span id="cb9-17"><a href="#cb9-17" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><strong>Output:</strong></p>
<p><img src="images/clipboard-3044504688.png" class="img-fluid"></p>
</section>
<section id="conclusion" class="level3" data-number="12.2.7">
<h3 data-number="12.2.7" class="anchored" data-anchor-id="conclusion"><span class="header-section-number">12.2.7</span> Conclusion</h3>
<p>In this tutorial, we explored the Word2Vec model using Gensim in Python. We trained a Word2Vec model using the Brown corpus, explored word embeddings, and visualized the vectors in a 2D space. Word embeddings are a powerful tool for capturing the semantic relationships between words and are widely used in natural language processing tasks.</p>
</section>
</section>
<section id="vector-similarity" class="level2" data-number="12.3">
<h2 data-number="12.3" class="anchored" data-anchor-id="vector-similarity"><span class="header-section-number">12.3</span> Vector Similarity</h2>
<p>Once text data is transformed into vectors (using methods like TF-IDF or word embeddings), we can compute similarity between documents or words. A common method for measuring similarity is <strong>Cosine Similarity</strong>, which measures the cosine of the angle between two vectors.</p>
<p><span class="math display">\[\text{cosine similarity}(A, B) = \frac{A \cdot B}{\|A\| \|B\|} \]</span></p>
<p>Where:</p>
<ul>
<li><p><span class="math inline">\(A \cdot B\)</span> is the dot product of the vectors.</p></li>
<li><p><span class="math inline">\(\|A\|\)</span> and <span class="math inline">\(\|B\|\)</span> are the magnitudes (norms) of vectors (A) and (B).</p></li>
</ul>
<section id="example-in-python-cosine-similarity" class="level3" data-number="12.3.1">
<h3 data-number="12.3.1" class="anchored" data-anchor-id="example-in-python-cosine-similarity"><span class="header-section-number">12.3.1</span> Example in Python (Cosine Similarity)</h3>
<div class="sourceCode" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics.pairwise <span class="im">import</span> cosine_similarity</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Example vectors (from TF-IDF or word embedding)</span></span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>vector_a <span class="op">=</span> np.array([<span class="fl">0.1</span>, <span class="fl">0.3</span>, <span class="fl">0.7</span>])</span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>vector_b <span class="op">=</span> np.array([<span class="fl">0.2</span>, <span class="fl">0.4</span>, <span class="fl">0.5</span>])</span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute cosine similarity</span></span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a>similarity <span class="op">=</span> cosine_similarity([vector_a], [vector_b])</span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Cosine Similarity between vector_a and vector_b: </span><span class="sc">{</span>similarity[<span class="dv">0</span>][<span class="dv">0</span>]<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><strong>Output:</strong></p>
<pre><code>Cosine Similarity between vector_a and vector_b: 0.9509634325746505</code></pre>
</section>
</section>
<section id="using-pre-trained-word-vectors-using-spacy" class="level2" data-number="12.4">
<h2 data-number="12.4" class="anchored" data-anchor-id="using-pre-trained-word-vectors-using-spacy"><span class="header-section-number">12.4</span> Using Pre-trained Word Vectors (using spaCy)</h2>
<p>Below is an example of how to compute document similarity using spaCy, which is a powerful NLP library. This code uses spaCy’s pre-trained word vectors to compute similarity between documents.</p>
<section id="install-spacy-and-download-language-model" class="level3" data-number="12.4.1">
<h3 data-number="12.4.1" class="anchored" data-anchor-id="install-spacy-and-download-language-model"><span class="header-section-number">12.4.1</span> Install spaCy and Download Language Model</h3>
<p>If you haven’t already installed spaCy and its language model, you can do so using the following commands:</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="ex">pip</span> install spacy</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a><span class="ex">python</span> <span class="at">-m</span> spacy download en_core_web_md  <span class="co"># Medium-sized English model with word vectors</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="example-code-document-similarity-using-spacy" class="level3" data-number="12.4.2">
<h3 data-number="12.4.2" class="anchored" data-anchor-id="example-code-document-similarity-using-spacy"><span class="header-section-number">12.4.2</span> Example Code: Document Similarity Using spaCy</h3>
<p>Here is a Python script that demonstrates how to compute similarity between two documents using spaCy:</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> spacy</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Load the spaCy language model (with word vectors)</span></span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>nlp <span class="op">=</span> spacy.load(<span class="st">'en_core_web_md'</span>)</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Define two sample documents</span></span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>doc1 <span class="op">=</span> nlp(<span class="st">"Natural language processing is a fascinating field of AI that focuses on the interaction between humans and computers through language."</span>)</span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>doc2 <span class="op">=</span> nlp(<span class="st">"Machine learning is a part of AI that allows computers to learn from data without being explicitly programmed."</span>)</span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute similarity between the two documents</span></span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a>similarity <span class="op">=</span> doc1.similarity(doc2)</span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Similarity between doc1 and doc2: </span><span class="sc">{</span>similarity<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-15"><a href="#cb13-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Compare additional documents for similarity</span></span>
<span id="cb13-16"><a href="#cb13-16" aria-hidden="true" tabindex="-1"></a>doc3 <span class="op">=</span> nlp(<span class="st">"Understanding how humans speak is the first step in improving human-computer interaction."</span>)</span>
<span id="cb13-17"><a href="#cb13-17" aria-hidden="true" tabindex="-1"></a>similarity_with_doc3 <span class="op">=</span> doc1.similarity(doc3)</span>
<span id="cb13-18"><a href="#cb13-18" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Similarity between doc1 and doc3: </span><span class="sc">{</span>similarity_with_doc3<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb13-19"><a href="#cb13-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-20"><a href="#cb13-20" aria-hidden="true" tabindex="-1"></a>doc4 <span class="op">=</span> nlp(<span class="st">"I love hiking in the mountains."</span>)</span>
<span id="cb13-21"><a href="#cb13-21" aria-hidden="true" tabindex="-1"></a>similarity_with_doc4 <span class="op">=</span> doc1.similarity(doc4)</span>
<span id="cb13-22"><a href="#cb13-22" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Similarity between doc1 and doc4: </span><span class="sc">{</span>similarity_with_doc4<span class="sc">:.4f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="explanation" class="level3" data-number="12.4.3">
<h3 data-number="12.4.3" class="anchored" data-anchor-id="explanation"><span class="header-section-number">12.4.3</span> Explanation:</h3>
<ol type="1">
<li><strong>Loading the Model</strong>: We load the medium-sized <code>en_core_web_md</code> model in spaCy, which contains word vectors necessary for computing similarity.</li>
<li><strong>Creating Documents</strong>: We create document objects (<code>doc1</code>, <code>doc2</code>, etc.) by passing strings to the <code>nlp</code> object, which processes them.</li>
<li><strong>Computing Similarity</strong>: The <code>doc1.similarity(doc2)</code> method computes the similarity between two documents using word vectors. This similarity score is a number between 0 and 1, where 1 indicates perfect similarity.</li>
<li><strong>Additional Comparisons</strong>: We also compare <code>doc1</code> with <code>doc3</code> and <code>doc4</code> to see how similar different texts are.</li>
</ol>
</section>
<section id="output" class="level3" data-number="12.4.4">
<h3 data-number="12.4.4" class="anchored" data-anchor-id="output"><span class="header-section-number">12.4.4</span> Output:</h3>
<p>You will get an output that shows similarity scores for each pair of documents. A higher similarity score means the documents are more closely related in terms of their content.</p>
<pre class="plaintext"><code>Similarity between doc1 and doc2: 0.9172
Similarity between doc1 and doc3: 0.9096
Similarity between doc1 and doc4: 0.7760</code></pre>
</section>
<section id="notes" class="level3" data-number="12.4.5">
<h3 data-number="12.4.5" class="anchored" data-anchor-id="notes"><span class="header-section-number">12.4.5</span> Notes:</h3>
<ul>
<li><strong>Similarity Score</strong>: This is computed based on the semantic content of the text using pre-trained word vectors. It considers the meaning of words, so two documents that use different words but are semantically similar should have a high similarity score.</li>
<li><strong>Word Vector Model</strong>: You could also use a larger or smaller language model depending on your needs (<code>en_core_web_sm</code> for a small model, or <code>en_core_web_lg</code> for a large model).</li>
</ul>
</section>
<section id="summary" class="level3" data-number="12.4.6">
<h3 data-number="12.4.6" class="anchored" data-anchor-id="summary"><span class="header-section-number">12.4.6</span> Summary:</h3>
<p>This code computes the similarity between different documents using spaCy’s pre-trained word vectors. You can use this method to compare documents in various NLP tasks, such as document clustering, summarization, or recommendation systems.</p>
</section>
</section>
<section id="spacys-vs.-gensims-word2vec" class="level2" data-number="12.5">
<h2 data-number="12.5" class="anchored" data-anchor-id="spacys-vs.-gensims-word2vec"><span class="header-section-number">12.5</span> spaCy’s vs.&nbsp;Gensim’s Word2Vec</h2>
<section id="pre-trained-vs-custom-training" class="level4" data-number="12.5.0.1">
<h4 data-number="12.5.0.1" class="anchored" data-anchor-id="pre-trained-vs-custom-training"><span class="header-section-number">12.5.0.1</span> <strong>Pre-trained vs Custom Training</strong></h4>
<ul>
<li><strong>spaCy’s Pre-trained Word Vectors</strong>:
<ul>
<li><strong>Pre-trained</strong>: spaCy comes with pre-trained word vectors as part of its language models, such as <code>en_core_web_md</code> and <code>en_core_web_lg</code>. These vectors have already been trained on large datasets (like Common Crawl) and are ready to use out of the box.</li>
<li><strong>Cannot Retrain Easily</strong>: spaCy is designed primarily for loading and using pre-trained models. While it is possible to integrate custom vectors into spaCy, <strong>it’s not built for training word vectors from scratch.</strong></li>
</ul></li>
<li><strong>Gensim Word2Vec</strong>:
<ul>
<li><strong>Customizable Training</strong>: Gensim’s Word2Vec is a flexible tool that <strong>allows you to train word vectors on your own corpus</strong>. You can adjust the training parameters (e.g., <code>window</code>, <code>vector_size</code>, etc.) to suit your specific needs.</li>
<li><strong>No Pre-trained Models</strong>: Gensim provides the implementation of Word2Vec, but you need to either train the model on your data or use pre-trained vectors from external sources like Google News.</li>
</ul></li>
</ul>
</section>
<section id="training-algorithm" class="level4" data-number="12.5.0.2">
<h4 data-number="12.5.0.2" class="anchored" data-anchor-id="training-algorithm"><span class="header-section-number">12.5.0.2</span> <strong>Training Algorithm</strong></h4>
<ul>
<li><strong>spaCy</strong>:
<ul>
<li>SpaCy uses <strong>pre-trained models</strong> that are typically trained using <strong>Word2Vec-like</strong> or <strong>FastText-like</strong> algorithms, but the specifics may vary depending on the source of the vectors.</li>
<li>SpaCy’s pre-trained models also include other components such as dependency parsers, named entity recognizers, etc., in addition to word vectors.</li>
</ul></li>
<li><strong>Gensim Word2Vec</strong>:
<ul>
<li>Gensim implements the <strong>original Word2Vec algorithm</strong> as described by Mikolov et al.&nbsp;It offers two main algorithms:
<ul>
<li><strong>Skip-gram</strong>: Predicts surrounding words given a target word.</li>
<li><strong>CBOW (Continuous Bag of Words)</strong>: Predicts a target word from surrounding words.</li>
</ul></li>
<li>It is a more specific tool aimed at training word vectors.</li>
</ul></li>
</ul>
</section>
<section id="vector-quality-and-size" class="level4" data-number="12.5.0.3">
<h4 data-number="12.5.0.3" class="anchored" data-anchor-id="vector-quality-and-size"><span class="header-section-number">12.5.0.3</span> <strong>Vector Quality and Size</strong></h4>
<ul>
<li><strong>spaCy</strong>:
<ul>
<li>SpaCy offers models like <code>en_core_web_sm</code>, <code>en_core_web_md</code>, and <code>en_core_web_lg</code>:
<ul>
<li><strong>sm</strong> (small model): Does not contain word vectors and only includes word embeddings based on context.</li>
<li><strong>md</strong> (medium model): Includes word vectors, but they are smaller and less fine-grained (300 dimensions).</li>
<li><strong>lg</strong> (large model): Contains more detailed word vectors and a larger vocabulary (300 dimensions).</li>
</ul></li>
<li>The quality of vectors depends on the size of the model you load.</li>
</ul></li>
<li><strong>Gensim Word2Vec</strong>:
<ul>
<li>Gensim allows you to specify the <strong>vector size</strong>, the <strong>window size</strong>, the <strong>min_count</strong> (to ignore infrequent words), and other hyperparameters during training. This means you can control the trade-off between model complexity and the quality of word embeddings.</li>
<li>Gensim also allows for training on <strong>domain-specific corpora</strong>, which can give higher-quality word embeddings for specific use cases (e.g., medical, legal text).</li>
</ul></li>
</ul>
</section>
<section id="use-case-differences" class="level4" data-number="12.5.0.4">
<h4 data-number="12.5.0.4" class="anchored" data-anchor-id="use-case-differences"><span class="header-section-number">12.5.0.4</span> <strong>Use Case Differences</strong></h4>
<ul>
<li><strong>spaCy</strong>:
<ul>
<li><strong>Best for Pre-trained NLP Pipelines</strong>: SpaCy is primarily used for building NLP pipelines and includes tools for tokenization, part-of-speech tagging, dependency parsing, named entity recognition, and more. Word vectors are just one part of spaCy’s broader functionality.</li>
<li><strong>Out-of-the-Box Similarity</strong>: If you need to compute similarities between documents or words quickly without training your own model, spaCy’s pre-trained vectors are an excellent choice.</li>
</ul></li>
<li><strong>Gensim Word2Vec</strong>:
<ul>
<li><strong>Best for Custom Training on Specific Domains</strong>: If you have a large corpus specific to your domain (e.g., legal, medical, or technical text) and you want to train your own word embeddings, Gensim’s Word2Vec is the tool to use.</li>
<li><strong>Flexible and Customizable</strong>: Gensim is ideal when you want to experiment with different training parameters or create embeddings for specialized applications.</li>
</ul></li>
</ul>
</section>
<section id="performance-and-speed" class="level4" data-number="12.5.0.5">
<h4 data-number="12.5.0.5" class="anchored" data-anchor-id="performance-and-speed"><span class="header-section-number">12.5.0.5</span> <strong>Performance and Speed</strong></h4>
<ul>
<li><strong>spaCy</strong>:
<ul>
<li>SpaCy is optimized for high performance. When using pre-trained vectors, it’s designed to be fast, especially for inference tasks like similarity comparisons.</li>
</ul></li>
<li><strong>Gensim Word2Vec</strong>:
<ul>
<li>Gensim is optimized for large-scale training and can work with very large corpora efficiently. However, training from scratch on large datasets can take some time depending on the size of your data and chosen parameters.</li>
</ul></li>
</ul>
</section>
<section id="additional-features" class="level4" data-number="12.5.0.6">
<h4 data-number="12.5.0.6" class="anchored" data-anchor-id="additional-features"><span class="header-section-number">12.5.0.6</span> <strong>Additional Features</strong></h4>
<ul>
<li><strong>spaCy</strong>:
<ul>
<li>SpaCy’s models are multi-functional, including components for parsing, tagging, named entity recognition, etc. Word vectors are just one feature among many.</li>
<li>SpaCy focuses on providing an end-to-end pipeline for various NLP tasks, making it a comprehensive tool for many tasks, but less specialized in word vector training.</li>
</ul></li>
<li><strong>Gensim Word2Vec</strong>:
<ul>
<li>Gensim is a specialized tool for generating word embeddings and working with them. While it does not provide the full NLP pipeline like spaCy, it excels in generating custom word embeddings and performing tasks like topic modeling, document similarity, and more.</li>
</ul></li>
</ul>
<p>In summary, spaCy’s pre-trained word vectors are great for general-purpose applications and fast deployment, while Gensim’s Word2Vec is better for training custom embeddings on specialized data.</p>
</section>
</section>
<section id="relational-similarity" class="level2" data-number="12.6">
<h2 data-number="12.6" class="anchored" data-anchor-id="relational-similarity"><span class="header-section-number">12.6</span> Relational Similarity</h2>
<p>The famous example (see <a href="https://arxiv.org/abs/1509.01692" class="uri">https://arxiv.org/abs/1509.01692</a>) of “King” - “man” + “woman” = “Queen” demonstrates how word embeddings can capture semantic relationships between words. This is often referred to as vector arithmetic or analogy in word embeddings.</p>
<p>You can use both <strong>Gensim’s Word2Vec</strong> and <strong>spaCy</strong> to demonstrate this kind of analogy. Below is an example using both libraries:</p>
<section id="using-gensim-word2vec" class="level3" data-number="12.6.1">
<h3 data-number="12.6.1" class="anchored" data-anchor-id="using-gensim-word2vec"><span class="header-section-number">12.6.1</span> Using Gensim Word2Vec</h3>
<p>If you already have a pre-trained Word2Vec model (like Google News vectors or one trained on your own corpus), you can use Gensim to perform the analogy.</p>
<div class="sourceCode" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> gensim.downloader <span class="im">as</span> api</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Load pre-trained Word2Vec model (Google News vectors)</span></span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a><span class="co"># This might take a few minutes to download (around 1.6GB)</span></span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> api.load(<span class="st">"word2vec-google-news-300"</span>)</span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Perform the vector arithmetic: King - Man + Woman = ?</span></span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a>result <span class="op">=</span> model.most_similar(positive<span class="op">=</span>[<span class="st">'king'</span>, <span class="st">'woman'</span>], negative<span class="op">=</span>[<span class="st">'man'</span>])</span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Result: </span><span class="sc">{</span>result[<span class="dv">0</span>]<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><strong>Explanation:</strong></p>
<ul>
<li><p><strong>Positive Words</strong>: Words that should be added (<code>king</code> and <code>woman</code>).</p></li>
<li><p><strong>Negative Words</strong>: Words that should be subtracted (<code>man</code>).</p></li>
<li><p><strong>Result</strong>: This will return the top word that matches the resulting vector after the arithmetic operation.</p></li>
</ul>
<p><strong>Example Output:</strong></p>
<pre class="plaintext"><code>Result: ('queen', 0.7118192911148071)</code></pre>
<p><img src="images/clipboard-356955461.png" class="img-fluid"></p>
<p>This indicates that the word closest to the result of the vector arithmetic “King” - “man” + “woman” is “Queen” with a similarity score of <code>0.71</code>.</p>
</section>
<section id="using-spacy" class="level3" data-number="12.6.2">
<h3 data-number="12.6.2" class="anchored" data-anchor-id="using-spacy"><span class="header-section-number">12.6.2</span> Using spaCy</h3>
<p>If you are using spaCy with a pre-trained model, such as <code>en_core_web_md</code> or <code>en_core_web_lg</code>, you can perform similar vector arithmetic.</p>
<div class="sourceCode" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> spacy</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Load spaCy medium or large model (that includes word vectors)</span></span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a><span class="co">#nlp = spacy.load("en_core_web_md")</span></span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a>nlp <span class="op">=</span> spacy.load(<span class="st">"en_core_web_lg"</span>)</span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Get vectors for words</span></span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a>king <span class="op">=</span> nlp.vocab[<span class="st">'king'</span>].vector</span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a>man <span class="op">=</span> nlp.vocab[<span class="st">'man'</span>].vector</span>
<span id="cb17-10"><a href="#cb17-10" aria-hidden="true" tabindex="-1"></a>woman <span class="op">=</span> nlp.vocab[<span class="st">'woman'</span>].vector</span>
<span id="cb17-11"><a href="#cb17-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-12"><a href="#cb17-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Perform vector arithmetic: King - Man + Woman</span></span>
<span id="cb17-13"><a href="#cb17-13" aria-hidden="true" tabindex="-1"></a>result_vector <span class="op">=</span> king <span class="op">-</span> man <span class="op">+</span> woman</span>
<span id="cb17-14"><a href="#cb17-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-15"><a href="#cb17-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Find the word closest to the result vector</span></span>
<span id="cb17-16"><a href="#cb17-16" aria-hidden="true" tabindex="-1"></a>similar_word <span class="op">=</span> <span class="va">None</span></span>
<span id="cb17-17"><a href="#cb17-17" aria-hidden="true" tabindex="-1"></a>max_similarity <span class="op">=</span> <span class="op">-</span><span class="dv">1</span></span>
<span id="cb17-18"><a href="#cb17-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-19"><a href="#cb17-19" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> word <span class="kw">in</span> nlp.vocab:</span>
<span id="cb17-20"><a href="#cb17-20" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> word.has_vector <span class="kw">and</span> word.is_lower <span class="kw">and</span> word.is_alpha:</span>
<span id="cb17-21"><a href="#cb17-21" aria-hidden="true" tabindex="-1"></a>        similarity <span class="op">=</span> result_vector.dot(word.vector)</span>
<span id="cb17-22"><a href="#cb17-22" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> similarity <span class="op">&gt;</span> max_similarity:</span>
<span id="cb17-23"><a href="#cb17-23" aria-hidden="true" tabindex="-1"></a>            max_similarity <span class="op">=</span> similarity</span>
<span id="cb17-24"><a href="#cb17-24" aria-hidden="true" tabindex="-1"></a>            similar_word <span class="op">=</span> word</span>
<span id="cb17-25"><a href="#cb17-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-26"><a href="#cb17-26" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Result: </span><span class="sc">{</span>similar_word<span class="sc">.</span>text<span class="sc">}</span><span class="ss">, Similarity: </span><span class="sc">{</span>max_similarity<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><strong>Explanation:</strong></p>
<ul>
<li><p><strong>Vector Operations</strong>: We manually subtract and add the word vectors for <code>king</code>, <code>man</code>, and <code>woman</code>.</p></li>
<li><p><strong>Finding Similar Words</strong>: We compare the resulting vector to all other word vectors in the model’s vocabulary using a dot product to find the most similar word.</p></li>
<li><p><strong>Performance</strong>: This might take a bit longer depending on your system since it loops through the entire vocabulary.</p></li>
</ul>
<p><strong>Example Output:</strong></p>
<pre class="plaintext"><code>Result: woman, Similarity: 69.10123443603516</code></pre>
<p>Both methods achieve the same goal of showing that “King” - “man” + “woman” results in a vector closest to “Queen”.</p>
<p>End.</p>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./11.Text_Representation.html" class="pagination-link" aria-label="Text Representation Based on Counts">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Text Representation Based on Counts</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./13.NLP_toolkit.html" class="pagination-link" aria-label="Word Embedding Activities">
        <span class="nav-page-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Word Embedding Activities</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>