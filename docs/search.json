[
  {
    "objectID": "01.setup.html",
    "href": "01.setup.html",
    "title": "1  Setting Up Environment",
    "section": "",
    "text": "1.1 Setting Up VSCode for Data Analysis\nVisual Studio Code (VSCode) is a free, open-source code editor developed by Microsoft. It supports multiple programming languages, extensions, and integrations, making it an excellent choice for data analysis.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Setting Up Environment</span>"
    ]
  },
  {
    "objectID": "01.setup.html#setting-up-vscode-for-data-analysis",
    "href": "01.setup.html#setting-up-vscode-for-data-analysis",
    "title": "1  Setting Up Environment",
    "section": "",
    "text": "1.1.1 Installing VSCode\n\nDownload VSCode: Visit the VSCode official website and download the installer for your operating system (Windows, macOS, or Linux).\nInstall VSCode: Run the installer and follow the installation instructions.\n\n\n\n1.1.2 Setting Up Python in VSCode\n\nInstall Python: Ensure Python is installed on your system. You can download it from the official Python website.\nInstall Python Extension for VSCode:\n\nOpen VSCode and go to the Extensions view by clicking on the Extensions icon in the sidebar or pressing Ctrl+Shift+X.\nSearch for “Python” and install the extension provided by Microsoft.\n\nVerify Python Installation:\n\nOpen a terminal in VSCode (`Ctrl+``) and type:\npython --version\nYou should see the installed Python version. If not, check your installation path.\n\nSet Up a Virtual Environment (Optional but recommended):\n\nCreate a virtual environment to manage dependencies for your project:\npython -m venv myenv\nActivate the virtual environment:\n\nWindows: myenv\\Scripts\\activate\nmacOS/Linux: source myenv/bin/activate\n\n\nInstall Essential Libraries:\n\nUse pip to install essential libraries like pandas, NumPy, and matplotlib:\npip install pandas numpy matplotlib seaborn\n\nCreate a Python File:\n\nOpen a new file, save it with a .py extension, and write your first Python code. For example:\nimport pandas as pd\nimport numpy as np\n\n# Create a simple DataFrame\ndata = {'Name': ['Alice', 'Bob', 'Charlie'], 'Age': [25, 30, 35]}\ndf = pd.DataFrame(data)\n\nprint(df)\n\nRunning Python Code:\n\nRun the code by clicking the “Run” button at the top right or by pressing F5.\n\n\n\n\n1.1.3 Additional Tips\n\nJupyter Notebook in VSCode: You can also use Jupyter notebooks within VSCode by installing the Jupyter extension.\nLinting and Formatting: Use extensions like Pylint or Black to maintain code quality.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Setting Up Environment</span>"
    ]
  },
  {
    "objectID": "01.setup.html#setting-up-google-colab-for-data-analysis",
    "href": "01.setup.html#setting-up-google-colab-for-data-analysis",
    "title": "1  Setting Up Environment",
    "section": "1.2 Setting Up Google Colab for Data Analysis",
    "text": "1.2 Setting Up Google Colab for Data Analysis\nGoogle Colab is a free, cloud-based platform that provides Jupyter Notebook environments. It allows you to write and execute Python code in your browser, making it a great tool for data analysis, especially when dealing with large datasets and GPU-based computations.\n\n1.2.1 Getting Started with Google Colab\n\nAccess Google Colab:\n\nGo to Google Colab using your Google account.\n\nCreating a New Notebook:\n\nClick on “New Notebook” to create a new notebook. This will open a new page with a code cell ready to run.\n\nInstalling and Importing Libraries:\n\nUse pip to install any additional libraries you need directly within a cell:\n# Install pandas and numpy\n!pip install pandas numpy\n\nWriting and Running Code:\n\nWrite your code in the code cells and run it by clicking the play button on the left side of the cell or pressing Shift+Enter.\nimport pandas as pd\nimport numpy as np\n\n# Creating a DataFrame\ndata = {'Product': ['Apples', 'Bananas', 'Cherries'], 'Price': [1.2, 0.8, 2.5]}\ndf = pd.DataFrame(data)\n\n# Displaying the DataFrame\nprint(df)\n\n\n\n\n1.2.2 Uploading Files and Connecting to Google Drive\n\nUploading Files: You can upload files directly to the Colab environment by using the file upload button on the left sidebar.\nConnecting to Google Drive: If you have data stored on Google Drive, you can easily access it by mounting your drive.\nfrom google.colab import drive\ndrive.mount('/content/drive')\nThis code will prompt you to authorize access to your Google Drive.\n\n\n\n1.2.3 Using GPU or TPU for Accelerated Computing\nGoogle Colab allows you to use GPUs or TPUs to speed up your computations, especially useful for machine learning tasks.\n\nChanging Runtime Type:\n\nGo to Runtime &gt; Change runtime type.\nSelect “GPU” or “TPU” under “Hardware accelerator” and click “Save”.\n\n\n\n\n1.2.4 Saving and Exporting Your Work\n\nSaving Notebooks: Colab automatically saves your work to Google Drive, but you can also save a copy manually by selecting File &gt; Save a copy in Drive.\nExporting Notebooks: You can download your notebook as a .ipynb or .py file by selecting File &gt; Download.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Setting Up Environment</span>"
    ]
  },
  {
    "objectID": "01.setup.html#summary",
    "href": "01.setup.html#summary",
    "title": "1  Setting Up Environment",
    "section": "1.3 Summary",
    "text": "1.3 Summary\n\nVSCode is great for local development with rich support for extensions, virtual environments, and debugging.\nGoogle Colab is ideal for cloud-based development, especially when you need quick access to computational resources like GPUs and TPUs.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Setting Up Environment</span>"
    ]
  },
  {
    "objectID": "01.setup.html#both-environments-are-powerful-tools-for-data-analysis-and-the-choice-depends-on-your-specific-needs-and-workflow-preferences.",
    "href": "01.setup.html#both-environments-are-powerful-tools-for-data-analysis-and-the-choice-depends-on-your-specific-needs-and-workflow-preferences.",
    "title": "1  Setting Up Environment",
    "section": "1.4 Both environments are powerful tools for data analysis, and the choice depends on your specific needs and workflow preferences.",
    "text": "1.4 Both environments are powerful tools for data analysis, and the choice depends on your specific needs and workflow preferences.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Setting Up Environment</span>"
    ]
  },
  {
    "objectID": "02.intro.html",
    "href": "02.intro.html",
    "title": "2  Constants and Variables in Python",
    "section": "",
    "text": "2.1 Variables in Python\nA variable is a named location in memory used to store data that can change during the execution of a program. Variables can hold different types of data, such as numbers, strings, or more complex data structures.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Constants and Variables in Python</span>"
    ]
  },
  {
    "objectID": "02.intro.html#variables-in-python",
    "href": "02.intro.html#variables-in-python",
    "title": "2  Constants and Variables in Python",
    "section": "",
    "text": "2.1.1 Declaring Variables\nTo declare a variable in Python, you simply assign a value to a name using the assignment operator =. Python is dynamically typed, meaning you do not need to declare the type of the variable explicitly.\n# Variable declaration\nx = 10           # An integer variable\nname = \"Alice\"   # A string variable\npi = 3.14159     # A float variable\n\nprint(x)         # Output: 10\nprint(name)      # Output: Alice\nprint(pi)        # Output: 3.14159\n# Updating variable values\nx = 20           # Reassigning a new value to x\nname = \"Bob\"     # Changing the value of name\n\nprint(x)         # Output: 20\nprint(name)      # Output: Bob",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Constants and Variables in Python</span>"
    ]
  },
  {
    "objectID": "02.intro.html#constants-in-python",
    "href": "02.intro.html#constants-in-python",
    "title": "2  Constants and Variables in Python",
    "section": "2.2 Constants in Python",
    "text": "2.2 Constants in Python\nA constant is a value that does not change during the execution of a program. Python does not have a built-in way to define constants explicitly, but naming conventions are used to indicate that a variable should be treated as a constant.\n\n2.2.1 Defining Constants\nPython does not have a specific syntax for defining constants. However, by convention, constants are written in all uppercase letters, and these variables are not supposed to be modified.\n# Defining constants (by convention)\nMAX_SPEED = 120          # A constant integer\nGRAVITY = 9.81           # A constant float\nWELCOME_MESSAGE = \"Hello\" # A constant string\n\nprint(MAX_SPEED)         # Output: 120\nprint(GRAVITY)           # Output: 9.81\nprint(WELCOME_MESSAGE)   # Output: Hello",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Constants and Variables in Python</span>"
    ]
  },
  {
    "objectID": "02.intro.html#numeric-data-types",
    "href": "02.intro.html#numeric-data-types",
    "title": "2  Constants and Variables in Python",
    "section": "3.1 Numeric Data Types",
    "text": "3.1 Numeric Data Types\n\n3.1.1 Integer (int)\nIntegers are whole numbers, positive or negative, without decimals.\n# Integer example\nage = 25\nprint(age)        # Output: 25\nprint(type(age))  # Output: &lt;class 'int'&gt;\n\n\n3.1.2 Float (float)\nFloats represent numbers with decimal points.\n# Float example\nheight = 5.9\nprint(height)        # Output: 5.9\nprint(type(height))  # Output: &lt;class 'float'&gt;\n\n\n3.1.3 Complex (complex)\nComplex numbers consist of a real and an imaginary part.\n# Complex number example\nz = 3 + 4j\nprint(z)            # Output: (3+4j)\nprint(type(z))      # Output: &lt;class 'complex'&gt;",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Constants and Variables in Python</span>"
    ]
  },
  {
    "objectID": "02.intro.html#sequence-data-types",
    "href": "02.intro.html#sequence-data-types",
    "title": "2  Constants and Variables in Python",
    "section": "3.2 Sequence Data Types",
    "text": "3.2 Sequence Data Types\n\n3.2.1 String (str)\nStrings are sequences of characters enclosed in quotes.\n# String example\nname = \"Python\"\nprint(name)        # Output: Python\nprint(type(name))  # Output: &lt;class 'str'&gt;\n\n\n3.2.2 List (list)\nLists are ordered, mutable collections of items that can hold mixed data types.\n# List example\nfruits = [\"apple\", \"banana\", \"cherry\"]\nprint(fruits)        # Output: ['apple', 'banana', 'cherry']\nprint(type(fruits))  # Output: &lt;class 'list'&gt;\n\n3.2.2.1 List Indexing and Slicing\nIndexing allows you to access individual elements in a list using their position, while slicing lets you extract a sublist from a list.\n\n3.2.2.1.1 Indexing\nIndexing starts at 0. Negative indices can be used to access elements from the end of the list.\n# List indexing example\nfruits = ['apple', 'banana', 'cherry']\n\n# Accessing elements\nprint(fruits[0])  # Output: apple\nprint(fruits[-1]) # Output: cherry\n\n\n3.2.2.1.2 Slicing\nSlicing allows you to extract a range of elements. The syntax is list[start:end], where start is inclusive and end is exclusive.\n# List slicing example\nnumbers = [0, 1, 2, 3, 4, 5]\n\n# Extracting a sublist\nprint(numbers[1:4])   # Output: [1, 2, 3]\nprint(numbers[:3])    # Output: [0, 1, 2] (from the start to index 2)\nprint(numbers[3:])    # Output: [3, 4, 5] (from index 3 to the end)\nprint(numbers[-3:])   # Output: [3, 4, 5] (last three elements)\n\n\n\n3.2.2.2 Adding Elements to a List\nYou can add elements to a list using methods like append(), insert(), and extend().\n\n3.2.2.2.1 Using append()\nappend() adds an element to the end of the list.\n# Using append() to add an element\nnumbers = [1, 2, 3]\nnumbers.append(4)\nprint(numbers)  # Output: [1, 2, 3, 4]\n\n\n3.2.2.2.2 Using insert()\ninsert() adds an element at a specified position.\n# Using insert() to add an element at a specific position\nnumbers = [1, 3, 4]\nnumbers.insert(1, 2)  # Insert 2 at index 1\nprint(numbers)  # Output: [1, 2, 3, 4]\n\n\n3.2.2.2.3 Using extend()\nextend() adds multiple elements to the end of the list.\n# Using extend() to add multiple elements\nnumbers = [1, 2]\nnumbers.extend([3, 4, 5])\nprint(numbers)  # Output: [1, 2, 3, 4, 5]\n\n\n\n3.2.2.3 Modifying and Deleting List Elements\nYou can modify or remove elements from a list using index assignments, remove(), pop(), or del.\n\n3.2.2.3.1 Modifying Elements\nAssign a new value to an index to modify an element.\n# Modifying an element in the list\nnumbers = [1, 2, 3]\nnumbers[1] = 20\nprint(numbers)  # Output: [1, 20, 3]\n\n\n3.2.2.3.2 Deleting Elements\nUse remove() to delete an element by value, pop() to remove by index, or del to delete a slice.\n# Using remove() to delete by value\nnumbers = [1, 2, 3, 4]\nnumbers.remove(3)\nprint(numbers)  # Output: [1, 2, 4]\n\n# Using pop() to delete by index\nnumbers = [1, 2, 3, 4]\nremoved_element = numbers.pop(1)\nprint(numbers)  # Output: [1, 3, 4]\nprint(removed_element)  # Output: 2\n\n# Using del to delete by index or slice\nnumbers = [1, 2, 3, 4, 5]\ndel numbers[1:3]\nprint(numbers)  # Output: [1, 4, 5]\n\n\n\n3.2.2.4 Sorting Lists\nUse sort() to sort a list in place, or sorted() to return a new sorted list.\n# Sorting a list in ascending order\nnumbers = [3, 1, 4, 1, 5]\nnumbers.sort()\nprint(numbers)  # Output: [1, 1, 3, 4, 5]\n\n# Sorting a list in descending order\nnumbers.sort(reverse=True)\nprint(numbers)  # Output: [5, 4, 3, 1, 1]\n\n# Using sorted() to return a new sorted list\nnew_numbers = sorted(numbers)\nprint(new_numbers)  # Output: [1, 1, 3, 4, 5]\n\n\n\n3.2.3 3. Tuple (tuple)\nA tuple is an ordered, immutable collection of elements. Tuples are similar to lists but cannot be modified after creation.\n# Creating and accessing tuples\ncoordinates = (10, 20)\nprint(coordinates)        # Output: (10, 20)\nprint(coordinates[0])     # Output: 10\n\n# Tuples are immutable; attempting to modify will raise an error\n# coordinates[0] = 30  # This would cause an error",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Constants and Variables in Python</span>"
    ]
  },
  {
    "objectID": "02.intro.html#mapping-data-type",
    "href": "02.intro.html#mapping-data-type",
    "title": "2  Constants and Variables in Python",
    "section": "3.3 Mapping Data Type",
    "text": "3.3 Mapping Data Type\n\n3.3.1 Dictionary (dict)\nA dictionary is a collection of key-value pairs, where each key is unique. Dictionaries are mutable, allowing for dynamic updates.\n# Dictionary example\nperson = {\"name\": \"Alice\", \"age\": 30}\nprint(person)        # Output: {'name': 'Alice', 'age': 30}\nprint(type(person))  # Output: &lt;class 'dict'&gt;\n\n3.3.1.1 Accessing Values Using Keys\nAccess values in a dictionary using keys.\n# Accessing values using keys\nname = person[\"name\"]\nage = person.get(\"age\")\nprint(name)  # Output: Alice\nprint(age)   # Output: 30\n\n\n3.3.1.2 Adding and Removing Key-Value Pairs\nYou can add new key-value pairs or remove existing ones using methods like update(), del, or pop().\n\n3.3.1.2.1 Adding Key-Value Pairs\n# Adding new key-value pairs\nperson[\"city\"] = \"New York\"\nprint(person)  # Output: {'name': 'Alice', 'age': 30, 'city': 'New York'}\n\n\n3.3.1.2.2 Removing Key-Value Pairs\n# Removing a key-value pair using pop()\nremoved_value = person.pop(\"age\")\nprint(person)        # Output: {'name': 'Alice', 'city': 'New York'}\nprint(removed_value) # Output: 30\n\n# Removing a key-value pair using del\ndel person[\"city\"]\nprint(person)  # Output: {'name': 'Alice'}",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Constants and Variables in Python</span>"
    ]
  },
  {
    "objectID": "02.intro.html#set-data-types",
    "href": "02.intro.html#set-data-types",
    "title": "2  Constants and Variables in Python",
    "section": "3.4 Set Data Types",
    "text": "3.4 Set Data Types\n\n3.4.1 Set (set)\nSets are unordered collections of unique items.\n# Set example\nnumbers = {1, 2, 3, 4}\nprint(numbers)        # Output: {1, 2, 3, 4}\nprint(type(numbers))  # Output: &lt;class 'set'&gt;",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Constants and Variables in Python</span>"
    ]
  },
  {
    "objectID": "02.intro.html#boolean-data-type",
    "href": "02.intro.html#boolean-data-type",
    "title": "2  Constants and Variables in Python",
    "section": "3.5 Boolean Data Type",
    "text": "3.5 Boolean Data Type\n\n3.5.1 Boolean (bool)\nBooleans represent one of two values: True or False.\n# Boolean example\nis_active = True\nprint(is_active)        # Output: True\nprint(type(is_active))  # Output: &lt;class 'bool'&gt;",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Constants and Variables in Python</span>"
    ]
  },
  {
    "objectID": "02.intro.html#f-string-formatting-in-python",
    "href": "02.intro.html#f-string-formatting-in-python",
    "title": "2  Constants and Variables in Python",
    "section": "3.6 f-string Formatting in Python",
    "text": "3.6 f-string Formatting in Python\nf-strings, introduced in Python 3.6, are a modern and powerful way to format strings. They offer a concise and readable syntax, allowing you to embed expressions directly inside string literals by prefixing the string with the letter f or F.\nf-strings make string formatting simpler and more intuitive compared to older methods such as % formatting or the str.format() method. They allow for inline expression evaluation, formatting of numbers, and easy manipulation of string data.\n\n3.6.1 Embedding Variables\nTo use f-strings, simply place an f before the opening quote of the string and include expressions inside curly braces {}.\n# Basic usage of f-string\nname = \"Alice\"\nage = 30\n\n# Embedding variables in a string\ngreeting = f\"Hello, {name}! You are {age} years old.\"\nprint(greeting)  # Output: Hello, Alice! You are 30 years old.\n\n\n3.6.2 Inline Expressions\nf-strings allow you to include any valid Python expression inside the curly braces.\n# Using expressions inside f-strings\nx = 10\ny = 5\n\n# Inline arithmetic expression\nresult = f\"{x} + {y} = {x + y}\"\nprint(result)  # Output: 10 + 5 = 15",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Constants and Variables in Python</span>"
    ]
  },
  {
    "objectID": "02.intro.html#advanced-formatting-with-f-strings",
    "href": "02.intro.html#advanced-formatting-with-f-strings",
    "title": "2  Constants and Variables in Python",
    "section": "3.7 Advanced Formatting with f-strings",
    "text": "3.7 Advanced Formatting with f-strings\n\n3.7.1 Number Formatting\nf-strings provide options to format numbers in various ways, such as controlling decimal places, adding commas, or displaying percentages.\n\n3.7.1.1 Formatting Floats\nYou can specify the number of decimal places by using .nf, where n is the number of decimal places.\n# Formatting a float to 2 decimal places\npi = 3.14159265358979\nformatted_pi = f\"Pi rounded to 2 decimal places: {pi:.2f}\"\nprint(formatted_pi)  # Output: Pi rounded to 2 decimal places: 3.14\n\n\n3.7.1.2 Adding Commas to Large Numbers\nUse the :, format specifier to include commas as thousand separators.\n# Adding commas to large numbers\nlarge_number = 1000000\nformatted_number = f\"The number is: {large_number:,}\"\nprint(formatted_number)  # Output: The number is: 1,000,000\n\n\n3.7.1.3 Displaying Percentages\nTo display a number as a percentage, use the % format specifier.\n# Displaying a percentage\nsuccess_rate = 0.85\nformatted_rate = f\"Success rate: {success_rate:.2%}\"\nprint(formatted_rate)  # Output: Success rate: 85.00%\n\n\n\n3.7.2 Embedding Dictionary Values\nf-strings can also be used to format values from dictionaries.\n# Formatting dictionary values\nperson = {\"name\": \"Bob\", \"age\": 40}\nformatted_string = f\"{person['name']} is {person['age']} years old.\"\nprint(formatted_string)  # Output: Bob is 40 years old.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Constants and Variables in Python</span>"
    ]
  },
  {
    "objectID": "03.control.html",
    "href": "03.control.html",
    "title": "3  Control Structures in Python",
    "section": "",
    "text": "3.1 Control Structures",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Control Structures in Python</span>"
    ]
  },
  {
    "objectID": "03.control.html#control-structures",
    "href": "03.control.html#control-structures",
    "title": "3  Control Structures in Python",
    "section": "",
    "text": "3.1.1 if Statement\nThe if statement is used to execute a block of code only if a specified condition is true. You can use if, elif (else if), and else to build more complex conditions.\n\n3.1.1.1 Basic if Statement\n# Basic if statement\nage = 18\nif age &gt;= 18:\n    print(\"You are an adult.\")  # Output: You are an adult.\n\n\n3.1.1.2 if-elif-else Structure\nYou can chain multiple conditions using elif and provide a default action using else.\n# if-elif-else example\nscore = 85\nif score &gt;= 90:\n    print(\"Grade: A\")\nelif score &gt;= 80:\n    print(\"Grade: B\")  # Output: Grade: B\nelif score &gt;= 70:\n    print(\"Grade: C\")\nelse:\n    print(\"Grade: F\")\n\n\n3.1.1.3 Nested if Statements\nif statements can be nested within each other to handle complex conditions.\n# Nested if statement\nnumber = 10\nif number &gt; 0:\n    print(\"Positive number\")  # Output: Positive number\n    if number % 2 == 0:\n        print(\"Even number\")  # Output: Even number\n\n\n\n3.1.2 while Loop\nThe while loop repeatedly executes a block of code as long as the specified condition is true. It is used when the number of iterations is not known in advance.\n\n3.1.2.1 Basic while Loop\n# Basic while loop\ncount = 1\nwhile count &lt;= 5:\n    print(count)  # Output: 1 2 3 4 5\n    count += 1\n\n\n3.1.2.2 while Loop with break and continue\n\nbreak: Exits the loop immediately.\ncontinue: Skips the current iteration and moves to the next one.\n\n# Using break and continue in a while loop\nnum = 0\nwhile num &lt; 10:\n    num += 1\n    if num == 5:\n        continue  # Skip the number 5\n    if num == 8:\n        break  # Exit the loop when num is 8\n    print(num)  # Output: 1 2 3 4 6 7\n\n\n\n3.1.3 for Loop\nThe for loop iterates over a sequence (such as a list, tuple, string, or range). It is used when the number of iterations is known.\n\n3.1.3.1 Basic for Loop\n# Basic for loop\nfor i in range(5):\n    print(i)  # Output: 0 1 2 3 4\n\n\n3.1.3.2 Iterating Over a List\n# Iterating over a list\nfruits = ['apple', 'banana', 'cherry']\nfor fruit in fruits:\n    print(fruit)  # Output: apple banana cherry\n\n\n3.1.3.3 for Loop with break and continue\n# Using break and continue in a for loop\nfor num in range(1, 10):\n    if num == 4:\n        continue  # Skip number 4\n    if num == 7:\n        break  # Stop the loop when num is 7\n    print(num)  # Output: 1 2 3 5 6\n\n\n\n3.1.4 Exception Handling\nException handling allows you to manage errors gracefully without crashing your program. Use try, except, else, and finally blocks to handle exceptions.\n\n3.1.4.1 Basic Exception Handling with try and except\n# Basic try-except block\ntry:\n    result = 10 / 0  # This will raise a ZeroDivisionError\nexcept ZeroDivisionError:\n    print(\"Cannot divide by zero!\")  # Output: Cannot divide by zero!\n\n\n3.1.4.2 Multiple Exceptions\nYou can handle multiple exceptions by specifying different exception types.\n# Handling multiple exceptions\ntry:\n    value = int(\"abc\")  # This will raise a ValueError\nexcept ValueError:\n    print(\"Invalid number format.\")  # Output: Invalid number format.\nexcept ZeroDivisionError:\n    print(\"Division by zero is not allowed.\")\n\n\n3.1.4.3 Using else and finally\n\nelse: Executes if no exception occurs.\nfinally: Executes no matter what, useful for cleanup actions.\n\n# Using else and finally with try-except\ntry:\n    num = 5\n    result = num / 1\nexcept ZeroDivisionError:\n    print(\"Cannot divide by zero.\")\nelse:\n    print(\"Division successful:\", result)  # Output: Division successful: 5.0\nfinally:\n    print(\"Execution complete.\")  # Output: Execution complete.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Control Structures in Python</span>"
    ]
  },
  {
    "objectID": "04.function.html",
    "href": "04.function.html",
    "title": "4  Functions and Packages in Python",
    "section": "",
    "text": "4.1 Functions in Python\nFunctions are reusable blocks of code that perform specific tasks. They help in organizing code, making it modular, readable, and easy to maintain. Python allows you to define your own functions, and it also comes with many built-in functions.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Functions and Packages in Python</span>"
    ]
  },
  {
    "objectID": "04.function.html#functions-in-python",
    "href": "04.function.html#functions-in-python",
    "title": "4  Functions and Packages in Python",
    "section": "",
    "text": "4.1.1 What is a Function?\nA function is a block of organized, reusable code that is used to perform a single, related action. Functions provide better modularity for your application and allow code reuse.\n\n4.1.1.1 Key Points:\n\nModularity: Functions divide complex problems into smaller, manageable tasks.\nReusability: Functions allow the code to be used repeatedly without rewriting.\nReadability: Well-defined functions make the code clearer and easier to understand.\n\n\n\n\n4.1.2 Defining a Function\nFunctions are defined using the def keyword, followed by the function name and parentheses () containing any parameters. The function body is indented and contains the code that runs when the function is called.\n\n4.1.2.1 Basic Syntax\n# Defining a simple function\ndef function_name(parameters):\n    \"\"\"\n    Optional docstring describing the function.\n    \"\"\"\n    # Function body\n    # Code to execute\n    return value  # Optional return statement\n\n\n4.1.2.2 Example of a Simple Function\n# A function to add two numbers\ndef add(a, b):\n    return a + b\n\n# Calling the function\nresult = add(3, 5)\nprint(result)  # Output: 8\n\n\n\n4.1.3 Function Parameters and Arguments\n\n4.1.3.1 Positional Arguments\nArguments passed to a function in the order they are defined.\n# Function with positional arguments\ndef greet(name, message):\n    print(f\"{message}, {name}!\")\n\ngreet(\"Alice\", \"Hello\")  # Output: Hello, Alice!\n\n\n4.1.3.2 Default Arguments\nDefault arguments allow parameters to have default values, which are used if no argument is provided.\n# Function with default arguments\ndef greet(name, message=\"Hello\"):\n    print(f\"{message}, {name}!\")\n\ngreet(\"Bob\")           # Output: Hello, Bob!\ngreet(\"Charlie\", \"Hi\") # Output: Hi, Charlie!\n\n\n4.1.3.3 Keyword Arguments\nKeyword arguments are passed by explicitly specifying the parameter names, allowing you to change the order of arguments.\n# Function with keyword arguments\ndef describe_pet(animal_type, pet_name):\n    print(f\"I have a {animal_type} named {pet_name}.\")\n\ndescribe_pet(pet_name=\"Max\", animal_type=\"dog\")  # Output: I have a dog named Max.\n\n\n4.1.3.4 Variable-Length Arguments (*args and **kwargs)\n\n*args allows a function to accept any number of positional arguments, which are passed as a tuple.\n**kwargs allows a function to accept any number of keyword arguments, which are passed as a dictionary.\n\n# Using *args\ndef sum_all(*args):\n    return sum(args)\n\nprint(sum_all(1, 2, 3, 4))  # Output: 10\n\n# Using **kwargs\ndef print_info(**kwargs):\n    for key, value in kwargs.items():\n        print(f\"{key}: {value}\")\n\nprint_info(name=\"Alice\", age=30)  \n# Output:\n# name: Alice\n# age: 30\n\n\n\n4.1.4 Return Statement\nThe return statement is used to exit a function and return a value. If no return statement is used, the function will return None by default.\n# Function with a return statement\ndef square(x):\n    return x * x\n\nresult = square(5)\nprint(result)  # Output: 25\n\n\n4.1.5 Lambda Functions\nA lambda function is a small, anonymous function defined using the lambda keyword. Lambda functions can have any number of arguments but only one expression. They are often used for short, simple operations that are used once or as arguments to other functions.\n\n4.1.5.1 Basic Syntax of a Lambda Function\n# Syntax of a lambda function\n# lambda arguments: expression\n\n# Example of a lambda function to add two numbers\nadd = lambda x, y: x + y\nprint(add(3, 5))  # Output: 8\n\n\n4.1.5.2 Use Cases of Lambda Functions\n\nUsing with map(), filter(), and reduce(): Lambda functions are commonly used with these built-in functions for quick operations.\n\n# Using lambda with map() to square each number in a list\nnumbers = [1, 2, 3, 4]\nsquared = list(map(lambda x: x**2, numbers))\nprint(squared)  # Output: [1, 4, 9, 16]\n\nSorting by a Custom Key: You can use lambda functions as the key argument in sorting.\n\n# Sorting a list of tuples by the second element using a lambda function\npoints = [(2, 3), (1, 2), (3, 1)]\nsorted_points = sorted(points, key=lambda x: x[1])\nprint(sorted_points)  # Output: [(3, 1), (1, 2), (2, 3)]\n\n\n4.1.5.3 Limitations of Lambda Functions\n\nLimited to a single expression.\nLess readable for complex operations compared to regular functions.\nCannot contain statements or annotations.\n\n\n\n\n4.1.6 Scope and Lifetime of Variables\nVariables defined inside a function are local to that function and cannot be accessed outside. Variables defined outside functions are global.\n# Scope example\ndef my_function():\n    local_var = \"I am local\"\n    print(local_var)\n\nmy_function()            # Output: I am local\n# print(local_var)       # This would raise an error: NameError\n\n\n4.1.7 Docstrings\nDocstrings are used to document functions. They provide a way to describe what the function does, its parameters, and its return value.\n# Function with a docstring\ndef multiply(a, b):\n    \"\"\"\n    Multiply two numbers.\n\n    Parameters:\n    a (int or float): The first number.\n    b (int or float): The second number.\n\n    Returns:\n    int or float: The product of a and b.\n    \"\"\"\n    return a * b\n\nhelp(multiply)  # This will display the function's docstring",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Functions and Packages in Python</span>"
    ]
  },
  {
    "objectID": "04.function.html#packages",
    "href": "04.function.html#packages",
    "title": "4  Functions and Packages in Python",
    "section": "4.2 Packages",
    "text": "4.2 Packages\nA package is a collection of Python modules grouped together within a directory. Packages help organize and distribute reusable code across multiple projects. They make it easy to manage and share code.\n\n4.2.1 Importing a Package\nYou can import specific modules from a package or import the entire package.\n# Importing a specific module from a package\nfrom mypackage import module1\n\n# Importing a specific function from a module within a package\nfrom mypackage.module1 import my_function\n\n\n4.2.2 Using Standard Packages\nPython has a rich standard library with many built-in packages, such as math, os, and random. You can also install third-party packages using pip.\n# Using a function from the math package\nimport math\n\nresult = math.sqrt(16)\nprint(result)  # Output: 4.0",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Functions and Packages in Python</span>"
    ]
  },
  {
    "objectID": "04.function.html#difference-between-functions-and-methods",
    "href": "04.function.html#difference-between-functions-and-methods",
    "title": "4  Functions and Packages in Python",
    "section": "4.3 Difference Between Functions and Methods",
    "text": "4.3 Difference Between Functions and Methods\n\n4.3.1 Functions\n\nDefinition: A function is a block of reusable code that performs a specific task. It can be defined using the def keyword and can be called independently of objects.\nUsage: Functions are generally used to perform a task, and they can accept parameters and return values.\n\n# Example of a function\ndef greet(name):\n    return f\"Hello, {name}!\"\n\nprint(greet(\"Alice\"))  # Output: Hello, Alice!\n\n\n4.3.2 Methods\n\nDefinition: A method is a function that is associated with an object. It is called on an object and usually works with the data contained in that object.\nUsage: Methods are used to perform operations on objects and are called using the dot notation.\n\n# Example of a method in a class\nclass Person:\n    def __init__(self, name):\n        self.name = name\n\n    def greet(self):\n        return f\"Hello, {self.name}!\"\n\n# Creating an object and calling the greet method\nperson = Person(\"Bob\")\nprint(person.greet())  # Output: Hello, Bob!\n\n\n4.3.3 Key Differences\n\nAssociation: Functions are standalone, while methods are associated with objects.\nCalling: Functions are called directly, while methods are called on objects using dot notation.\n4.4 First Parameter: Methods often have self as their first parameter, referring to the object itself, whereas functions do not.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Functions and Packages in Python</span>"
    ]
  },
  {
    "objectID": "05.pandas.html",
    "href": "05.pandas.html",
    "title": "5  Data Analysis in Python",
    "section": "",
    "text": "5.1 Series\nA Series is a one-dimensional array-like object provided by pandas that can hold data of any type (integers, floats, strings, Python objects, etc.). It is similar to a column in an Excel spreadsheet or a single-column DataFrame in pandas. Each element in a Series has an associated index, which is used to access individual elements.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Analysis in Python</span>"
    ]
  },
  {
    "objectID": "05.pandas.html#series",
    "href": "05.pandas.html#series",
    "title": "5  Data Analysis in Python",
    "section": "",
    "text": "5.1.1 Creating a Series\nYou can create a Series in pandas using various data structures such as lists, dictionaries, or NumPy arrays. You can also specify the index manually.\n\n5.1.1.1 Creating a Series from a List\nThe simplest way to create a Series is by passing a list to the pd.Series() constructor.\nimport pandas as pd\n\n# Creating a Series from a list\ndata = [10, 20, 30, 40]\nseries = pd.Series(data)\nprint(series)\nOutput:\n0    10\n1    20\n2    30\n3    40\ndtype: int64\nIn this example, the default index starts at 0 and increments by 1.\n\n\n5.1.1.2 Creating a Series with Custom Index\nYou can specify custom indices when creating a Series, making it more descriptive.\n# Creating a Series with custom indices\ndata = [100, 200, 300]\nindex = ['a', 'b', 'c']\ncustom_series = pd.Series(data, index=index)\nprint(custom_series)\nOutput:\na    100\nb    200\nc    300\ndtype: int64\n\n\n5.1.1.3 Creating a Series from a Dictionary\nYou can also create a Series from a dictionary, where keys become the indices, and values become the Series values.\n# Creating a Series from a dictionary\ndata_dict = {'apple': 50, 'banana': 30, 'cherry': 20}\nfruit_series = pd.Series(data_dict)\nprint(fruit_series)\nOutput:\napple     50\nbanana    30\ncherry    20\ndtype: int64\n\n\n\n5.1.2 Selecting Elements\nSelecting elements from a Series can be done using indexing and slicing, similar to Python lists and NumPy arrays. The indexing can be done using integer-based indexing or label-based indexing if a custom index is set.\n\n5.1.2.1 1.2.1 Accessing Single Elements\nYou can access single elements by using their index position or label.\n# Accessing elements by index position\nprint(series[0])    # Output: 10\n\n# Accessing elements by index label\nprint(custom_series['b'])  # Output: 200\n\n\n5.1.2.2 Slicing a Series\nSlicing allows you to select a range of elements from a Series.\n# Slicing using index positions\nprint(series[1:3])  # Output: \n# 1    20\n# 2    30\n# dtype: int64\n\n# Slicing using index labels\nprint(custom_series['a':'b'])  # Output:\n# a    100\n# b    200\n# dtype: int64\n\n\n\n5.1.3 Series Operations\nSeries in pandas support vectorized operations, which means you can perform operations on entire Series without writing loops. These operations are element-wise, and pandas will align data using the index.\n\n5.1.3.1 Arithmetic Operations\nYou can perform arithmetic operations like addition, subtraction, multiplication, and division directly on Series.\n# Arithmetic operations\nseries1 = pd.Series([1, 2, 3, 4])\nseries2 = pd.Series([10, 20, 30, 40])\n\n# Adding two Series\nsum_series = series1 + series2\nprint(sum_series)\n\n# Output:\n# 0    11\n# 1    22\n# 2    33\n# 3    44\n# dtype: int64\n\n\n5.1.3.2 Mathematical Functions\nYou can use mathematical functions like mean(), sum(), max(), and others on Series.\n# Using mathematical functions\nvalues = pd.Series([5, 10, 15, 20])\nprint(\"Mean:\", values.mean())   # Output: Mean: 12.5\nprint(\"Sum:\", values.sum())     # Output: Sum: 50\nprint(\"Max:\", values.max())     # Output: Max: 20\n\n\n5.1.3.3 Applying Functions with apply()\nThe apply() function allows you to apply a custom function to each element in the Series.\n# Applying a custom function to each element\ndef square(x):\n    return x * x\n\nsquared_series = values.apply(square)\nprint(squared_series)\n\n# Output:\n# 0     25\n# 1    100\n# 2    225\n# 3    400\n# dtype: int64\n\n\n5.1.3.4 Handling Missing Data\nSeries can handle missing data (NaN values), and you can manipulate these values using methods like fillna(), dropna(), and isna().\n# Series with missing values\ndata_with_nan = pd.Series([1, 2, None, 4])\nprint(data_with_nan)\n\n# Filling missing values\nfilled_series = data_with_nan.fillna(0)\nprint(filled_series)\n\n# Output:\n# 0    1.0\n# 1    2.0\n# 2    0.0\n# 3    4.0\n# dtype: float64\n\n\n\n5.1.4 Conclusion\nSeries in pandas is a powerful tool for one-dimensional data manipulation, allowing for efficient data access, arithmetic operations, and handling of missing data. Understanding Series is fundamental to working with more complex pandas data structures like DataFrames.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Analysis in Python</span>"
    ]
  },
  {
    "objectID": "05.pandas.html#dataframe",
    "href": "05.pandas.html#dataframe",
    "title": "5  Data Analysis in Python",
    "section": "5.2 DataFrame",
    "text": "5.2 DataFrame\nA DataFrame is a two-dimensional, size-mutable, and heterogeneous tabular data structure with labeled axes (rows and columns) in pandas. Think of it as a table of data, similar to an Excel spreadsheet or a SQL table, which can hold data of various types (strings, integers, floats, etc.).\n\n5.2.1 Creating and Modifying DataFrames\nDataFrames can be created from various sources, including lists, dictionaries, NumPy arrays, and even other pandas objects like Series.\n\n5.2.1.1 Creating a DataFrame from a Dictionary\nThe most common way to create a DataFrame is by using a dictionary, where the keys become column names, and the values are lists representing column data.\nimport pandas as pd\n\n# Creating a DataFrame from a dictionary\ndata = {\n    'Name': ['Alice', 'Bob', 'Charlie'],\n    'Age': [25, 30, 35],\n    'City': ['New York', 'Los Angeles', 'Chicago']\n}\ndf = pd.DataFrame(data)\nprint(df)\nOutput:\n      Name  Age         City\n0    Alice   25     New York\n1      Bob   30  Los Angeles\n2  Charlie   35      Chicago\n\n\n5.2.1.2 Creating a DataFrame from a List of Lists\nYou can create a DataFrame from a list of lists, specifying column names separately.\n# Creating a DataFrame from a list of lists\ndata = [\n    ['Alice', 25, 'New York'],\n    ['Bob', 30, 'Los Angeles'],\n    ['Charlie', 35, 'Chicago']\n]\ncolumns = ['Name', 'Age', 'City']\ndf = pd.DataFrame(data, columns=columns)\nprint(df)\n\n\n5.2.1.3 Creating a DataFrame from a NumPy Array\nDataFrames can also be created from NumPy arrays. You must specify column names as pandas does not infer them automatically.\nimport numpy as np\n\n# Creating a DataFrame from a NumPy array\narray_data = np.array([[1, 2], [3, 4], [5, 6]])\ndf = pd.DataFrame(array_data, columns=['Column1', 'Column2'])\nprint(df)\n\n\n5.2.1.4 Modifying a DataFrame\nYou can add, modify, or delete rows and columns in a DataFrame.\nAdding a Column:\n# Adding a new column to an existing DataFrame\ndf['Score'] = [90, 85, 88]\nprint(df)\nModifying a Column:\n# Modifying an existing column\ndf['Age'] = df['Age'] + 1\nprint(df)\nDeleting a Column:\n# Deleting a column\ndf.drop('Score', axis=1, inplace=True)\nprint(df)\n\n\n\n5.2.2 Selecting Rows and Columns\nSelecting data from a DataFrame can be done using indexing, loc, and iloc.\n\n5.2.2.1 Selecting Columns\nColumns can be selected using square brackets, dot notation, or loc.\n# Selecting a single column\nnames = df['Name']\nprint(names)\n\n# Selecting multiple columns\nsubset = df[['Name', 'City']]\nprint(subset)\n\n\n5.2.2.2 Selecting Rows\nRows can be selected by index position using iloc or by label using loc.\nUsing iloc (Index Location):\niloc selects rows and columns by integer index positions.\n# Selecting rows using iloc\nprint(df.iloc[0])  # First row\nprint(df.iloc[1:3])  # Slicing rows\nUsing loc (Label Location):\nloc selects rows and columns by label (index and column names).\n# Selecting rows using loc\ndf.set_index('Name', inplace=True)  # Setting 'Name' column as index\nprint(df.loc['Alice'])  # Select row with index 'Alice'\n\n\n\n5.2.3 Filtering Data in DataFrames\nFiltering allows you to extract specific rows based on conditions.\nFiltering Rows Based on Conditions:\n# Filtering rows where Age &gt; 30\nfiltered_df = df[df['Age'] &gt; 30]\nprint(filtered_df)\n\n\n5.2.4 Adding and Removing Rows\nYou can add rows to a DataFrame using the append() method and remove rows using drop().\n\n5.2.4.1 Adding Rows\n# Adding a new row using append() # will be expired.\nnew_row = pd.DataFrame([['David', 28, 'Seattle']], columns=['Name', 'Age', 'City'])\ndf = df.append(new_row, ignore_index=True)\nprint(df)\nHere’s the revised version using updated pandas methods such as pd.concat() instead of append():\nimport pandas as pd\n\n# Creating the initial DataFrame\ndata = {\n    'Name': ['Alice', 'Bob', 'Charlie'],\n    'Age': [25, 30, 35],\n    'City': ['New York', 'Los Angeles', 'Chicago']\n}\ndf = pd.DataFrame(data)\n\n# Adding a new row using pd.concat()\nnew_row = pd.DataFrame([['David', 28, 'Seattle']], columns=['Name', 'Age', 'City'])\ndf = pd.concat([df, new_row], ignore_index=True)\nprint(df)\n\n\n5.2.4.2 Removing Rows\n# Removing rows by index\ndf = df.drop(0)  # Removes the first row\nprint(df)\nIn this updated code: - Rows are added using pd.concat(), which is the recommended method in the newer versions of pandas. - The drop() method is still valid and is used to remove rows by their index.\n\n\n\n5.2.5 Handling Missing Data in DataFrames\nHandling missing data is essential for clean and accurate data analysis.\n\n5.2.5.1 Identifying Missing Data\nimport pandas as pd\nimport numpy as np\n\n# Creating a DataFrame with missing values\ndata = {\n    'Name': ['Alice', 'Bob', 'Charlie', 'David'],\n    'Age': [25, 30, np.nan, 40],\n    'Salary': [50000, 60000, np.nan, 80000]\n}\ndf = pd.DataFrame(data)\n\n# Checking for missing data\nprint(df.isna())  # Returns a DataFrame of booleans\nprint(df.isna().sum())  # Sum of missing values in each column\n\n\n5.2.5.2 Filling Missing Data\nYou can fill missing data with a specific value, such as 0 or the mean of the column.\n# Filling missing values with a specified value\ndf['Age'].fillna(df['Age'].mean(), inplace=True)\nThe warning you received is related to pandas’ behavior when using chained assignments. In future versions, using inplace=True with column-level operations might not work as expected. To fix this issue and ensure compatibility with future versions of pandas, you should avoid using inplace=True in this context. Instead, you can assign the result of the fillna() operation directly back to the DataFrame.\nHere’s the corrected code:\nimport pandas as pd\nimport numpy as np\n\n# Creating a DataFrame with missing values\ndata = {\n    'Name': ['Alice', 'Bob', 'Charlie', 'David'],\n    'Age': [25, 30, np.nan, 40],\n    'Salary': [50000, 60000, np.nan, 80000]\n}\ndf = pd.DataFrame(data)\n\n# Filling missing values in the 'Salary' column and assigning back to the original DataFrame\ndf['Salary'] = df['Salary'].fillna(df['Salary'].mean())\n\nprint(df)\n\n\n5.2.5.3 Dropping Missing Data\nRemove rows or columns that contain missing values.\n# Dropping rows with missing values\ndf.dropna(inplace=True)\ndf\n\n\n\n5.2.6 Sorting Data in DataFrames\nSorting can be performed based on one or multiple columns using the sort_values() method.\n# Sorting DataFrame by a column\nsorted_df = df.sort_values(by='Age', ascending=False)\nprint(sorted_df)\n\n\n5.2.7 Grouping Data (cf. MS Excel ‘PivotTable’)\nGrouping data is useful for performing aggregate operations on subsets of data.\n\n5.2.7.1 Option 1: Select Numeric Columns Before Grouping\n# Grouping by a column and calculating the mean\nimport pandas as pd\n\n# Creating a DataFrame\ndata = {\n    'Name': ['Alice', 'Bob', 'Charlie', 'David', 'Emma'],\n    'Age': [25, 30, 35, 40, 28],\n    'Salary': [50000, 60000, 70000, 80000, 55000],\n    'Department': ['HR', 'IT', 'IT', 'HR', 'HR']\n}\ndf = pd.DataFrame(data)\nprint(df)\n\n# Grouping by the 'Department' column and calculating the mean of only numeric columns\ngrouped = df.groupby('Department')[['Age', 'Salary']].mean()\n\nprint(grouped)\n\n\n5.2.7.2 Option 2: Use numeric_only=True (if available)\nimport pandas as pd\n\n# Creating a DataFrame\ndata = {\n    'Name': ['Alice', 'Bob', 'Charlie', 'David', 'Emma'],\n    'Age': [25, 30, 35, 40, 28],\n    'Salary': [50000, 60000, 70000, 80000, 55000],\n    'Department': ['HR', 'IT', 'IT', 'HR', 'HR']\n}\ndf = pd.DataFrame(data)\n\n# Grouping by the 'Department' column and calculating the mean, ignoring non-numeric columns\ngrouped = df.groupby('Department').mean(numeric_only=True)\n\nprint(grouped)\n\n\n\n5.2.8 Merging DataFrames\nMerging combines multiple DataFrames into one, similar to SQL JOIN operations.\n\n5.2.8.1 Using merge()\n# Merging two DataFrames on a common column\ndf1 = pd.DataFrame({'Name': ['Alice', 'Bob'], 'Score': [85, 90]})\ndf2 = pd.DataFrame({'Name': ['Alice', 'Bob'], 'Grade': ['A', 'B']})\n\nprint(df1)\nprint('---')\nprint(df2)\nprint('---')\n\nmerged_df = pd.merge(df1, df2, on='Name')\nprint(merged_df)\n\n\n5.2.8.2 Using concat()\n# Concatenating DataFrames \ndf3 = pd.concat([df1, df2], axis=1)\nprint(df3)\n\n\n\n5.2.9 Exporting DataFrames\nYou can export DataFrames to various file formats, including CSV, Excel, and JSON.\n# Install the correct library\n!pip install openpyxl\n\nimport pandas as pd\nimport openpyxl\n\n# Creating a DataFrame\ndata = {\n    'Name': ['Alice', 'Bob', 'Charlie', 'David', 'Emma'],\n    'Age': [25, 30, 35, 40, 28],\n    'Salary': [50000, 60000, 70000, 80000, 55000],\n    'Department': ['HR', 'IT', 'IT', 'HR', 'HR']\n}\ndf = pd.DataFrame(data)\n# Saving DataFrame to a CSV file\ndf.to_csv('output.csv', index=False)\n\n# Saving DataFrame to an Excel file\ndf.to_excel('output.xlsx', index=False)\n\n\n5.2.10 Conclusion\nDataFrames in pandas are versatile and powerful for handling tabular data in Python. They provide robust functionality for data manipulation, cleaning, and analysis, making them an essential tool for data scientists and analysts.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Analysis in Python</span>"
    ]
  },
  {
    "objectID": "05.pandas.html#loading-and-saving-data",
    "href": "05.pandas.html#loading-and-saving-data",
    "title": "5  Data Analysis in Python",
    "section": "5.3 Loading and Saving Data",
    "text": "5.3 Loading and Saving Data\nPandas provides powerful methods to load data from various file formats and save it back after manipulation. Common formats include CSV, Excel, JSON, and SQL databases.\n\n5.3.1 Loading Data\nPandas offers various functions to read data from external sources into DataFrames, allowing for quick data analysis and manipulation.\n\n5.3.1.1 Reading CSV Files\nThe read_csv() function reads data from a CSV file into a DataFrame. This is one of the most common methods to import data.\nimport pandas as pd\n\n# Reading a CSV file into a DataFrame\ndf = pd.read_csv('data.csv')\nprint(df.head())  # Display the first 5 rows\n\n\n5.3.1.2 Reading Excel Files\nTo read Excel files, use the read_excel() function. You can specify the sheet name if the Excel file contains multiple sheets.\n# Reading data from an Excel file\ndf = pd.read_excel('data.xlsx', sheet_name='Sheet1')\nprint(df.head())\n\n\n5.3.1.3 Reading JSON Files\nJSON files can be read using the read_json() function. JSON is commonly used for data exchange between systems.\n# Reading data from a JSON file\ndf = pd.read_json('data.json')\nprint(df.head())\n\n\n\n5.3.2 Saving Data\nOnce you have manipulated your data, you can save it back to your preferred format using pandas functions.\n\n5.3.2.1 Saving to CSV\nThe to_csv() function saves the DataFrame to a CSV file. You can choose whether to include the index or not.\n# Saving DataFrame to a CSV file\ndf.to_csv('output.csv', index=False)\n\n\n5.3.2.2 Saving to Excel\nTo save data to an Excel file, use the to_excel() function. You can also specify the sheet name.\n# Saving DataFrame to an Excel file\ndf.to_excel('output.xlsx', index=False, sheet_name='Results')\n\n\n5.3.2.3 Saving to JSON\nYou can save data to JSON format using the to_json() function, which is useful for data interchange between applications.\n# Saving DataFrame to a JSON file\ndf.to_json('output.json')",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Analysis in Python</span>"
    ]
  },
  {
    "objectID": "05.pandas.html#exploring-summary-information-and-statistical-values",
    "href": "05.pandas.html#exploring-summary-information-and-statistical-values",
    "title": "5  Data Analysis in Python",
    "section": "5.4 Exploring Summary Information and Statistical Values",
    "text": "5.4 Exploring Summary Information and Statistical Values\nExploring data is an essential step in understanding the structure, distribution, and characteristics of your dataset. Pandas provides several methods to inspect and summarize DataFrames.\n\n5.4.1 Viewing Basic Information\nThe info() method provides a concise summary of the DataFrame, including the index, column names, non-null counts, and data types.\nimport pandas as pd\n\n# Expanding the data with more people and overlapping cities\ndata = {\n    'Name': ['Alice', 'Bob', 'Charlie', 'David', 'Emma', 'Frank', 'Grace', 'Hannah', 'Ian', 'Jack'],\n    'Age': [25, 30, 35, 28, 22, 40, 33, 26, 29, 31],\n    'City': ['New York', 'Los Angeles', 'Chicago', 'New York', 'Los Angeles', 'San Francisco', 'Chicago', 'Boston', 'New York', 'Boston']\n}\n\n# Creating the DataFrame\ndf = pd.DataFrame(data)\ndf\n# Viewing basic information about the DataFrame\nprint(df.info())\n\n\n5.4.2 Viewing Statistical Summaries\nThe describe() method generates descriptive statistics that summarize the central tendency, dispersion, and shape of the data distribution, excluding NaN values.\n# Viewing statistical summary of numerical columns\nprint(df.describe())\nYou can also include non-numerical data by specifying include='all'.\n# Including all columns, including non-numeric\nprint(df.describe(include='all'))\n\n\n5.4.3 Counting Unique Values\nThe value_counts() function counts the unique values in a column, which is particularly useful for categorical data.\n# Counting unique values in a column\nprint(df['City'].value_counts())\n\n\n5.4.4 Displaying First and Last Rows\nYou can use head() and tail() to view the first and last few rows of the DataFrame.\n# Displaying the first 5 rows\nprint(df.head())\n\n# Displaying the last 5 rows\nprint(df.tail())",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Analysis in Python</span>"
    ]
  },
  {
    "objectID": "05.pandas.html#handling-missing-data-1",
    "href": "05.pandas.html#handling-missing-data-1",
    "title": "5  Data Analysis in Python",
    "section": "5.5 Handling Missing Data",
    "text": "5.5 Handling Missing Data\nHandling missing data is a crucial aspect of data cleaning, as missing values can lead to incorrect analysis results.\n\n5.5.1 Identifying Missing Data\nUse isna() or isnull() to detect missing values. These methods return a DataFrame of booleans indicating the presence of missing values.\n# Creating a DataFrame with missing values for demonstrating missing data handling\ndata_with_missing = {\n    'Name': ['Alice', 'Bob', 'Charlie', 'David', 'Emma', 'Frank', 'Grace', 'Hannah', 'Ian', 'Jack'],\n    'Age': [25, 30, None, 28, 22, 40, 33, None, 29, 31],  # Introducing missing values in 'Age'\n    'City': ['New York', 'Los Angeles', 'Chicago', None, 'Los Angeles', 'San Francisco', 'Chicago', 'Boston', 'New York', None]  # Introducing missing values in 'City'\n}\n\n# Creating the DataFrame with missing data\ndf_missing = pd.DataFrame(data_with_missing)\ndf_missing\n# Identifying missing data\nprint(df_missing.isna())\n\n# Counting missing values in each column\nprint(df_missing.isna().sum())\n\n\n5.5.2 Removing Missing Data\nYou can remove rows or columns with missing values using the dropna() method.\n# Dropping rows with any missing values\ndf_cleaned1 = df_missing.dropna()\nprint(df_cleaned1)\n\n# Dropping columns with missing values\ndf_cleaned2 = df_missing.dropna(axis=1)\nprint(df_cleaned2)\n\n\n5.5.3 Filling Missing Data\nInstead of dropping missing data, you can fill it with specified values using fillna(). Common strategies include filling with zeros, the mean, or forward/backward filling.\n# Filling missing values with a specified value\ndf_missing['Age'].fillna(0)\n\n# Filling missing values with the column mean\ndf_missing['Age'].fillna(df_missing['Age'].mean())\ndf_missing\n\n# Forward filling missing values\ndf_missing.fillna(method='ffill')",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Analysis in Python</span>"
    ]
  },
  {
    "objectID": "05.pandas.html#working-with-indices",
    "href": "05.pandas.html#working-with-indices",
    "title": "5  Data Analysis in Python",
    "section": "5.6 Working with Indices",
    "text": "5.6 Working with Indices\nThe index of a DataFrame is used to uniquely identify rows. Managing indices effectively allows for more efficient data manipulation and retrieval.\n\n5.6.1 Setting and Resetting Index\nYou can set a column as the index using set_index() and reset it back to the default integer index using reset_index().\n# Setting 'Name' as the index\ndf.set_index('Name', inplace=True)\nprint(df)\n\n# Resetting the index to default\ndf.reset_index(inplace=True)\nprint(df)\n\n\n5.6.2 Sorting by Index\nThe sort_index() method sorts the DataFrame by its index.\n# Sorting the DataFrame by index\ndf.sort_index(inplace=True)\nprint(df)\n\n\n5.6.3 Changing the Index Name\nYou can rename the index by assigning a new name directly.\n# Renaming the index\ndf.index.name = 'ID'\nprint(df)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Analysis in Python</span>"
    ]
  },
  {
    "objectID": "05.pandas.html#filtering-data",
    "href": "05.pandas.html#filtering-data",
    "title": "5  Data Analysis in Python",
    "section": "5.7 Filtering Data",
    "text": "5.7 Filtering Data\nFiltering allows you to select rows based on specific conditions, enabling targeted analysis and manipulation.\n\n5.7.1 Boolean Indexing\nBoolean indexing uses conditional statements to filter data.\n# Filtering rows where Age &gt; 30\nfiltered_df = df[df['Age'] &gt; 30]\nprint(filtered_df)\n\n\n5.7.2 Using isin() Method\nThe isin() method filters rows based on multiple values in a column.\n# Filtering rows where Name is Alice or Bob\nfiltered_df = df[df['Name'].isin(['Alice', 'Bob'])]\nprint(filtered_df)\n\n\n5.7.3 Filtering with Multiple Conditions\nYou can combine multiple conditions using & (and) or | (or).\n# Filtering with multiple conditions\nfiltered_df = df[(df['Age'] &gt; 30) & (df['City'] == 'Chicago')]\nprint(filtered_df)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Analysis in Python</span>"
    ]
  },
  {
    "objectID": "05.pandas.html#merging-dataframes-1",
    "href": "05.pandas.html#merging-dataframes-1",
    "title": "5  Data Analysis in Python",
    "section": "5.8 Merging DataFrames",
    "text": "5.8 Merging DataFrames\nMerging DataFrames is an essential operation when working with multiple datasets that need to be combined. Pandas offers several functions for merging, joining, and concatenating DataFrames.\n\n5.8.1 Using concat() Function\nThe concat() function concatenates DataFrames along rows (default) or columns. It is useful when you have DataFrames with the same columns and want to stack them.\nimport pandas as pd\n\n# Creating sample DataFrames\ndf1 = pd.DataFrame({'Name': ['Alice', 'Bob'], 'Age': [25, 30]})\ndf2 = pd.DataFrame({'Name': ['Charlie', 'David'], 'Age': [35, 40]})\n\n# Concatenating DataFrames vertically\ndf_concat = pd.concat([df1, df2], ignore_index=True)\nprint(df_concat)\n\n# Output:\n#       Name  Age\n# 0    Alice   25\n# 1      Bob   30\n# 2  Charlie   35\n# 3    David   40\nYou can also concatenate DataFrames horizontally by specifying axis=1.\n# Concatenating DataFrames horizontally\ndf_concat_horizontal = pd.concat([df1, df2], axis=1)\nprint(df_concat_horizontal)\n\n\n5.8.2 Using merge() Function\nThe merge() function combines two DataFrames based on a key column or index. It works similarly to SQL JOIN operations (inner, outer, left, right).\n# Creating sample DataFrames for merging\ndf1 = pd.DataFrame({'Name': ['Alice', 'Bob'], 'Score': [85, 90]})\ndf2 = pd.DataFrame({'Name': ['Alice', 'Bob'], 'Grade': ['A', 'B']})\n\n# Merging DataFrames on the 'Name' column\ndf_merged = pd.merge(df1, df2, on='Name', how='inner')\nprint(df_merged)\n\n# Output:\n#     Name  Score Grade\n# 0  Alice     85     A\n# 1    Bob     90     B\n\n5.8.2.1 SQL Join Methods\n\n5.8.2.1.1 1. Inner Join\nAn inner join returns rows when there is a match in both tables.\n\n\n5.8.2.1.2 2. Left Join (or Left Outer Join)\nA left join returns all rows from the left table, and the matched rows from the right table. If no match is found, the result is NULL on the right side.\n\n\n5.8.2.1.3 3. Right Join (or Right Outer Join)\nA right join returns all rows from the right table, and the matched rows from the left table. If no match is found, the result is NULL on the left side.\n\n\n5.8.2.1.4 4. Full Join (or Full Outer Join)\nA full join returns rows when there is a match in either table. It returns all rows from both tables and fills NULL where there is no match.\n\n\n\n\n5.8.2.2 Diagram of SQL Joins:\n1. INNER JOIN:\n+--------+     +--------+\n|  LEFT  |     |  RIGHT |\n+--------+     +--------+\n   +--------+     +--------+\n   | common |     | common |\n   +--------+     +--------+\n   \n2. LEFT JOIN:\n+--------+     +--------+\n|  LEFT  |     |  RIGHT |\n+--------+     +--------+\n   +--------+     +--------+\n   | all    |     | common |\n   | from   |     +--------+\n   | left   |\n   +--------+\n\n3. RIGHT JOIN:\n+--------+     +--------+\n|  LEFT  |     |  RIGHT |\n+--------+     +--------+\n   +--------+     +--------+\n   | common |     | all    |\n   +--------+     | from   |\n                  | right  |\n                  +--------+\n\n4. FULL JOIN:\n+--------+     +--------+\n|  LEFT  |     |  RIGHT |\n+--------+     +--------+\n   +--------+     +--------+\n   | all    |     | all    |\n   | from   |     | from   |\n   | both   |     | both   |\n   +--------+     +--------+\n\n\n\n5.8.2.3 Python Example with DataFrames\nLet’s create two DataFrames and perform different types of joins using the merge() function in pandas, which simulates SQL joins.\nimport pandas as pd\n\n# Creating two DataFrames\ndf1 = pd.DataFrame({\n    'Name': ['Alice', 'Bob', 'Charlie', 'David'],\n    'Age': [25, 30, 35, 40]\n})\n\ndf2 = pd.DataFrame({\n    'Name': ['Alice', 'Charlie', 'Eve'],\n    'City': ['New York', 'Chicago', 'Los Angeles']\n})\n\n# Display the DataFrames\nprint(\"DataFrame 1:\")\nprint(df1)\nprint(\"DataFrame 2:\")\nprint(df2)\n\n# Inner Join\ninner_join = pd.merge(df1, df2, on='Name', how='inner')\nprint(\"\\nInner Join:\")\nprint(inner_join)\n\n# Left Join\nleft_join = pd.merge(df1, df2, on='Name', how='left')\nprint(\"\\nLeft Join:\")\nprint(left_join)\n\n# Right Join\nright_join = pd.merge(df1, df2, on='Name', how='right')\nprint(\"\\nRight Join:\")\nprint(right_join)\n\n# Full Outer Join\nfull_join = pd.merge(df1, df2, on='Name', how='outer')\nprint(\"\\nFull Outer Join:\")\nprint(full_join)\n\n\n\n5.8.3 Using join() Function\nThe join() function is used to join two DataFrames based on their indices. It is particularly useful for merging DataFrames with overlapping indices.\n# Creating sample DataFrames with indices\ndf1 = pd.DataFrame({'Score': [85, 90]}, index=['Alice', 'Bob'])\ndf2 = pd.DataFrame({'Grade': ['A', 'B']}, index=['Alice', 'Bob'])\n\n# Joining DataFrames on indices\ndf_joined = df1.join(df2)\nprint(df_joined)\n\n# Output:\n#        Score Grade\n# Alice     85     A\n# Bob       90     B",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Analysis in Python</span>"
    ]
  },
  {
    "objectID": "05.pandas.html#reshaping-data",
    "href": "05.pandas.html#reshaping-data",
    "title": "5  Data Analysis in Python",
    "section": "5.9 Reshaping Data",
    "text": "5.9 Reshaping Data\nReshaping allows you to change the layout of a DataFrame, making it suitable for different types of analysis.\n\n5.9.1 Using melt() Function\nThe melt() function unpivots a DataFrame from a wide format to a long format, making it easier to analyze and visualize.\n# Creating a sample DataFrame\ndf = pd.DataFrame({\n    'Name': ['Alice', 'Bob'],\n    'Math': [90, 80],\n    'Science': [85, 95]\n})\n\n# Melting the DataFrame\ndf_melted = pd.melt(df, id_vars=['Name'], value_vars=['Math', 'Science'], \n                    var_name='Subject', value_name='Score')\nprint(df_melted)\n\n# Output:\n#     Name  Subject  Score\n# 0  Alice     Math     90\n# 1    Bob     Math     80\n# 2  Alice  Science     85\n# 3    Bob  Science     95\n\n\n5.9.2 Using pivot_table() Function\nThe pivot_table() function reshapes data by creating a new summary table, which is useful for aggregating data.\n# Creating a DataFrame for pivoting\ndf = pd.DataFrame({\n    'Name': ['Alice', 'Bob', 'Alice', 'Bob'],\n    'Subject': ['Math', 'Math', 'Science', 'Science'],\n    'Score': [90, 80, 85, 95]\n})\n\nprint(df)\nprint('---')\n\n# Pivoting the DataFrame\ndf_pivot = df.pivot_table(values='Score', index='Name', columns='Subject', \n                          aggfunc='mean')\nprint(df_pivot)\n\n# Output:\n# Subject  Math  Science\n# Name                  \n# Alice      90       85\n# Bob        80       95\n\n\n5.9.3 Using stack() and unstack()\nstack() compresses columns into rows, while unstack() does the opposite, expanding rows into columns.\n# Stacking and unstacking a DataFrame\ndf = pd.DataFrame({\n    'A': [1, 2],\n    'B': [3, 4]\n}, index=['X', 'Y'])\n\n# Stacking the DataFrame\nstacked = df.stack()\nprint(stacked)\n\n# Unstacking the stacked DataFrame\nunstacked = stacked.unstack()\nprint(unstacked)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Analysis in Python</span>"
    ]
  },
  {
    "objectID": "05.pandas.html#applying-functions-to-dataframes",
    "href": "05.pandas.html#applying-functions-to-dataframes",
    "title": "5  Data Analysis in Python",
    "section": "5.10 Applying Functions to DataFrames",
    "text": "5.10 Applying Functions to DataFrames\nApplying functions to DataFrames allows you to transform data easily, using built-in or custom functions.\n\n5.10.1 Applying Functions to Series\nThe apply() function applies a function along an axis of the DataFrame.\n# Creating a sample DataFrame\ndf = pd.DataFrame({\n    'A': [1, 2, 3],\n    'B': [4, 5, 6]\n})\n\n# Applying a function to each column\ndf_squared = df.apply(lambda x: x ** 2)\nprint(df_squared)\n\n# Output:\n#    A   B\n# 0  1  16\n# 1  4  25\n# 2  9  36\n\n# Applying a function to each column\n# def square(x):\n#     y = x ** 2\n#     return y\n\n# df_squared = df.apply(square)\n\n\n5.10.2 Applying Functions to Entire DataFrames\nThe applymap() function applies a function to each element of the DataFrame.\n# Applying a function to each element\ndf_negated = df.applymap(lambda x: -x)\nprint(df_negated)\n\n# Output:\n#    A  B\n# 0 -1 -4\n# 1 -2 -5\n# 2 -3 -6",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Analysis in Python</span>"
    ]
  },
  {
    "objectID": "06.arithmetic.html",
    "href": "06.arithmetic.html",
    "title": "6  Basic Arithmetic Operations in Python",
    "section": "",
    "text": "6.1 Simple Examples with Numbers",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Basic Arithmetic Operations in Python</span>"
    ]
  },
  {
    "objectID": "06.arithmetic.html#simple-examples-with-numbers",
    "href": "06.arithmetic.html#simple-examples-with-numbers",
    "title": "6  Basic Arithmetic Operations in Python",
    "section": "",
    "text": "6.1.1 Addition (+)\nThe addition operator adds two numbers together.\na = 10\nb = 5\nresult = a + b\nprint(result)  # Output: 15\n\n\n6.1.2 Subtraction (-)\nThe subtraction operator subtracts the second number from the first.\na = 10\nb = 5\nresult = a - b\nprint(result)  # Output: 5\n\n\n6.1.3 Multiplication (*)\nThe multiplication operator multiplies two numbers.\na = 10\nb = 5\nresult = a * b\nprint(result)  # Output: 50\n\n\n6.1.4 Division (/)\nThe division operator divides the first number by the second.\na = 10\nb = 5\nresult = a / b\nprint(result)  # Output: 2.0\n\n\n6.1.5 Modulus (%)\nThe modulus operator returns the remainder of the division of the first number by the second.\na = 10\nb = 3\nresult = a % b\nprint(result)  # Output: 1\n\n6.1.5.1 Example Explanation\nIn this example, 10 % 3 computes the remainder when 10 is divided by 3, which is 1.\n\n\n\n6.1.6 Exponentiation (**)\nThe exponentiation operator raises the first number to the power of the second.\na = 2\nb = 3\nresult = a ** b\nprint(result)  # Output: 8\n\n\n6.1.7 Floor Division (//)\nThe floor division operator divides and returns the largest integer less than or equal to the result.\na = 10\nb = 3\nresult = a // b\nprint(result)  # Output: 3\n\n\n6.1.8 divmod() Function\nThe divmod() function returns a tuple containing the quotient and the remainder when dividing two numbers.\na = 10\nb = 3\nquotient, remainder = divmod(a, b)\nprint(f\"Quotient: {quotient}, Remainder: {remainder}\")\n# Output: Quotient: 3, Remainder: 1\n\n6.1.8.1 Example Explanation\nHere, divmod(10, 3) returns (3, 1), where 3 is the quotient and 1 is the remainder when 10 is divided by 3.\n\n\n\n6.1.9 Summary Table\n\n\n\n\n\n\n\n\n\nOperator/Function\nDescription\nExample\nOutput\n\n\n\n\n+\nAddition\n10 + 5\n15\n\n\n-\nSubtraction\n10 - 5\n5\n\n\n*\nMultiplication\n10 * 5\n50\n\n\n/\nDivision\n10 / 5\n2.0\n\n\n%\nModulus (Remainder)\n10 % 3\n1\n\n\n**\nExponentiation\n2 ** 3\n8\n\n\n//\nFloor Division\n10 // 3\n3\n\n\ndivmod()\nQuotient and Remainder as a Tuple\ndivmod(10, 3)\n(3, 1)",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Basic Arithmetic Operations in Python</span>"
    ]
  },
  {
    "objectID": "06.arithmetic.html#advanced-examples-with-dataframes",
    "href": "06.arithmetic.html#advanced-examples-with-dataframes",
    "title": "6  Basic Arithmetic Operations in Python",
    "section": "6.2 Advanced Examples with DataFrames",
    "text": "6.2 Advanced Examples with DataFrames\nIn data analysis, we often work with tabular data. The pandas library provides a powerful DataFrame object to handle such data. Arithmetic operations can be applied to DataFrame objects directly, enabling efficient data manipulation.\n\n6.2.1 Setting Up the Environment\nFirst, install and import the necessary library:\n# Install pandas if not already installed\n# !pip install pandas\n\nimport pandas as pd\n\n\n6.2.2 Creating a DataFrame\nLet’s create a simple DataFrame for demonstration:\ndata = {\n    'A': [10, 20, 30],\n    'B': [1, 2, 3],\n    'C': [5, 4, 3]\n}\ndf = pd.DataFrame(data)\nprint(df)\nOutput:\n    A  B  C\n0  10  1  5\n1  20  2  4\n2  30  3  3\n\n\n6.2.3 Adding a Constant to a Column\nYou can add a constant value to an entire column:\ndf['A_plus_10'] = df['A'] + 10\nprint(df)\nOutput:\n    A  B  C  A_plus_10\n0  10  1  5         20\n1  20  2  4         30\n2  30  3  3         40\n\n\n6.2.4 Column-Wise Operations\nPerform arithmetic operations between columns:\ndf['A_minus_C'] = df['A'] - df['C']\nprint(df)\nOutput:\n    A  B  C  A_plus_10  A_minus_C\n0  10  1  5         20          5\n1  20  2  4         30         16\n2  30  3  3         40         27\n\n\n6.2.5 Using Modulus Operator with DataFrames\nApply the modulus operator to a DataFrame column to get the remainder of division element-wise:\ndf['A_mod_3'] = df['A'] % 3\nprint(df)\nOutput:\n    A  B  C  A_plus_10  A_minus_C  A_mod_3\n0  10  1  5         20          5        1\n1  20  2  4         30         16        2\n2  30  3  3         40         27        0\n\n6.2.5.1 Example Explanation\n\nFor row 0: 10 % 3 equals 1.\nFor row 1: 20 % 3 equals 2.\nFor row 2: 30 % 3 equals 0.\n\n\n\n\n6.2.6 Using divmod() with DataFrames\nWhile divmod() isn’t directly applicable to DataFrame columns, you can achieve similar results using apply along with a lambda function:\ndf[['Quotient', 'Remainder']] = df['A'].apply(lambda x: pd.Series(divmod(x, 3)))\nprint(df)\nOutput:\n    A  B  C  A_plus_10  A_minus_C  A_mod_3  Quotient  Remainder\n0  10  1  5         20          5        1         3          1\n1  20  2  4         30         16        2         6          2\n2  30  3  3         40         27        0        10          0\n\n6.2.6.1 Example Explanation\n\nFor row 0: divmod(10, 3) returns (3, 1).\nFor row 1: divmod(20, 3) returns (6, 2).\nFor row 2: divmod(30, 3) returns (10, 0).\n\n\n\n\n6.2.7 Row-Wise Operations\nUse the sum function to perform operations across rows:\ndf['Sum_A_B_C'] = df[['A', 'B', 'C']].sum(axis=1)\nprint(df)\nOutput:\n    A  B  C  A_plus_10  A_minus_C  A_mod_3  Quotient  Remainder  Sum_A_B_C\n0  10  1  5         20          5        1         3          1         16\n1  20  2  4         30         16        2         6          2         26\n2  30  3  3         40         27        0        10          0         36\n\n\n6.2.8 Applying Functions\nApply a function to a column:\ndf['A_squared'] = df['A'].apply(lambda x: x ** 2)\nprint(df)\nOutput:\n    A  B  C  A_plus_10  A_minus_C  A_mod_3  Quotient  Remainder  Sum_A_B_C  A_squared\n0  10  1  5         20          5        1         3          1         16        100\n1  20  2  4         30         16        2         6          2         26        400\n2  30  3  3         40         27        0        10          0         36        900\n\n\n6.2.9 Conditional Operations\nUse conditions to modify data:\ndf['A_gt_15'] = df['A'] &gt; 15\nprint(df)\nOutput:\n    A  B  C  A_plus_10  A_minus_C  A_mod_3  Quotient  Remainder  Sum_A_B_C  A_squared  A_gt_15\n0  10  1  5         20          5        1         3          1         16        100    False\n1  20  2  4         30         16        2         6          2         26        400     True\n2  30  3  3         40         27        0        10          0         36        900     True\n\n\n6.2.10 Handling Missing Data\nArithmetic operations handle missing data (NaN) gracefully:\ndf.loc[1, 'B'] = None  # Introduce a NaN value\ndf['B_plus_2'] = df['B'] + 2\nprint(df)\nOutput:\n    A    B  C  A_plus_10  A_minus_C  A_mod_3  Quotient  Remainder  Sum_A_B_C  A_squared  A_gt_15  B_plus_2\n0  10  1.0  5         20          5        1         3          1       16.0        100    False       3.0\n1  20  NaN  4         30         16        2         6          2        NaN        400     True       NaN\n2  30  3.0  3         40         27        0        10          0       36.0        900     True       5.0\nThis code snippet demonstrates how to introduce a missing value (NaN) into a DataFrame and perform an arithmetic operation on a column that contains NaN values. Let me break down the key points:\n\n\n6.2.11 Explanation of Code:\n\nIntroducing a Missing Value:\ndf.loc[1, 'B'] = None  # Introduce a NaN value\nThis line sets the value at row index 1 in column 'B' to None, which pandas treats as NaN (Not a Number). This introduces a missing value in the DataFrame.\nPerforming Arithmetic with NaN:\ndf['B_plus_2'] = df['B'] + 2\nThis line creates a new column, 'B_plus_2', by adding 2 to the values in column 'B'. When you perform arithmetic with NaN in pandas, the result for that specific operation is also NaN. This is why in the resulting DataFrame, the row with NaN in column 'B' also has NaN in the new column 'B_plus_2'.\n\n\n\n6.2.12 Grouped Operations\nPerform operations on grouped data:\ndf['Group'] = ['X', 'Y', 'X']\n\nprint(df)\nprint('---')\n\ngrouped = df.groupby('Group')\nsum_per_group = grouped['A'].sum()\nprint(sum_per_group)\nOutput:\nGroup\nX    40\nY    20\nName: A, dtype: int64",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Basic Arithmetic Operations in Python</span>"
    ]
  },
  {
    "objectID": "07.Regular_Expression.html",
    "href": "07.Regular_Expression.html",
    "title": "7  Regular Expressions (RegEx) in Python",
    "section": "",
    "text": "7.1 Basics of Regular Expressions\nHere are some basic symbols in regular expressions:",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Regular Expressions (RegEx) in Python</span>"
    ]
  },
  {
    "objectID": "07.Regular_Expression.html#basics-of-regular-expressions",
    "href": "07.Regular_Expression.html#basics-of-regular-expressions",
    "title": "7  Regular Expressions (RegEx) in Python",
    "section": "",
    "text": ". : Matches any single character except newline.\n^ : Asserts the start of a string.\n$ : Asserts the end of a string.\n[]: Defines a set of characters. For example, [abc] matches either “a”, “b”, or “c”.\n\\d: Matches any digit (equivalent to [0-9]).\n\\w: Matches any word character (alphanumeric and underscore, equivalent to [a-zA-Z0-9_]).\n\\s: Matches any whitespace character (spaces, tabs, etc.).\n*: Matches 0 or more occurrences of the preceding element.\n+: Matches 1 or more occurrences.\n?: Matches 0 or 1 occurrence.\n{n,m}: Matches between n and m occurrences of the preceding element.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Regular Expressions (RegEx) in Python</span>"
    ]
  },
  {
    "objectID": "07.Regular_Expression.html#findall-finding-all-matches",
    "href": "07.Regular_Expression.html#findall-finding-all-matches",
    "title": "7  Regular Expressions (RegEx) in Python",
    "section": "7.2 findall(): Finding All Matches",
    "text": "7.2 findall(): Finding All Matches\nThe findall() function in Python returns a list of all matches for a given pattern.\nimport re\n\ndata = \"1234 abc가나다ABC_555_6 13 43435 2213433577869 23ab\"\n\n# Finding single digits\nprint(re.findall(\"[0-9]\", data))  # ['1', '2', '3', '4', '5', '5', '5', ...]\n\n# Finding sequences of digits\nprint(re.findall(\"[0-9]+\", data))  # ['1234', '555', '6', '13', '43435', ...]\n\n# Finding exactly two-digit numbers\nprint(re.findall(\"[0-9]{2}\", data))  # ['12', '34', '55', '13', '43', ...]\n\n# Finding numbers with 2 to 6 digits\nprint(re.findall(\"[0-9]{2,6}\", data))  # ['1234', '555', '13', '43435', ...]",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Regular Expressions (RegEx) in Python</span>"
    ]
  },
  {
    "objectID": "07.Regular_Expression.html#matching-specific-patterns",
    "href": "07.Regular_Expression.html#matching-specific-patterns",
    "title": "7  Regular Expressions (RegEx) in Python",
    "section": "7.3 Matching Specific Patterns",
    "text": "7.3 Matching Specific Patterns\ndata = \"1234 abc가나다ABC_555_6 mbc kbs sbs 58672 newsline kkbc dreamair air airline air\"\n\n# Finding patterns that start with 'a' followed by two characters\nprint(re.findall(\"a..\", data))  # ['abc', 'air', 'air']\n\n# Finding strings that end with \"air\"\nprint(re.findall(\"air$\", data))  # ['air']",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Regular Expressions (RegEx) in Python</span>"
    ]
  },
  {
    "objectID": "07.Regular_Expression.html#finding-numeric-patterns",
    "href": "07.Regular_Expression.html#finding-numeric-patterns",
    "title": "7  Regular Expressions (RegEx) in Python",
    "section": "7.4 Finding Numeric Patterns",
    "text": "7.4 Finding Numeric Patterns\n\\d is used to represent any digit, and it is one of the most commonly used patterns.\ndata = \"johnson 80, Bong 100, David 50\"\nprint(re.findall(\"\\d\", data))  # ['8', '0', '1', '0', '0', '5', '0']\nprint(re.findall(\"\\d{2}\", data))  # ['80', '10', '00', '50']",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Regular Expressions (RegEx) in Python</span>"
    ]
  },
  {
    "objectID": "07.Regular_Expression.html#split-splitting-strings",
    "href": "07.Regular_Expression.html#split-splitting-strings",
    "title": "7  Regular Expressions (RegEx) in Python",
    "section": "7.5 split(): Splitting Strings",
    "text": "7.5 split(): Splitting Strings\nRegular expressions can be used to split strings based on various patterns.\n# Splitting based on space (default)\nprint(\"mbc,kbs sbs:ytn\".split())  # ['mbc,kbs', 'sbs:ytn']\n\n# Splitting based on commas\nprint(\"mbc,kbs sbs:ytn\".split(\",\"))  # ['mbc', 'kbs sbs:ytn']\n\n# Splitting using a regex pattern (\\W means non-alphanumeric characters)\nprint(re.split(\"\\W+\", \"mbc,kbs sbs:ytn\"))  # ['mbc', 'kbs', 'sbs', 'ytn']",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Regular Expressions (RegEx) in Python</span>"
    ]
  },
  {
    "objectID": "07.Regular_Expression.html#sub-substituting-strings",
    "href": "07.Regular_Expression.html#sub-substituting-strings",
    "title": "7  Regular Expressions (RegEx) in Python",
    "section": "7.6 sub(): Substituting Strings",
    "text": "7.6 sub(): Substituting Strings\nThe sub() function is used to replace parts of a string that match a pattern with another string.\nnumber = \"1234 abc가나다ABC_567_34234\"\nprint(number)  # Original string\n\n# Replacing any digits with '888'\nm = re.sub(\"[0-9]+\", \"888\", number)\nprint(m)  # '888 abc가나다ABC_888_888'",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Regular Expressions (RegEx) in Python</span>"
    ]
  },
  {
    "objectID": "07.Regular_Expression.html#text-preprocessing-examples",
    "href": "07.Regular_Expression.html#text-preprocessing-examples",
    "title": "7  Regular Expressions (RegEx) in Python",
    "section": "7.7 Text Preprocessing Examples",
    "text": "7.7 Text Preprocessing Examples\nThank you for sharing the image of your Excel data. To help you use this as an example in your regular expression textbook, we can focus on several key aspects of preprocessing this text. Here are some steps we could cover:\n\n7.7.1 Removing Special Characters\nThe text contains placeholders like “@@@” and non-standard line breaks, which can be removed or standardized. For instance:\n\nRegex Pattern: @@@|\\n\n\nThis pattern will match both the placeholder and newlines.\n\n\nimport re\n\ntext = \"Hello. I'm @@@.\\nToday I brought that bear doll.\\nThis is a bear doll that I like...\\n thank you. Thank you all.\"\nclean_text = re.sub(r\"@@@|\\n\", \"\", text)\nprint(clean_text)\n\n\n7.7.2 Lowercasing Text\nUniformity in text processing often requires converting all text to lowercase.\n\nRegex or Method: .lower()\n\nlower_text = clean_text.lower()\n\n\n7.7.3 Correcting Simple Typos\nThe text contains phrases like “thank you.” where punctuation might need correction. We can use regex to identify incorrect casing or punctuation.\n\nRegex Pattern: [Tt]hank you[.,]\n\nfixed_text = re.sub(r\"[Tt]hank you[.,]?\", \"Thank you.\", lower_text)\n\n\n7.7.4 Identifying Sentences or Key Phrases\nIf you want to extract specific phrases or sentences for further analysis:\n\nRegex Pattern: r\"\\b(?:brought|present|special)\\b\"\n\nmatches = re.findall(r\"\\b(?:brought|present|special)\\b\", fixed_text)\nprint(matches)  # Extracts words like 'brought', 'present', 'special'\n\n\n7.7.5 Sentence Tokenization\nYou can split sentences based on punctuation using regex:\n\nRegex Pattern: r\"[.!?]\\s+\"\n\nsentences = re.split(r\"[.!?]\\s+\", fixed_text)",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Regular Expressions (RegEx) in Python</span>"
    ]
  },
  {
    "objectID": "07.Regular_Expression.html#apply-regular-expression-in-dataframe",
    "href": "07.Regular_Expression.html#apply-regular-expression-in-dataframe",
    "title": "7  Regular Expressions (RegEx) in Python",
    "section": "7.8 Apply Regular Expression in DataFrame",
    "text": "7.8 Apply Regular Expression in DataFrame\n\n7.8.1 Step 1: Create a DataFrame\nFirst, we create a DataFrame from the sample data that was visible in the image.\nimport pandas as pd\n\n# Sample data based on the image\ndata = {\n    'NO': [1, 2, 3],\n    'Text_Typo': [\n        \"Hello. I'm @@@.\\nToday I brought that bear doll.\\nThis is a bear doll that I like.\\nThis bear doll is special to me because my aunt gave it to me.\\nI remember that it was my birthday present from my aunt.\\nI love my aunt.\\nThank you for listening to my presentation.\",\n        \"Hello, I'm @@@. Today I brought a baseball bat. This is a bat that makes me funny. This is special to me because I wanted to be a baseball player when I was a child. And I was a baseball player in baseball club. I remember that I played baseball with my friend. I was very happy at that time. Thank you.\",\n        \"Hello, I'm @@@. Let me show you this music box. It's special to me because it's my first music box. I was interested in the music.\"\n    ]\n}\n\n# Create DataFrame\ndf = pd.DataFrame(data)\nprint(df)\n\n\n7.8.2 Step 2: Apply Regular Expressions\nLet’s apply some regular expressions to clean up the text.\n\nRemove the placeholder “@@@” and extra line breaks \\n.\nLowercase the text for uniformity.\nReplace certain common typos like missing or incorrect punctuation around “thank you”.\nExtract key sentences using regular expressions.\n\n\n7.8.2.1 Applying Regular Expression to Each Row\nimport re\n\n# Function to clean text\ndef clean_text(text):\n    # Remove the placeholder and line breaks\n    text = re.sub(r\"@@@|\\n\", \"\", text)\n    \n    # Fix common typo with \"thank you\"\n    text = re.sub(r\"[Tt]hank you[.,]?\", \"Thank you.\", text)\n    \n    # Convert to lowercase\n    text = text.lower()\n    \n    return text\n\n# Apply the function to each row in the 'Text_Typo' column\ndf['Cleaned_Text'] = df['Text_Typo'].apply(clean_text)\n\n# Display the updated DataFrame\ndf[['NO', 'Text_Typo','Cleaned_Text']]\n\n\n\n7.8.3 Step 3: Extracting Specific Patterns\nIf you want to extract certain patterns like sentences containing the word “special” or phrases where the speaker brought an item, you can do this with regex as well.\n\n7.8.3.1 Extract Sentences with the Word “Special”\n# Function to extract sentences with the word \"special\"\ndef extract_special_sentences(text):\n    # Split sentences based on punctuation\n    sentences = re.split(r\"[.!?]\\s+\", text)\n    \n    # Find sentences containing the word \"special\"\n    special_sentences = [sent for sent in sentences if re.search(r\"\\bspecial\\b\", sent)]\n\n    ## FULL CODE ##\n    # special_sentences = []\n    # for sent in sentences:\n    #     if re.search(r\"\\bspecial\\b\", sent):\n    #         special_sentences.append(sent)    \n    \n    return special_sentences\n\n# Apply the function to extract sentences with \"special\"\ndf['Special_Sentences'] = df['Cleaned_Text'].apply(extract_special_sentences)\n\n# Display the DataFrame with extracted sentences\ndf[['NO', 'Text_Typo','Cleaned_Text', 'Special_Sentences']]\nThis DataFrame now contains cleaned text and a column with sentences that mention the word “special.”",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Regular Expressions (RegEx) in Python</span>"
    ]
  },
  {
    "objectID": "07.Regular_Expression.html#regular-expression-practice-website",
    "href": "07.Regular_Expression.html#regular-expression-practice-website",
    "title": "7  Regular Expressions (RegEx) in Python",
    "section": "7.9 Regular Expression Practice Website",
    "text": "7.9 Regular Expression Practice Website\nRegexr.com is an interactive website for practicing and learning regular expressions (regex). It offers tools to create and test regex patterns, along with explanations, a pattern library, and a reference guide. The site is suitable for both beginners and advanced users, providing real-time feedback and community-contributed examples to enhance learning.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Regular Expressions (RegEx) in Python</span>"
    ]
  },
  {
    "objectID": "08.Custom_Corpus.html",
    "href": "08.Custom_Corpus.html",
    "title": "8  Building a Custom Corpus",
    "section": "",
    "text": "8.1 Procedure",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Building a Custom Corpus</span>"
    ]
  },
  {
    "objectID": "08.Custom_Corpus.html#procedure",
    "href": "08.Custom_Corpus.html#procedure",
    "title": "8  Building a Custom Corpus",
    "section": "",
    "text": "8.1.1 Define the Purpose\nBefore starting, clearly define the purpose of your corpus. For instance:\n\nLanguage Analysis: Are you studying phonetics, syntax, or semantics?\nMachine Learning/NLP: Do you need a training dataset for sentiment analysis, text classification, or language modeling?\nDomain-specific Corpus: Are you focusing on medical, legal, academic, or informal conversational language?\n\n\n\n8.1.2 Decide on Data Sources\nDetermine where to source the data from:\n\nWeb scraping: Use tools like BeautifulSoup or Scrapy to extract text from websites.\nBooks, Papers, Articles: Digitize and preprocess text from publications.\nSocial Media/Forums: Use APIs like Twitter, Reddit, or specific domain forums.\nTranscriptions: If you need spoken language, consider using speech-to-text services to create transcripts.\nExisting Corpora: You can augment your corpus with publicly available ones like the Brown Corpus, Gutenberg Project, or specialized databases.\n\n\n\n8.1.3 Collect and Store the Data\n\nWeb Scraping/Downloading: Use web crawlers, APIs, or manual downloads to gather the raw text data. Make sure to follow ethical guidelines, including copyright considerations.\nDatabase/Spreadsheet: Store collected texts in an organized format (e.g., CSV, JSON, or a relational database).\nTagging Metadata: Include useful metadata (e.g., author, publication date, text genre, and source) to allow for targeted corpus queries.\n\n\n\n8.1.4 Data Cleaning & Preprocessing\nRaw text data needs to be cleaned and prepared for analysis:\n\nTokenization: Break the text into sentences and words.\nLowercasing: Convert all text to lowercase (if case sensitivity isn’t required).\nRemove Stopwords/Punctuation: You may want to exclude common words (like ‘and’, ‘the’) or punctuation, depending on the goal.\nNormalization: Convert numbers, dates, and other elements into consistent formats.\nHandling Special Characters: Replace or remove non-ASCII characters, HTML tags, or other artifacts from the data source.\nLemmatization/Stemming: Reduce words to their base forms for consistency (e.g., running -&gt; run).\n\n\n\n8.1.5 Data Annotation (Optional)\n\nIf you need a labeled dataset, annotate the text with additional information, such as:\nPOS Tags: Parts of Speech tagging for grammatical structure.\nNamed Entities: Tag entities like persons, locations, organizations.\nSentiment or Categories: Manually label texts for sentiment, emotions, or specific classification tasks.\nYou can use annotation tools like Brat, Prodigy, or Label Studio.\n\n\n\n8.1.6 Corpus Formatting\nAfter cleaning and processing the data, store it in a structured format:\n\nPlain Text: Simple text files (one document per file).\nXML/JSON: Structured formats with metadata tags and hierarchy.\nCSV/TSV: Useful if you need to store data with multiple fields (e.g., sentences, labels, tags).\nSpecialized Formats: If the corpus is for NLP tasks, use formats like CoNLL or CSV files with token annotations.\n\n\n\n8.1.7 Corpus Size & Diversity\n\nDetermine the size of your corpus: Depending on the task, your corpus may need to be large (for language models) or small but highly focused (for linguistic studies).\nEnsure diversity: Balance the corpus with data from various genres, demographics, or domains, as needed.\n\n\n\n8.1.8 Corpus Tools (Optional)\n\nIf you need tools to manage or explore your corpus:\nAntConc: A free tool for analyzing text corpora (concordance, collocation, etc.).\nNLTK, spaCy: Python libraries for processing and analyzing corpora, which also provide tools for tokenization, tagging, and more.\n\n\n\n8.1.9 Export/Share the Corpus\n\nOnce built, you can export your corpus for analysis or use in machine learning models.\nIf sharing, consider adding documentation and metadata about its sources, structure, and purpose.\n\n\n\n8.1.10 Example: Steps for Building a Custom Sentiment Analysis Corpus\n\nDefine Purpose: Create a sentiment analysis corpus.\nCollect Data: Scrape reviews from a website (e.g., Amazon reviews).\nPreprocess: Remove HTML tags, stopwords, punctuation, and tokenize sentences.\nAnnotate: Manually label each review with positive, negative, or neutral sentiment.\nStore: Save the corpus in a CSV format with columns: “Review Text”, “Sentiment”.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Building a Custom Corpus</span>"
    ]
  },
  {
    "objectID": "08.Custom_Corpus.html#web-scraping",
    "href": "08.Custom_Corpus.html#web-scraping",
    "title": "8  Building a Custom Corpus",
    "section": "8.2 Web Scraping",
    "text": "8.2 Web Scraping\nWeb scraping is the process of extracting data from websites. In Python, there are several popular libraries used for this purpose, including BeautifulSoup, requests, and Scrapy. Below is an overview of the steps involved in web scraping, followed by a Python code example using BeautifulSoup and requests.\n\n8.2.1 Steps for Web Scraping\n\nInstall Required Libraries: You will need to install the following libraries:\n\nrequests: To send HTTP requests to a web page and get the HTML content.\nBeautifulSoup (from the bs4 package): To parse the HTML and extract the data.\n(Optional) pandas: To store the extracted data in a structured format like a DataFrame.\n\nInstall them via pip:\npip install requests beautifulsoup4 pandas\nUnderstand the Website Structure:\n\nOpen the website you want to scrape.\nUse your browser’s developer tools (F12 or right-click → Inspect) to inspect the HTML structure and identify the elements containing the data you want to scrape (e.g., headings, paragraphs, tables, etc.).\n\nWrite the Web Scraping Script:\n\nUse requests to fetch the HTML content of the page.\nParse the HTML using BeautifulSoup and extract the required data.\nOptionally, save the data in a CSV file or a DataFrame.\n\n\n\n\n8.2.2 Example: Scraping Job Listings from a Webpage\nTo scrape data from all pages of the Books to Scrape website, including subsequent pages, you’ll need to set up a loop that continues until it reaches the last page. Each page’s URL follows the pattern https://books.toscrape.com/catalogue/page-X.html, where X is the page number.\nThe key is to keep requesting new pages and checking if a next page exists by inspecting the “Next” button. If it exists, you continue to the next page; otherwise, the loop stops.\nHere’s the modified Python code that automatically handles pagination until the end of the list:\n\n\n8.2.3 Python Code to Scrape All Pages with Pagination:\nimport requests\nfrom bs4 import BeautifulSoup\nimport pandas as pd\n\n# Create lists to store all extracted data\nall_titles = []\nall_prices = []\nall_availability = []\nall_links = []\nall_descriptions = []\n\n# Base URL for books to scrape\nbase_url = \"https://books.toscrape.com/catalogue/\"\npage_url = \"https://books.toscrape.com/catalogue/page-1.html\"\n\n# Loop through each page until there is no next page\nwhile True:\n    # Send a GET request to the current page\n    response = requests.get(page_url, timeout=10)\n\n    # Check if the request was successful\n    if response.status_code == 200:\n        soup = BeautifulSoup(response.content, 'html.parser')\n\n        # Find all book entries on the page\n        books = soup.find_all('article', class_='product_pod')\n\n        for book in books:\n            title = book.h3.a['title']\n            all_titles.append(title)\n\n            price = book.find('p', class_='price_color').text.strip()\n            all_prices.append(price)\n\n            availability = book.find('p', class_='instock availability').text.strip()\n            all_availability.append(availability)\n\n            relative_link = book.h3.a['href']\n            full_link = base_url + relative_link.replace('../../', '')\n            all_links.append(full_link)\n\n            # Now scrape the product description from the book's page\n            try:\n                book_response = requests.get(full_link, timeout=10)\n                if book_response.status_code == 200:\n                    book_soup = BeautifulSoup(book_response.content, 'html.parser')\n                    # Scrape the product description\n                    description_element = book_soup.select_one('#content_inner &gt; article &gt; p')\n                    if description_element:\n                        description = description_element.text.strip()\n                    else:\n                        description = \"No description available.\"\n                else:\n                    description = \"Failed to retrieve description.\"\n            except Exception as e:\n                description = f\"Error: {str(e)}\"\n\n            all_descriptions.append(description)\n\n        # Check if there's a next page by looking for the \"next\" button\n        next_button = soup.select_one('li.next &gt; a')\n        if next_button:\n            next_page_relative_url = next_button['href']\n            page_url = base_url + next_page_relative_url  # Construct the URL for the next page\n        else:\n            break  # No more pages, exit the loop\n    else:\n        print(f\"Failed to retrieve page. Status code: {response.status_code}\")\n        break\n\n# Save all the data to a DataFrame and CSV file\nbooks_df = pd.DataFrame({\n    'Title': all_titles,\n    'Price': all_prices,\n    'Availability': all_availability,\n    'Link': all_links,\n    'Product Description': all_descriptions\n})\n\nbooks_df.to_csv('books_scraped_with_descriptions_all_pages.csv', index=False)\n\n# Print the extracted data\nprint(books_df)\n\n\n8.2.4 Explanation:\n\nPagination Handling:\n\nThe loop starts from the first page (page-1.html) and checks for the presence of a “Next” button (li.next &gt; a) at the bottom of the page.\nIf the “Next” button exists, the script extracts the URL for the next page and continues the loop. If not, the loop ends.\n\nExtracting Book Information:\n\nFor each book on a page, the script extracts the title, price, availability, and the link to the book’s detail page.\nIt then sends a separate request for each book’s detail page and scrapes the product description from the section (e.g., #content_inner &gt; article &gt; p).\n\n\n\n\n\n\n\n\nFig. 1. How to copy “selector” address (F12 key + Selector)\n\nExiting the Loop:\n\nThe loop breaks when there is no “Next” button on the page, meaning it has reached the last page of the pagination.\n\nSaving Data:\n\nThe extracted data (titles, prices, availability, links, and product descriptions) is stored in a Pandas DataFrame and saved as a CSV file (books_scraped_with_descriptions_all_pages.csv).\n\n\n\n\n8.2.5 Expected Output:\nThe script will scrape all available pages of books and store the data in a CSV file with columns for the book title, price, availability, link, and product description.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Building a Custom Corpus</span>"
    ]
  },
  {
    "objectID": "08.Custom_Corpus.html#youtube-subtitle-extraction",
    "href": "08.Custom_Corpus.html#youtube-subtitle-extraction",
    "title": "8  Building a Custom Corpus",
    "section": "8.3 YouTube Subtitle Extraction",
    "text": "8.3 YouTube Subtitle Extraction\nYou can extract subtitles (also called closed captions) from YouTube videos using the YouTube Data API or by directly downloading the subtitles if they are available. Here’s a guide on how to do both using Python.\n\n8.3.1 Method 1: Using YouTube Transcript API (Simpler Approach)\nA Python package called youtube-transcript-api provides an easy way to extract subtitles from YouTube videos without needing an API key.\n\n8.3.1.1 Step 1: Install youtube-transcript-api\nYou can install this package using pip:\npip install youtube-transcript-api\n\n\n8.3.1.2 Step 2: Code Example to Extract Subtitles\nfrom youtube_transcript_api import YouTubeTranscriptApi\nimport csv\n\n# Replace with your YouTube video ID\nvideo_id = 'YOUR_VIDEO_ID'\n\n# Fetch the transcript\ntry:\n    transcript = YouTubeTranscriptApi.get_transcript(video_id)\n    \n    # Prepare to save the transcript to a .txt file\n    with open('transcript.txt', 'w', encoding='utf-8') as txt_file:\n        for entry in transcript:\n            start_time = entry['start']\n            duration = entry['duration']\n            text = entry['text']\n            # Writing to txt file\n            txt_file.write(f\"Start: {start_time} seconds, Duration: {duration} seconds\\nText: {text}\\n\\n\")\n    \n    # Prepare to save the transcript to a .csv file\n    with open('transcript.csv', 'w', newline='', encoding='utf-8') as csv_file:\n        writer = csv.writer(csv_file)\n        # Writing the header\n        writer.writerow(['Start Time (seconds)', 'Duration (seconds)', 'Text'])\n        \n        # Writing each transcript entry\n        for entry in transcript:\n            writer.writerow([entry['start'], entry['duration'], entry['text']])\n    \n    print(\"Transcript saved as 'transcript.txt' and 'transcript.csv'\")\n    \nexcept Exception as e:\n    print(f\"Error: {e}\")\n\n\n8.3.1.3 Example:\nIf the video URL is https://www.youtube.com/watch?v=abcd1234, then the video_id would be abcd1234.\nThis will return a list of subtitles with their start times and duration for the video. The subtitles must be available for the video (either auto-generated or uploaded by the video creator).\n\n\n8.3.1.4 Explanation:\n\nText File (transcript.txt):\n\nWe open a text file (transcript.txt) in write mode.\nFor each subtitle entry, we format and write the start time, duration, and the text of the subtitle into the file.\n\nCSV File (transcript.csv):\n\nWe open a CSV file in write mode.\nWe write the header with three columns: Start Time, Duration, and Text.\nWe then write each subtitle entry (start time, duration, and text) as a row in the CSV file.\n\n\n\n\n8.3.1.5 Output:\n\ntranscript.txt: Contains a formatted transcript with start times, durations, and text for each subtitle entry.\ntranscript.csv: Contains the transcript data organized in a table with three columns: Start Time, Duration, and Text.\n\n\n\n\n8.3.2 Method 2: Using YouTube Data API v3\nThe YouTube Data API v3 does not directly provide subtitles but can help you identify videos that have captions, and from there, you can manually fetch the captions if they are available.\n\n8.3.2.1 Step 1: Set Up YouTube Data API\n\nGo to the Google Developers Console.\nCreate a new project and enable the YouTube Data API v3.\nGenerate an API key from the credentials tab.\n\n\n\n\n\n8.3.2.2 Step 2: Install the Required Libraries\npip install google-api-python-client\n\n\n8.3.2.3 Step 3: Code Example to Check for Subtitles Availability\nIf you only want to save the video title and the entire subtitle (combined) as a single row in the CSV file, we can modify the script to:\n\nFetch the title of the video using the YouTube Data API.\nFetch the transcript (subtitles) using the youtube-transcript-api and combine the entire transcript text into one row.\nSave the video title and entire subtitle as a single row in the CSV file.\n\nHere’s the modified code:\n\n\n8.3.2.4 Modified Code:\nfrom googleapiclient.discovery import build\nfrom youtube_transcript_api import YouTubeTranscriptApi\nimport csv\n\n# Replace with your API key\napi_key = 'YOUR_API_KEY'\n\nyoutube = build('youtube', 'v3', developerKey=api_key)\n\n# Replace with the video ID you want to check\nvideo_id = 'YOUR_VIDEO_ID'\n\n# Fetch video details\nresponse = youtube.videos().list(\n    part='snippet,contentDetails',\n    id=video_id\n).execute()\n\n# Check if video exists and fetch the title\nif 'items' in response and response['items']:\n    video_title = response['items'][0]['snippet']['title']  # Get the video title\n    print(f\"Video Title: {video_title}\")\n    \n    # Check if captions are available and fetch the transcript\n    if 'contentDetails' in response['items'][0] and 'caption' in response['items'][0]['contentDetails']:\n        if response['items'][0]['contentDetails']['caption'] == 'true':\n            print(f\"Captions are available for video ID: {video_id}\")\n            \n            # Fetch transcript using youtube-transcript-api\n            try:\n                transcript = YouTubeTranscriptApi.get_transcript(video_id)\n                \n                # Combine all the subtitles into a single string\n                combined_subtitle = \" \".join([entry['text'] for entry in transcript])\n                \n                # Save the video title and combined subtitles to a .csv file\n                with open('transcript.csv', 'w', newline='', encoding='utf-8') as csv_file:\n                    writer = csv.writer(csv_file)\n                    writer.writerow(['Video Title', 'Subtitle'])  # Header row\n                    writer.writerow([video_title, combined_subtitle])  # Data row\n                \n                print(\"Transcript saved as 'transcript.csv'\")\n            \n            except Exception as e:\n                print(f\"Error fetching transcript: {e}\")\n        else:\n            print(f\"No captions available for video ID: {video_id}\")\nelse:\n    print(f\"No video found for video ID: {video_id}\")\n\n\n8.3.2.5 Explanation of Changes:\n\nVideo Title:\n\nWe extract the video title using the YouTube Data API from response['items'][0]['snippet']['title'].\n\nCombining Subtitles:\n\nInstead of saving the start time, duration, and individual subtitle entries, the script now combines all the subtitle texts into a single string using \" \".join([entry['text'] for entry in transcript]).\n\nSaving to CSV:\n\nThe script saves only two fields in the CSV: the video title and the combined subtitles (as one row).\n\n\n\n\n8.3.2.6 Output:\n\ntranscript.csv: Contains the video title and the entire subtitle in one row.\n\n\n\n8.3.2.7 Notes:\n\nThis code assumes that the subtitles are available for the video. If not, it will print an appropriate message.\nThe entire subtitle text is combined into a single string and saved in the CSV file.\n\n\n\n\n8.3.3 Method 3: Retrieve the subtitles of multiple videos from a YouTube playlist\nTo retrieve the subtitles of multiple videos from a YouTube playlist and save the results in a CSV file with each row containing the video title and the subtitles, we can follow these steps:\n\n8.3.3.1 Steps:\n\nUse the YouTube Data API to fetch all the video IDs from the playlist.\nFor each video, fetch the video title and subtitles using youtube-transcript-api.\nSave the video title and combined subtitles for each video in a CSV file.\n\n\n\n8.3.3.2 Notes:\n\nPlaylist ID: You need to replace 'YOUR_PLAYLIST_ID' with the actual YouTube Playlist ID.\nAPI Key: Replace 'YOUR_API_KEY' with your valid YouTube Data API key.\nNo Subtitles: If a video has no subtitles available, it will display \"No subtitles available\" in the CSV file.\n\n\n\n8.3.3.3 Code to Achieve This:\nfrom googleapiclient.discovery import build\nfrom youtube_transcript_api import YouTubeTranscriptApi, NoTranscriptFound, TranscriptsDisabled\nimport csv\n\n# Replace with your YouTube API key\napi_key = 'YOUR_API_KEY'\n\n# Initialize the YouTube API client\nyoutube = build('youtube', 'v3', developerKey=api_key)\n\n# Replace with your Playlist ID\nplaylist_id = 'YOUR_PLAYLIST_ID'\n\n# Function to get all video IDs from a playlist\ndef get_video_ids_from_playlist(playlist_id):\n    video_ids = []\n    next_page_token = None\n\n    while True:\n        # Fetch the playlist items\n        request = youtube.playlistItems().list(\n            part='contentDetails',\n            playlistId=playlist_id,\n            maxResults=50,\n            pageToken=next_page_token\n        )\n        response = request.execute()\n\n        # Extract video IDs\n        for item in response['items']:\n            video_ids.append(item['contentDetails']['videoId'])\n\n        # Check if there's another page of results\n        next_page_token = response.get('nextPageToken')\n        if not next_page_token:\n            break\n\n    return video_ids\n\n# Function to fetch video title and transcript\ndef fetch_video_details_and_transcript(video_id):\n    try:\n        # Fetch video details (title)\n        video_response = youtube.videos().list(\n            part='snippet',\n            id=video_id\n        ).execute()\n\n        if 'items' in video_response and video_response['items']:\n            video_title = video_response['items'][0]['snippet']['title']\n\n            # Fetch transcript using youtube-transcript-api\n            try:\n                transcript = YouTubeTranscriptApi.get_transcript(video_id)\n                combined_subtitle = \" \".join([entry['text'] for entry in transcript])\n\n                return video_title, combined_subtitle\n            except (NoTranscriptFound, TranscriptsDisabled):\n                # Handle cases where subtitles are not available\n                return video_title, \"No subtitles available\"\n        else:\n            return None, None\n    except Exception as e:\n        print(f\"Error fetching details for video ID {video_id}: {e}\")\n        return None, None\n\n# Fetch all video IDs from the playlist\nvideo_ids = get_video_ids_from_playlist(playlist_id)\n\n# Prepare to save the video title and subtitles into a CSV file\nwith open('playlist_transcripts.csv', 'w', newline='', encoding='utf-8') as csv_file:\n    writer = csv.writer(csv_file)\n    writer.writerow(['Video Title', 'Subtitle'])  # Header row\n\n    # Process each video in the playlist\n    for video_id in video_ids:\n        video_title, combined_subtitle = fetch_video_details_and_transcript(video_id)\n        if video_title:\n            # Ensure both title and subtitles are on a single row\n            writer.writerow([video_title, combined_subtitle])  # Write video title and combined subtitle\n\nprint(\"Transcripts for the playlist videos saved to 'playlist_transcripts.csv'\")\n\n\n\n8.3.4 Explanation of the Code:\n\nYouTube Data API:\n\nWe use the YouTube Data API to retrieve all the video IDs in the playlist using youtube.playlistItems().list().\nEach API call can retrieve a maximum of 50 videos per request, and we loop through the playlist if there are more videos (using pagination).\n\nYouTube Transcript API:\n\nFor each video, we fetch its transcript using the youtube-transcript-api package.\nIf subtitles are available, they are concatenated into one string.\nIf no subtitles are found or they are disabled, the message \"No subtitles available\" is saved.\n\nCSV File Output:\n\nThe result is saved in a CSV file with two columns: Video Title and Subtitle.\nEach row corresponds to one video from the playlist, containing the video’s title and its entire subtitle (or a message if subtitles are unavailable).\n\nErrors to be Fixed:\n\n\n\n\n8.3.5 Updated Code:\nTo address the issue, each video’s subtitles are saved as a separate .txt file in a folder called playlist_txt. These .txt files are then imported into a pandas DataFrame and finally exported as an Excel (.xlsx) file. Below is the complete solution:\nimport os\nimport pandas as pd\nfrom googleapiclient.discovery import build\nfrom youtube_transcript_api import YouTubeTranscriptApi, NoTranscriptFound, TranscriptsDisabled\nimport csv\nimport openpyxl\n\n# Replace with your YouTube API key\napi_key = 'YOUR_API_KEY'\n\n# Initialize the YouTube API client\nyoutube = build('youtube', 'v3', developerKey=api_key)\n\n# Replace with your Playlist ID\nplaylist_id = 'YOUR_PLAYLIST_ID'\n\n# Create directory for storing text files\nif not os.path.exists('playlist_txt'):\n    os.makedirs('playlist_txt')\n\n# Function to get all video IDs from a playlist\ndef get_video_ids_from_playlist(playlist_id):\n    video_ids = []\n    next_page_token = None\n\n    while True:\n        # Fetch the playlist items\n        request = youtube.playlistItems().list(\n            part='contentDetails',\n            playlistId=playlist_id,\n            maxResults=50,\n            pageToken=next_page_token\n        )\n        response = request.execute()\n\n        # Extract video IDs\n        for item in response['items']:\n            video_ids.append(item['contentDetails']['videoId'])\n\n        # Check if there's another page of results\n        next_page_token = response.get('nextPageToken')\n        if not next_page_token:\n            break\n\n    return video_ids\n\n# Function to fetch video title and transcript\ndef fetch_video_details_and_transcript(video_id):\n    try:\n        # Fetch video details (title)\n        video_response = youtube.videos().list(\n            part='snippet',\n            id=video_id\n        ).execute()\n\n        if 'items' in video_response and video_response['items']:\n            video_title = video_response['items'][0]['snippet']['title']\n\n            # Fetch transcript using youtube-transcript-api\n            try:\n                transcript = YouTubeTranscriptApi.get_transcript(video_id)\n                # Combine all subtitle text into a single string, ensuring it's properly joined\n                combined_subtitle = \" \".join([entry['text'].strip() for entry in transcript])\n\n                return video_title, combined_subtitle\n            except (NoTranscriptFound, TranscriptsDisabled):\n                # Handle cases where subtitles are not available\n                return video_title, \"No subtitles available\"\n        else:\n            return None, None\n    except Exception as e:\n        print(f\"Error fetching details for video ID {video_id}: {e}\")\n        return None, None\n\n# Function to save transcript as a .txt file\ndef save_transcript_as_txt(video_title, combined_subtitle):\n    sanitized_title = \"\".join([c if c.isalnum() else \"_\" for c in video_title])  # Sanitize the title to be filesystem safe\n    txt_filename = f'playlist_txt/{sanitized_title}.txt'\n    with open(txt_filename, 'w', encoding='utf-8') as txt_file:\n        txt_file.write(f\"Video Title: {video_title}\\n\\n\")\n        txt_file.write(combined_subtitle)\n    return txt_filename\n\n# Fetch all video IDs from the playlist\nvideo_ids = get_video_ids_from_playlist(playlist_id)\n\n# List to store data for importing into pandas\ntranscript_data = []\n\n# Process each video in the playlist and save transcripts as text files\nfor video_id in video_ids:\n    video_title, combined_subtitle = fetch_video_details_and_transcript(video_id)\n    if video_title:\n        # Save each video transcript as a .txt file\n        txt_filename = save_transcript_as_txt(video_title, combined_subtitle)\n        \n        # Append video title and file path for pandas import\n        transcript_data.append({'Video Title': video_title, 'Text File': txt_filename})\n    else:\n        print(f\"Skipping video ID {video_id} due to missing title.\")\n\n# Now import the .txt files into pandas and create an Excel sheet\n\n# List to hold the contents of each txt file\ndf_list = []\n\n# Read each txt file and store its content in the DataFrame\nfor transcript in transcript_data:\n    with open(transcript['Text File'], 'r', encoding='utf-8') as file:\n        text_content = file.read()\n    df_list.append({'Video Title': transcript['Video Title'], 'Transcript': text_content})\n\n# Create a pandas DataFrame\ndf = pd.DataFrame(df_list)\n\n# Export the DataFrame to an Excel file\noutput_excel = 'playlist_transcripts.xlsx'\ndf.to_excel(output_excel, index=False)\n\nprint(f\"Transcripts for the playlist videos saved to 'playlist_transcripts.xlsx'\")\n\n\n8.3.6 Explanation of Changes:\n\nDirectory Creation:\n\nThe folder playlist_txt is created using os.makedirs() if it doesn’t already exist. All .txt files are saved in this directory.\n\nText File Saving:\n\nThe function save_transcript_as_txt saves each video’s transcript as a .txt file with a sanitized file name (to avoid illegal characters).\nThe title of the video and the transcript content are written into the .txt file.\n\nPandas DataFrame Import:\n\nEach .txt file is read into a pandas DataFrame where the video title and transcript content are stored.\n\nExporting to Excel:\n\nThe DataFrame is exported as an Excel file (playlist_transcripts.xlsx) using df.to_excel().\n\n\n\n\n8.3.7 Output:\n\nText Files: A directory called playlist_txt will be created, and each video’s subtitles will be saved as a .txt file within this directory.\nExcel File: The script generates an Excel file (playlist_transcripts.xlsx) containing the video title and the complete transcript for each video.\n\n\n8.3.7.1 Example Excel Output:",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Building a Custom Corpus</span>"
    ]
  },
  {
    "objectID": "09.Preprocessing.html",
    "href": "09.Preprocessing.html",
    "title": "9  Text Preprocessing",
    "section": "",
    "text": "9.1 Key Steps in Text Preprocessing",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Text Preprocessing</span>"
    ]
  },
  {
    "objectID": "09.Preprocessing.html#key-steps-in-text-preprocessing",
    "href": "09.Preprocessing.html#key-steps-in-text-preprocessing",
    "title": "9  Text Preprocessing",
    "section": "",
    "text": "Lowercasing: Converting all text to lowercase ensures uniformity. Without this, words like “Book” and “book” would be treated as different entities, leading to unnecessary complexity in the analysis.\nTokenization: Tokenization is the process of splitting text into individual words or tokens. These tokens are the basic units on which further processing steps are applied.\nRemoving Punctuation: Punctuation marks often do not contribute to the meaning of text for analytical purposes. Stripping punctuation ensures a cleaner input for the model.\nStopword Removal: Stopwords are common words like “the,” “is,” “in,” “of,” etc., that appear frequently but contribute little to the overall meaning of the text. Removing stopwords helps reduce noise and improve the focus on the core content.\nLemmatization (Optional): Lemmatization is the process of reducing words to their base or dictionary form. While this is an optional step, it is useful for ensuring that words like “running” and “run” are treated as the same entity. In this section, however, we omit lemmatization for simplicity.\n\n\n9.1.1 Python Code Example 1\nBelow is a Python code that demonstrates how to preprocess a dataset containing raw text, specifically in the “Product Description” column. This dataset includes information about books, and we aim to clean the text data to prepare it for further analysis.\nThe code performs the following operations: - Loads the CSV file that contains the product descriptions. - Converts the text to lowercase. - Removes punctuation marks. - Tokenizes the text into individual words. - Removes common stopwords. - Saves the processed text back into a new column named “preprocessed_text” and exports the updated CSV file.\nimport pandas as pd\nimport re\n\n# Load the CSV file\nfile_path = '/mnt/data/books_scraped_with_descriptions_all_pages.csv'\ndata = pd.read_csv(file_path)\n\n# Preprocess function (without external libraries)\ndef preprocess_text_no_external(text):\n    # Lowercase the text\n    text = text.lower()\n    \n    # Remove punctuation using regex\n    text = re.sub(r'[^\\w\\s]', '', text)\n    \n    # Tokenize by splitting text\n    tokens = text.split()\n    \n    # Simple list of stopwords\n    stop_words = set(['a', 'the', 'and', 'is', 'in', 'it', 'to', 'this', 'of', 'that'])\n    \n    # Remove stopwords\n    tokens = [word for word in tokens if word not in stop_words]\n    \n    # Join tokens back to string\n    return ' '.join(tokens)\n\n# Apply preprocessing to the \"Product Description\" column\ndata['cleaned_description'] = data['Product Description'].dropna().apply(preprocess_text_no_external)\n\n# Add the \"preprocessed text\" column to the original data\ndata['preprocessed_text'] = data['cleaned_description']\n\n# Save the updated data back to the input CSV file, including the new column\nupdated_file_path = '/mnt/data/books_scraped_with_preprocessed_descriptions.csv'\ndata.to_csv(updated_file_path, index=False)\n\nprint(\"Preprocessing complete! The updated file has been saved with preprocessed text.\")\n\n\n9.1.2 Explanation of the Code 1\n\nData Loading: The CSV file containing book data is loaded into a pandas DataFrame, and the column “Product Description” is selected for preprocessing.\nText Cleaning: The preprocess_text_no_external function handles all preprocessing tasks, including:\n\nLowercasing the text.\nRemoving punctuation using regular expressions.\nTokenization by splitting the text on whitespace.\nStopword Removal by filtering out common English stopwords.\n\nProcessed Output: The cleaned text is stored in a new column called “preprocessed_text” in the DataFrame.\nFile Export: Finally, the updated DataFrame is saved back to a CSV file, which includes both the original product description and the newly preprocessed text.\n\n\n\n9.1.3 Python Code Example 2\nimport pandas as pd\nimport nltk\nimport string\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import WordNetLemmatizer\n\n# Ensure required NLTK resources are downloaded\nnltk.download('punkt')\nnltk.download('stopwords')\nnltk.download('wordnet')\nnltk.download('punkt_tab')\n# Load the CSV file\nfile_path = 'books_scraped_with_descriptions_all_pages.csv'\ndata = pd.read_csv(file_path)\n\n# Select the \"Product Description\" column\ndescriptions = data['Product Description'].dropna()\n\n# Initialize the lemmatizer\nlemmatizer = WordNetLemmatizer()\n\n# Preprocess function\ndef preprocess_text(text):\n    # Lowercase the text\n    text = text.lower()\n    \n    # Tokenize the text\n    tokens = word_tokenize(text)\n    \n    # Remove punctuation\n    tokens = [word for word in tokens if word.isalnum()]\n    \n    # Remove stopwords\n    stop_words = set(stopwords.words('english'))\n    tokens = [word for word in tokens if word not in stop_words]\n    \n    # Lemmatize tokens\n    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n    \n    # Join tokens back to string\n    return ' '.join(tokens)\n\n# Apply preprocessing to the \"Product Description\" column\ndata['cleaned_description'] = descriptions.apply(preprocess_text)\n# Add the \"preprocessed text\" column to the original data\ndata['preprocessed_text'] = data['cleaned_description']\n\n# Save the updated data back to the input CSV file, including the new column\nupdated_file_path = 'books_scraped_with_preprocessed_descriptions.csv'\ndata.to_csv(updated_file_path, index=False)\ndata\n\n\n9.1.4 Explanation of the Code 2\nThe provided code performs text preprocessing on a CSV file that contains book descriptions and saves the cleaned text back to a new CSV file. Let’s break it down step by step:\n\n9.1.4.1 Loading the CSV File\n# Load the CSV file\nfile_path = 'books_scraped_with_descriptions_all_pages.csv'\ndata = pd.read_csv(file_path)\n\nThe pd.read_csv() function loads a CSV file located at file_path into a pandas DataFrame called data. This DataFrame will hold all the data from the file, including the book descriptions.\n\n\n\n9.1.4.2 Selecting the ‘Product Description’ Column\n# Select the \"Product Description\" column\ndescriptions = data['Product Description'].dropna()\n\nThe code extracts the ‘Product Description’ column from the DataFrame using data['Product Description'].\nThe dropna() method removes any rows with missing values (i.e., NaN values) in this column to ensure only valid descriptions are processed.\n\n\n\n9.1.4.3 Initializing the Lemmatizer\n# Initialize the lemmatizer\nlemmatizer = WordNetLemmatizer()\n\nThe WordNetLemmatizer() is initialized here. Lemmatization is the process of reducing words to their base or root form (e.g., “running” → “run”). This helps standardize different forms of the same word.\n\n\n\n9.1.4.4 Defining the Preprocessing Function\n# Preprocess function\ndef preprocess_text(text):\n    # Lowercase the text\n    text = text.lower()\n    \n    # Tokenize the text\n    tokens = word_tokenize(text)\n    \n    # Remove punctuation\n    tokens = [word for word in tokens if word.isalnum()]\n    \n    # Remove stopwords\n    stop_words = set(stopwords.words('english'))\n    tokens = [word for word in tokens if word not in stop_words]\n    \n    # Lemmatize tokens\n    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n    \n    # Join tokens back to string\n    return ' '.join(tokens)\nThis function processes the raw text through the following steps:\n\nLowercasing:\n\nThe text is converted to lowercase using text.lower() to ensure uniformity (e.g., “Book” and “book” are treated the same).\n\nTokenization:\n\nword_tokenize(text) splits the text into individual tokens (words). For example, “This is a book” becomes ['This', 'is', 'a', 'book'].\n\nRemoving Punctuation:\n\nword.isalnum() checks if each word is alphanumeric. Words that contain punctuation marks or symbols are excluded.\n\nRemoving Stopwords:\n\nA set of common stopwords (e.g., “the,” “is,” “and”) is defined using stopwords.words('english'). These words are removed from the tokenized text since they usually carry little meaning in most natural language tasks.\n\nLemmatization:\n\nEach word is lemmatized using the lemmatizer.lemmatize(word) method to ensure that different forms of the same word are reduced to their root (e.g., “books” → “book”, “running” → “run”).\n\nRejoining the Tokens:\n\nAfter processing, the tokens are rejoined back into a single string using ' '.join(tokens).\n\n\n\n\n9.1.4.5 Applying the Preprocessing Function\n# Apply preprocessing to the \"Product Description\" column\ndata['cleaned_description'] = descriptions.apply(preprocess_text)\n\nThe preprocess_text() function is applied to each entry in the ‘Product Description’ column using descriptions.apply(preprocess_text).\nThe cleaned and preprocessed text is then stored in a new column in the DataFrame called 'cleaned_description'.\n\n\n\n9.1.4.6 Adding the ‘Preprocessed Text’ Column\n# Add the \"preprocessed text\" column to the original data\ndata['preprocessed_text'] = data['cleaned_description']\n\nThis line adds a new column called 'preprocessed_text' to the data DataFrame, which contains the preprocessed (cleaned) descriptions.\n\n\n\n9.1.4.7 Saving the Updated Data to a CSV File\n# Save the updated data back to the input CSV file, including the new column\nupdated_file_path = 'books_scraped_with_preprocessed_descriptions.csv'\ndata.to_csv(updated_file_path, index=False)\n\nThe updated data DataFrame, now containing both the original and preprocessed descriptions, is saved back to a new CSV file named 'books_scraped_with_preprocessed_descriptions.csv' using to_csv().\nThe index=False parameter ensures that the DataFrame index is not written to the file.\n\n\n\n9.1.4.8 Final Output:\n\nThe output CSV file contains all the original data, with the cleaned descriptions stored in the preprocessed_text column.\nThis file can be used for further analysis, such as topic modeling, sentiment analysis, or other natural language processing tasks.x\n\n\n\n\n9.1.5 Importance of Preprocessing\nEffective text preprocessing is crucial for generating meaningful insights from unstructured data. By reducing noise and standardizing the text, preprocessing enhances the performance of topic models and other natural language processing tasks.\nIn the context of topic modeling, preprocessing ensures that the algorithm focuses on meaningful words rather than irrelevant tokens, making the model’s output more interpretable and accurate.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Text Preprocessing</span>"
    ]
  },
  {
    "objectID": "10.Word_Cloud.html",
    "href": "10.Word_Cloud.html",
    "title": "10  Visualizing Word Frequencies with Graphs and Word Clouds",
    "section": "",
    "text": "10.1 Gutenberg Text Analysis\nThe code provided is meant to download and display the names of books from the Gutenberg corpus using the NLTK (Natural Language Toolkit) library.\nIn the code below, you’re selecting 5 books from the Gutenberg corpus using the books_idx list. Each index in books_idx corresponds to a specific book in books_names.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Visualizing Word Frequencies with Graphs and Word Clouds</span>"
    ]
  },
  {
    "objectID": "10.Word_Cloud.html#gutenberg-text-analysis",
    "href": "10.Word_Cloud.html#gutenberg-text-analysis",
    "title": "10  Visualizing Word Frequencies with Graphs and Word Clouds",
    "section": "",
    "text": "import numpy as np      # Import the numpy library, used for numerical operations and array handling\nimport pandas as pd     # Import the pandas library, used for data manipulation and analysis\nimport nltk             # Import the nltk library, used for natural language processing (NLP) tasks\n\nnltk.download('gutenberg')    # Download the 'gutenberg' corpus, a collection of literary texts\nbooks_names = nltk.corpus.gutenberg.fileids()  # Get a list of available book file IDs (names of the books)\nbooks_names   # Display the list of book names\n\nbooks_idx = [1, 3, 5, -5, -1]     # List of indices to select specific books from the books_names list\nselected_books = []               # Initialize an empty list to store selected books\n\nfor idx in books_idx:             # Loop through each index in books_idx\n    selected_books.append(books_names[idx])  # Append the book name at the given index to the selected_books list\n\nprint(selected_books)             # Print the list of selected book names\n\n10.1.1 (Gutenberg) Example of Selected Books:\nIf the books_names list is the following:\n['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', 'bible-kjv.txt', 'blake-poems.txt', 'bryant-stories.txt', 'burgess-busterbrown.txt', 'carroll-alice.txt', 'chesterton-ball.txt', 'chesterton-brown.txt', 'chesterton-thursday.txt', 'edgeworth-parents.txt', 'melville-moby_dick.txt', 'milton-paradise.txt', 'shakespeare-caesar.txt', 'shakespeare-hamlet.txt', 'shakespeare-macbeth.txt', 'whitman-leaves.txt']\nThe indices [1, 3, 5, -5, -1] correspond to: - 1: 'austen-persuasion.txt' - 3: 'bible-kjv.txt' - 5: 'bryant-stories.txt - -5: 'milton-paradise.txt' - -1: 'whitman-leaves.txt'\nSo, selected_books will contain:\n['austen-persuasion.txt', 'bible-kjv.txt', 'bryant-stories.txt', 'milton-paradise.txt', 'whitman-leaves.txt']\nSave the selected texts to my local drive.\nimport os\nimport nltk\n\n# Ensure the required corpus is available\nnltk.download('gutenberg')\n\n# Sample selected_books for demonstration, in case corpus access is not available\nselected_books = ['austen-persuasion.txt', 'bible-kjv.txt', 'chesterton-ball.txt', 'melville-moby_dick.txt', 'whitman-leaves.txt']\n\n# Define the folder name where the files will be saved\nfolder_name = \"/mnt/data/selected_books_texts\"\n\n# Create the folder if it doesn't exist\nif not os.path.exists(folder_name):\n    os.makedirs(folder_name)  # Create the subfolder\n\n# Iterate through each selected book and save its content to a file in the subfolder\nfor book in selected_books:\n    # Get the full text of the book using nltk.corpus.gutenberg.raw() function\n    book_text = nltk.corpus.gutenberg.raw(book)\n    \n    # Create a file name for each book by removing \".txt\" from the file id\n    file_name = book.replace('.txt', '') + '_content.txt'\n    \n    # Define the full path to save the file in the subfolder\n    full_path = os.path.join(folder_name, file_name)\n    \n    # Write the book content to a text file in the subfolder\n    with open(full_path, 'w') as file:\n        file.write(book_text)  # Write the full text of the book to the file\n\n# Displaying the directory path where the books are saved\nfolder_name\n\n\n10.1.2 (Gutenberg) Load the text of txt:\n# Load the text of \"austen-persuasion.txt\"\ndoc_austen = nltk.corpus.gutenberg.raw('austen-persuasion.txt')  # Use nltk.corpus.gutenberg.raw() to get the full text\n\n# Print the number of characters in the text\nprint('#Num of characters used:', len(doc_austen))  # Print the total number of characters in the book\n\n# Print a sample of the first 100 characters\nprint('#Text sample:')\nprint(doc_austen[:100])  # Print the first 100 characters of the book\nOutput:\n```\n\n```\n\n\n10.1.3 Tokenize\nTokenization is a fundamental step in Natural Language Processing (NLP) that involves splitting text into smaller units, known as tokens. These tokens could be words, phrases, sentences, or even characters, depending on the type of tokenization applied.\nTokenization is a crucial preprocessing step for text analysis because it breaks down raw text into manageable pieces that can be analyzed or manipulated. It allows algorithms to understand and process text data effectively. For example, tokenization helps convert a text document into a form that can be used for tasks like text classification, sentiment analysis, machine translation, etc.\n\n10.1.3.1 Types of Tokenization:\n\nWord Tokenization:\n\nSplits the text into individual words.\nExample:\n\nInput: \"The cat sat on the mat.\"\nOutput: ['The', 'cat', 'sat', 'on', 'the', 'mat']\n\n\nSentence Tokenization:\n\nSplits the text into sentences.\nExample:\n\nInput: \"The cat sat on the mat. It was happy.\"\nOutput: ['The cat sat on the mat.', 'It was happy.']\n\n\nCharacter Tokenization:\n\nSplits the text into individual characters.\nExample:\n\nInput: \"cat\"\nOutput: ['c', 'a', 't']\n\n\nSubword Tokenization:\n\nSplits words into smaller units, useful in languages where words are made up of many parts (like in morphologically rich languages). Models like BPE (Byte Pair Encoding) or WordPiece use subword tokenization.\nExample:\n\nInput: \"unhappiness\"\nOutput: ['un', 'happi', 'ness']\n\n\n\n\n\n10.1.3.2 Example of Tokenization in Python Using NLTK:\nThe NLTK (Natural Language Toolkit) provides functions to tokenize text easily.\nWord Tokenization Example:\nimport nltk\nfrom nltk.tokenize import word_tokenize\n\n# Example sentence\ntext = \"The cat sat on the mat.\"\n\n# Tokenize the sentence into words\ntokens = word_tokenize(text)\n\nprint(tokens)\nOutput:\n['The', 'cat', 'sat', 'on', 'the', 'mat', '.']\nSentence Tokenization Example:\nfrom nltk.tokenize import sent_tokenize\n\n# Example paragraph\ntext = \"The cat sat on the mat. It was happy.\"\n\n# Tokenize the text into sentences\nsentences = sent_tokenize(text)\n\nprint(sentences)\nOutput:\n['The cat sat on the mat.', 'It was happy.']\n\n\n10.1.3.3 Applications of Tokenization:\n\nText Classification: Breaking down text into words to feed into a machine learning model.\nMachine Translation: Splitting input and output sentences to translate between languages.\nInformation Retrieval: Indexing tokens (words) to help retrieve documents based on search queries.\nText Analysis: Tokenized text allows algorithms to count word frequencies, perform sentiment analysis, and more.\n\n\n\n\n10.1.4 (Gutenberg) Tokenize the selected txt:\nfrom nltk.tokenize import word_tokenize  # Import the word_tokenize function from NLTK\n\n# Tokenize the doc_alice text (assuming doc_alice contains the raw text of \"Alice in Wonderland\")\ntokens_austen = word_tokenize(doc_austen)\n\n# Print the number of tokens generated after tokenization\nprint('#Num of tokens used:', len(tokens_austen))\n\n# Print a sample of the first 20 tokens\nprint('#Token sample:')\nprint(tokens_austen[:20])\nOutput:\n\nThe following pattern captures words with 3 or more alphanumeric characters or apostrophes.\nfrom nltk.tokenize import RegexpTokenizer  # Import the RegexpTokenizer from NLTK\n\n# Initialize the tokenizer with a regular expression pattern\ntokenizer = RegexpTokenizer(r\"[\\w']{3,}\")  # This pattern captures words with 3 or more alphanumeric characters or apostrophes\n\n# Tokenize the text (converted to lowercase) using the RegexpTokenizer\nreg_tokens_austen = tokenizer.tokenize(doc_austen.lower())  # Convert text to lowercase before tokenization\n\n# Print the number of tokens after Regexp tokenization\nprint('#Num of tokens with RegexpTokenizer:', len(reg_tokens_austen))\n\n# Print a sample of the first 20 tokens\nprint('#Token sample:')\nprint(reg_tokens_austen[:20])\nOutput:",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Visualizing Word Frequencies with Graphs and Word Clouds</span>"
    ]
  },
  {
    "objectID": "10.Word_Cloud.html#what-is-stopwords",
    "href": "10.Word_Cloud.html#what-is-stopwords",
    "title": "10  Visualizing Word Frequencies with Graphs and Word Clouds",
    "section": "10.2 What is Stopwords?",
    "text": "10.2 What is Stopwords?\nStopwords are common words in a language that are often filtered out before processing natural language text. These words carry very little semantic meaning and are often omitted during tasks such as text mining, search indexing, and information retrieval. The goal of removing stopwords is to reduce the noise and focus on the more meaningful words that contribute to the context or content of the text.\n\n10.2.1 Why Remove Stopwords?\nStopwords such as “the”, “is”, “in”, “and”, “on”, etc., are very frequent in most texts but don’t carry significant meaning by themselves. Removing them can:\n\nReduce text size: Fewer words mean faster processing, which can improve the efficiency of algorithms.\nImprove focus: Removing common, non-informative words helps focus on the important terms that can provide insights.\nReduce dimensionality: When working with techniques like TF-IDF, vectorization, or word embeddings, removing stopwords helps lower the dimensionality of the data.\n\n\n\n10.2.2 Common Examples of Stopwords:\n\nEnglish stopwords: \"the\", \"is\", \"in\", \"at\", \"on\", \"and\", \"a\", \"an\", \"but\", \"or\", \"so\", \"if\", \"then\", etc.\nStopwords vary by language. Each language has its own set of stopwords based on the common, non-essential words in that language.\n\n\n\n10.2.3 Example of Stopwords in Python Using NLTK:\nThe NLTK library provides a built-in list of stopwords for various languages. Here’s how you can use them:\nimport nltk\nfrom nltk.corpus import stopwords\n\n# Download the stopwords dataset\nnltk.download('stopwords')\n\n# Get the list of English stopwords\nstop_words = set(stopwords.words('english'))\n\n# Example sentence\ntext = \"The cat sat on the mat and the dog lay beside it.\"\n\n# Tokenize the text into words\nfrom nltk.tokenize import word_tokenize\nwords = word_tokenize(text)\n\n# Filter out the stopwords\nfiltered_words = [word for word in words if word.lower() not in stop_words]\n\nprint(\"Original Words:\", words)\nprint(\"Filtered Words (without stopwords):\", filtered_words)\nOutput:\nOriginal Words: ['The', 'cat', 'sat', 'on', 'the', 'mat', 'and', 'the', 'dog', 'lay', 'beside', 'it', '.']\nFiltered Words (without stopwords): ['cat', 'sat', 'mat', 'dog', 'lay', 'beside', '.']\nAs you can see, common words like “the”, “on”, “and”, “it” have been removed, leaving only the more meaningful words.\n\n\n10.2.4 When Not to Remove Stopwords:\nWhile stopwords are typically removed for most NLP tasks, there are cases where you might want to retain them, such as:\n\nSentiment Analysis: Words like “not”, “is”, “but” can change the sentiment of a sentence.\nLanguage Modeling: For tasks like speech generation, translation, or conversational agents, removing stopwords might lose important grammatical structure.\n\n\n\n10.2.5 Stopwords in Other NLP Libraries:\nIn addition to NLTK, stopwords are also supported in other libraries like spaCy and Scikit-learn.\n\n10.2.5.1 Stopwords in spaCy:\nimport spacy\nfrom spacy.lang.en.stop_words import STOP_WORDS\n\n# Load English model\nnlp = spacy.load(\"en_core_web_sm\")\n\n# Example of using spaCy stopwords\nprint(STOP_WORDS)\nfrom nltk.corpus import stopwords  # Import the stopwords list from NLTK\n\n# Load the set of English stopwords (commonly excluded words like 'the', 'is', etc.)\nenglish_stops = set(stopwords.words('english'))  # Convert to set for faster lookup\n\n# Filter out stopwords from the list of tokens using list comprehension\nresult_austen = [word for word in reg_tokens_austen if word not in english_stops]\n\n# Print the number of tokens after stopword elimination\nprint('#Num of tokens after stopword elimination:', len(result_austen))\n\n# Print a sample of the first 20 tokens after removing stopwords\nprint('#Token sample:')\nprint(result_austen[:20])\nOutput:\n\n\n\n\n10.2.6 (Gutenberg) Word Frequency Count of the selected txt:\n# Initialize an empty dictionary to store word counts\nausten_word_count = dict()\n\n# Count the frequency of each word in result_alice\nfor word in result_austen:\n    austen_word_count[word] = austen_word_count.get(word, 0) + 1  # Increment the count for each word\n\n# Print the total number of unique words used\nprint('#Num of used words:', len(austen_word_count))\n\n# Sort the dictionary by word frequency in descending order\nsorted_word_count = sorted(austen_word_count, key=austen_word_count.get, reverse=True)\n\n# Print the top 20 most frequent words\nprint(\"#Top 20 high frequency words:\")\nfor key in sorted_word_count[:20]:  # Loop through the top 20 most frequent words\n    print(f'{repr(key)}: {austen_word_count[key]}', end=', ')\nOutput:\n\n\n\n10.2.7 (Gutenberg) Word Frequency Count of the selected txt with Part-of-Speech (POS):\n# Define the set of part-of-speech (POS) tags we are interested in\nmy_tag_set = ['NN', 'VB', 'VBD', 'JJ']  # NN: Nouns, VB: Verbs (base), VBD: Verbs (past), JJ: Adjectives\n\n# Filter words based on their POS tags\nmy_words = [word for word, tag in nltk.pos_tag(result_austen) if tag in my_tag_set]\n\n# Initialize an empty dictionary to store word counts\nausten_word_count = dict()\n\n# Count the frequency of each filtered word\nfor word in my_words:\n    austen_word_count[word] = austen_word_count.get(word, 0) + 1  # Increment the count for each word\n\n# Print the total number of unique words used after filtering by POS tags\nprint('#Num of used words:', len(austen_word_count))\n\n# Sort the dictionary by word frequency in descending order\nsorted_word_count = sorted(austen_word_count, key=austen_word_count.get, reverse=True)\n\n# Print the top 20 most frequent words\nprint(\"#Top 20 high frequency words:\")\nfor key in sorted_word_count[:20]:  # Loop through the top 20 most frequent words\n    print(f'{repr(key)}: {austen_word_count[key]}', end=', ')\nOutput:",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Visualizing Word Frequencies with Graphs and Word Clouds</span>"
    ]
  },
  {
    "objectID": "10.Word_Cloud.html#visualization",
    "href": "10.Word_Cloud.html#visualization",
    "title": "10  Visualizing Word Frequencies with Graphs and Word Clouds",
    "section": "10.3 Visualization",
    "text": "10.3 Visualization\nVisualizing word frequencies can help you understand the most common words in a text and gain insights into the text data. Here are some common methods to visualize word frequencies:\n\n10.3.1 Bar Plot\nA bar plot is a simple and effective way to visualize word frequencies. The most frequent words can be shown on the x-axis, and their frequencies on the y-axis.\n\n\n10.3.2 Word Cloud\nA word cloud represents words with varying font sizes depending on their frequencies. Words that appear more frequently are displayed larger, while less frequent words are smaller.\n\n\n\n10.3.3 Example 1: Visualize Word Frequencies Using a Bar Plot\nYou can use the matplotlib and seaborn libraries to create a bar plot showing the most frequent words.\n\n10.3.3.1 Steps:\n\nCount the word frequencies.\nSort the word frequencies in descending order.\nPlot the top N most frequent words.\n\n\n\n10.3.3.2 Code Example:\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Example word count dictionary (can be your `austen_word_count`)\nword_frequencies = {\"the\": 50, \"cat\": 10, \"sat\": 8, \"on\": 20, \"mat\": 5}\n\n# Convert to a list of tuples and sort by frequency (descending)\nsorted_words = sorted(word_frequencies.items(), key=lambda x: x[1], reverse=True)\n\n# Get top N most frequent words\ntop_words = sorted_words[:5]  # Top 5 most frequent words\n\n# Separate words and frequencies for plotting\nwords, counts = zip(*top_words)\n\n# Create a bar plot\nplt.figure(figsize=(8, 6))\nsns.barplot(x=list(words), y=list(counts), palette=\"viridis\")\nplt.title(\"Top 5 Most Frequent Words\")\nplt.ylabel(\"Frequency\")\nplt.xlabel(\"Words\")\nplt.show()\n\n\n10.3.3.3 Explanation:\nLet’s break down the line of code in detail:\nsorted_words = sorted(word_frequencies.items(), key=lambda x: x[1], reverse=True)\nword_frequencies.items()\nword_frequencies is a dictionary that stores the words as keys and their frequencies as values. For example, the dictionary might look like this:\nword_frequencies = {\n    \"the\": 50,\n    \"cat\": 10,\n    \"sat\": 8,\n    \"on\": 20,\n    \"mat\": 5\n}\nThe .items() method is used to get the items of the dictionary. When you call .items() on a dictionary, it returns a view object that contains a list of key-value pairs (tuples). Each tuple contains a word and its corresponding frequency.\nFor example:\nword_frequencies.items()\nOutput:\n[('the', 50), ('cat', 10), ('sat', 8), ('on', 20), ('mat', 5)]\nSo, word_frequencies.items() returns the following list of tuples:\n[('the', 50), ('cat', 10), ('sat', 8), ('on', 20), ('mat', 5)]\nsorted(..., key=lambda x: x[1], reverse=True)\nThis part of the code is responsible for sorting the word-frequency pairs based on the frequency values.\nsorted() Function:\nThe sorted() function takes an iterable (in this case, the list of word-frequency pairs) and returns a new sorted list. By default, sorted() sorts items in ascending order. However, it allows for customization through two important arguments:\n\nkey: This defines a function that will be applied to each item to extract a comparison key for sorting. In this case, we want to sort the tuples by their frequency (the second item in each tuple, x[1]).\nreverse: If set to True, this will sort the list in descending order instead of the default ascending order.\n\nkey=lambda x: x[1]\n\nThe key argument allows us to specify a custom sorting criterion. In this case, we use a lambda function (lambda x: x[1]) to tell Python how to sort the list.\nA lambda function is an anonymous, short function that is often used for small operations that are only needed temporarily.\n\nlambda x: x[1] means: - For each tuple in the list (e.g., ('the', 50)), treat x as the tuple. - x[1] refers to the second element of the tuple, which is the frequency in this case.\nSo, when the sorting process happens, the sorted function will compare the second element of each tuple (i.e., the frequencies) to determine the order.\nreverse=True\n\nThis tells the sorted() function to sort the list in descending order. By default, sorting is done in ascending order (smallest to largest), but reverse=True reverses that order, sorting from the highest frequency to the lowest.\n\n\nwords, counts = zip(*top_words)\nzip(*top_words):\n\n- zip() is a Python built-in function that takes multiple iterables (like lists, tuples, etc.) and returns an iterator of tuples, where each tuple contains elements from the provided iterables at corresponding positions.\n- In this case, the * operator is called the unpacking operator (also known as the splat operator). It unpacks the list of tuples (top_words) so that each tuple in the list is treated as a separate argument to the zip() function.\n\nLet’s see what happens step by step:\n\ntop_words is:\n[('the', 50), ('on', 20), ('cat', 10), ('sat', 8), ('mat', 5)]\nThe *top_words unpacks the list of tuples into separate arguments. The zip() function will receive each tuple separately, as if you passed:\nzip(('the', 50), ('on', 20), ('cat', 10), ('sat', 8), ('mat', 5))\nNow, zip() will combine the first elements of each tuple into one iterable, and the second elements into another iterable:\n\nIt takes the first element of each tuple (words) and groups them together: ('the', 'on', 'cat', 'sat', 'mat').\nIt takes the second element of each tuple (counts) and groups them together: (50, 20, 10, 8, 5).\n\n\nSo, zip(*top_words) effectively separates the words and their frequencies into two separate iterables.\nwords, counts = zip(*top_words):\n\nwords and counts are assigned the result of zip(*top_words).\n\nwords will contain the tuple with all the words: ('the', 'on', 'cat', 'sat', 'mat').\ncounts will contain the tuple with all the word frequencies: (50, 20, 10, 8, 5).\n\n\n\n\n\n10.3.4 Example in Action:\nLet’s say you have the following top_words:\ntop_words = [('the', 50), ('on', 20), ('cat', 10), ('sat', 8), ('mat', 5)]\nWhen you apply the line:\nwords, counts = zip(*top_words)\nHere’s what happens:\n\nUnpacking top_words:\n\nThe * unpacks the list into separate tuples:\n\n('the', 50)\n('on', 20)\n('cat', 10)\n('sat', 8)\n('mat', 5)\n\n\nzip() function groups elements:\n\nGroups the first elements of all tuples (the words): ('the', 'on', 'cat', 'sat', 'mat')\nGroups the second elements of all tuples (the frequencies): (50, 20, 10, 8, 5)\n\nAssigning the results:\n\nwords will contain: ('the', 'on', 'cat', 'sat', 'mat')\ncounts will contain: (50, 20, 10, 8, 5)\n\n\nNow, you have two separate sequences (words and their corresponding frequencies), which can be used for plotting or further processing.\n\n\n\n10.3.5 Example 2: Visualize Word Frequencies Using a Word Cloud\nThe wordcloud library can be used to generate word clouds. You can adjust the appearance by changing parameters such as the background color, maximum number of words, etc.\n\n10.3.5.1 Code Example:\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\n\n# Example word count dictionary (can be your `austen_word_count`)\nword_frequencies = {\"the\": 50, \"cat\": 10, \"sat\": 8, \"on\": 20, \"mat\": 5}\n\n# Create a word cloud object\nwordcloud = WordCloud(width=800, height=400, background_color=\"white\").generate_from_frequencies(word_frequencies)\n\n# Plot the word cloud\nplt.figure(figsize=(10, 5))\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis(\"off\")  # No axes, just the word cloud\nplt.show()\n\n\n10.3.5.2 Explanation:\n\nThe WordCloud() object is created using the word frequencies.\nThe generate_from_frequencies() method is used to pass the word frequencies into the word cloud.\nThe plot is displayed with matplotlib, and the axes are turned off for a cleaner look.\n\n\n\n10.3.5.3 Output:\n\n\n\n\n\n10.3.6 Choosing the Right Visualization:\n\nBar Plot: Best when you want to compare the exact counts of the most frequent words.\nWord Cloud: Provides a more artistic and immediate visual representation but doesn’t allow for easy comparison of exact counts.\n\n\n\n10.3.7 (Gutenberg) Frequency Curve:\nimport matplotlib.pyplot as plt  # Import the matplotlib library for plotting\n# %matplotlib inline  # Ensure plots appear inline in notebooks (if using Jupyter)\n\n# Extract the frequencies of the words in the sorted word list\nw = [austen_word_count[key] for key in sorted_word_count]  # Get the frequencies of sorted words\n\n# Plot the word frequencies\nplt.plot(w)\n\n# Display the plot\nplt.show()\nOutput:\n\n\n\n10.3.8 Zipf’s Law curve or Zipfian distribution.\n\n10.3.8.1 Zipf’s Law:\nZipf’s Law states that in a corpus of natural language text, the frequency of any word is inversely proportional to its rank in the frequency table. In simpler terms:\n\nThe most frequent word occurs twice as often as the second most frequent word, three times as often as the third most frequent word, and so on.\n\n\n\n10.3.8.2 Key Characteristics:\n\nThe x-axis typically represents the rank of words in terms of frequency (with the most frequent word ranked 1).\nThe y-axis represents the frequency of those words.\nThe curve shows a steep drop for the highest frequency words, followed by a long tail of lower frequency words, which is characteristic of power-law distributions.\n\n* What is power-law distribution?\nhttps://alcera.wpcomstaging.com/2016/09/14/another-powerful-idea-the-power-law/\n\n\n10.3.8.3 Why Does It Matter?\nZipf’s law highlights the natural imbalance in word usage: a few words (like “the”, “is”, “and”) are used very frequently, while the majority of words in a language are used infrequently. This insight is crucial for tasks like:\n\nStopword removal: Removing extremely common words that provide little unique information.\nVocabulary management: Determining which words to prioritize when building language models.\n\n\n\n\n10.3.9 (Gutenberg) Bar Plot:\nimport matplotlib.pyplot as plt  # Ensure matplotlib is imported\n\n# Extract the top 20 most frequent words\nn = sorted_word_count[:20]  # Select the top 20 words based on frequency\n\n# Get the frequencies of the top 20 words\nw = [austen_word_count[key] for key in n]  # Get the frequency values for the top 20 words\n\n# Plot the bar chart\nplt.bar(range(len(n)), w, tick_label=n)  # Create a bar chart; 'tick_label' adds the word labels\n\n# Rotate the x-axis labels by 45 degrees\nplt.xticks(rotation=45)\n\n# Display the plot\nplt.show()\nOutput:\n\n\n\n10.3.10 (Gutenberg) Word Cloud:\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt  # Import matplotlib for displaying the word cloud\n\n# Generate a word cloud image from the raw text\nwordcloud = WordCloud().generate(doc_austen)\n\n# Turn off the axis lines and labels\nplt.axis(\"off\")\n\n# Display the word cloud image using matplotlib\nplt.imshow(wordcloud, interpolation='bilinear')\n\n# Show the plot\nplt.show()\nOutput:\n\nwordcloud = WordCloud(max_font_size=60).generate_from_frequencies(austen_word_count)\nplt.figure()\nplt.axis(\"off\")\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.show()\nOutput:\n\nWith mask image:\nimport numpy as np  # Import numpy to handle arrays\nfrom PIL import Image  # Import PIL for image processing\nfrom wordcloud import WordCloud  # Import the WordCloud class for generating word clouds\nimport matplotlib.pyplot as plt  # Import matplotlib for displaying the word cloud\n\n# Load the mask image (used as the background shape for the word cloud)\nausten_mask = np.array(Image.open(\"cloud.png\"))  # Convert the image to a numpy array\n\n# Create a word cloud object with customization options\nwc = WordCloud(\n    background_color=\"white\",  # Set the background color of the word cloud\n    max_words=30,  # Maximum number of words to display\n    mask=austen_mask,  # Use the mask image to shape the word cloud\n    contour_width=3,  # Set the width of the outline of the mask\n    contour_color='steelblue'  # Set the color of the contour/outline\n)\n\n# Generate the word cloud using the word frequencies from alice_word_count\nwc.generate_from_frequencies(austen_word_count)\n\n# Save the generated word cloud to a file\nwc.to_file(\"cloud_output.png\")  # Save the output as \"cloud_output.png\"\n\n# Display the word cloud using matplotlib\nplt.figure()  # Create a new figure\nplt.axis(\"off\")  # Turn off axis lines and labels\nplt.imshow(wc, interpolation='bilinear')  # Display the word cloud\nplt.show()  # Show the word cloud\nOutput:\n\nEnd.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Visualizing Word Frequencies with Graphs and Word Clouds</span>"
    ]
  },
  {
    "objectID": "11.Text_Representation.html",
    "href": "11.Text_Representation.html",
    "title": "11  Text Representation Based on Counts",
    "section": "",
    "text": "11.0.1 What is a vector?\nA vector is explained in relation to scalars, matrices, and tensors. These concepts are important for understanding various mathematical and physical quantities.\nVectors and these structures are also applied in natural language processing to represent characteristics of words, phrases, and sentences.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Text Representation Based on Counts</span>"
    ]
  },
  {
    "objectID": "11.Text_Representation.html#embedding-representation-methods",
    "href": "11.Text_Representation.html#embedding-representation-methods",
    "title": "11  Text Representation Based on Counts",
    "section": "11.1 Embedding Representation Methods",
    "text": "11.1 Embedding Representation Methods\nTo understand embedding, it is necessary first to understand how vectors are represented and generated. Vector representation can broadly be classified into sparse representation and distributed representation.\n\n\n\n(Image Source: https://www.oreilly.com/content/how-neural-networks-learn-distributed-representations/)\n\n\nTo illustrate the concepts of sparse and distributed representations, we can use a simple shape example. In the above Figure, four shapes and their names are shown. The target shape’s name is represented in black, and the other shapes’ names are left blank. In this method, the number of shapes corresponds to the number of features (i.e., dimensions), and each feature is independent.\nIn sparse representation, features are encoded using 1s and 0s. The black mark is represented as 1, and the white marks as 0s. Most of the values are 0, with only the corresponding feature being marked as 1. This method is called sparse representation because only a few features are active (non-zero).\nIn the figure, the first shape is a horizontally long rectangle, and the second shape is a vertically long rectangle. These two shapes share no similarities in sparse representation. Each shape is represented independently, with no overlap in features.\n\nIn contrast, distributed representation captures the similarity between shapes. In the following Figure, both the long horizontal and long vertical rectangles share a common feature, i.e., the fact that they are rectangles, but they differ in their specific directions (horizontal or vertical). This allows for more compact and meaningful vector representations by capturing shared properties between different objects.\n\n\n\n(Image Soruce: https://www.oreilly.com/content/how-neural-networks-learn-distributed-representations/)\n\n\nFor instance, the first and second shapes share the property of being rectangles, while the third and fourth shapes share the property of being towers. In distributed representation, the shapes are described based on their common characteristics, rather than independent one-hot features as in sparse representation.\n\n\n\n\n(Image Source: https://wikidocs.net/31767)\n\n\n\n11.1.1 Local Representation:\n\nOne-hot Vector: A simple count-based method where each word is represented by a vector where only one position is marked as 1 (the word’s position in the vocabulary), while all other positions are 0.\nN-gram: A technique that considers sequences of words (or characters) of length N to capture local context in text.\n\n\n\n11.1.2 Continuous Representation:\nThis is more advanced than local representation and is generally better at capturing the semantic relationships between words.\n\nCount-Based Methods:\n\nBag of Words (DTM): A representation that counts word occurrences in documents without considering word order. This includes methods like document-term matrices (DTM).\nLSA (Latent Semantic Analysis): A method based on singular value decomposition of the document-term matrix, often used for capturing the global meaning of words across full documents.\nGloVe (Global Vectors for Word Representation): A word representation that combines the benefits of global matrix factorization and local context window methods.\n\nPrediction-Based Methods:\n\nWord2Vec (FastText): A continuous bag-of-words (CBOW) and skip-gram model that predicts either a word from its surrounding context or the context from a given word. FastText is an extension of Word2Vec that also considers subword information.\n\n\n\n\n\n11.1.3 Sparse Representation: One-Hot Encoding\nOne of the simplest forms of embedding is one-hot encoding, a type of sparse representation. In this method, each element (such as a word) in a collection (e.g., sentence, document) is represented as a vector where the target element is assigned a value of 1, and all other elements are assigned a value of 0.\nFor example, if we have the sentence “all is well” with the words “all,” “is,” and “well,” the one-hot encoding representation for these words would be:\n\n\n\nWord\nall\nis\nwell\n\n\n\n\nall\n1\n0\n0\n\n\nis\n0\n1\n0\n\n\nwell\n0\n0\n1\n\n\n\n[Figure] One-Hot Encoding Method of Embedding\nThis table represents the one-hot encoding embedding of the words “all,” “is,” and “well,” where each word is assigned a unique vector. Only the position corresponding to the word has a value of 1, and all other positions are 0.\n\n11.1.3.1 Advantages:\n\nIt is easy to understand which words are being used, as the representation is simple.\n\n\n\n11.1.3.2 Disadvantages:\n\nAs the number of words increases, the size of the vector grows, leading to high memory usage.\nOne-hot encoding does not capture any semantic similarity between words; for example, “king” and “queen” would be completely different.\n\nThe document-term matrix (DTM) is an extension of one-hot encoding at the document level. The image also mentions the term frequency-inverse document frequency (TF-IDF), which is another approach that assigns importance to words based on how often they appear across multiple documents.\n\n\n\n11.1.4 Distributed Representation\nThe second major embedding type described is distributed representation, where words are represented as vectors with real-numbered values. Unlike one-hot encoding, distributed representation captures semantic relationships between words. For example, the word “all” could be represented by a vector like [-0.0031, -0.0421, 0.0125, 0.0062].\nHere is the translation of the table from the image to English:\n\n\n\nWord\nDimension 1\nDimension 2\nDimension 3\nDimension 4\n\n\n\n\nall\n-0.0031\n-0.0421\n0.0125\n0.0062\n\n\nis\n0.0212\n0.0125\n-0.0089\n0.0376\n\n\nwell\n-0.0543\n0.0684\n-0.0023\n0.0181\n\n\n\n[Figure] Distributed Representation Method of Embedding\nThis table shows an example of distributed embeddings for the words “all,” “is,” and “well,” where each word is represented as a vector in a 4-dimensional space. Each dimension has a real number value, capturing more information about the relationships between words compared to one-hot encoding.\n\n11.1.4.1 Advantages:\n\nDistributed representations can capture the semantic meaning of words.\nThe dimensionality of vectors does not increase as more words are added.\n\nThis method is widely used in natural language processing today because it can efficiently capture word meanings and relationships. Techniques like Word2Vec calculate these word embeddings. Two popular models for Word2Vec are CBOW (Continuous Bag of Words Model) and Skip-Gram (see Chapter 12. Word Embedding and Similarity)",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Text Representation Based on Counts</span>"
    ]
  },
  {
    "objectID": "11.Text_Representation.html#examples-of-text-representation-methods",
    "href": "11.Text_Representation.html#examples-of-text-representation-methods",
    "title": "11  Text Representation Based on Counts",
    "section": "11.2 Examples of Text Representation Methods",
    "text": "11.2 Examples of Text Representation Methods\n\n11.2.1 Document-Term Matrix (DTM)\n\n\n11.2.2 Concept of Document-Term Matrix\nA Document-Term Matrix (DTM) is a matrix that represents the frequency of terms occurring in a set of documents. This matrix is widely used in natural language processing (NLP) for tasks such as text mining, information retrieval, and topic modeling.\nimport pandas as pd\n\n# Example Document-Term Matrix (DTM) data\ndata = {\n    'word1': [4, 0, 0, 3, 0, 1],\n    'word2': [0, 1, 5, 0, 2, 0],\n    'word3': [0, 7, 0, 8, 1, 4],\n    'word4': [0, 0, 3, 0, 0, 0],\n    'word5': [1, 0, 0, 1, 2, 4],\n    'word6': [2, 1, 9, 0, 12, 0],\n    'wordN': [0, 3, 1, 0, 4, 2],\n}\n\n# Creating the Document-Term Matrix DataFrame\ndtm_df = pd.DataFrame(data, index=[f'doc{i+1}' for i in range(6)])\n\ndtm_df\n\n\n\n11.2.3 Understanding the Components:\n\nRows (doc1, doc, … ,doc6): Each row represents a document in the corpus.\nColumns (word1, word2, … , wordN): Each column represents a unique word (term) in the corpus.\nValues: The numbers in the matrix represent the frequency of each term in the corresponding document. For instance, in document ( doc1 ), term ( word1 ) appears 4 times, while term ( word1 ) appears once.\n\nHere is an example of how to create a Document-Term Matrix using Python.\n# Import necessary libraries\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport pandas as pd\n\n# Define sample documents\ndocuments = [ \n    \"Sir Walter has resented it.  As the head of the house, he felt that he ought to have been consulted, especially after taking the young man so publicly by the hand\",\n    \"Of Man's first disobedience, and the fruit Of that forbidden tree whose mortal taste Brought death into the World, and all our woe,  With loss of Eden, till one greater Man  Restore us, and regain the blissful seat, \",\n    \"Come, said my soul, Such verses for my Body let us write, (for we are one,) That should I after return, Or, long, long hence, in other spheres, There to some group of mates the chants resuming, (Tallying Earth's soil, trees, winds, tumultuous waves,) Ever with pleas'd smile I may keep on, Ever and ever yet the verses owning--as, first, I here and now Signing for Soul and Body, set to them my name,\"\n]\n\n# Initialize CountVectorizer to count word frequencies\nvectorizer = CountVectorizer()\n\n# Fit and transform the documents to generate a word count vector\nX = vectorizer.fit_transform(documents)\n\n# Get the list of words\nwords = vectorizer.get_feature_names_out()\n\n# Convert the result to a DataFrame for better visualization\ndf_bow = pd.DataFrame(X.toarray(), columns=words)\n\n# Display the result\ndf_bow\nThis code creates a Document-Term Matrix from three sample documents. The matrix counts the occurrences of each word in the documents and presents it in the following format:\n\nThis structure allows for further analysis such as identifying important words in a document or across documents, performing clustering, or calculating term importance metrics like TF-IDF (Term Frequency-Inverse Document Frequency).",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Text Representation Based on Counts</span>"
    ]
  },
  {
    "objectID": "11.Text_Representation.html#tf-idf-term-frequency-inverse-document-frequency",
    "href": "11.Text_Representation.html#tf-idf-term-frequency-inverse-document-frequency",
    "title": "11  Text Representation Based on Counts",
    "section": "11.3 TF-IDF (Term Frequency-Inverse Document Frequency)",
    "text": "11.3 TF-IDF (Term Frequency-Inverse Document Frequency)\n\n11.3.1 Concept of TF-IDF\nTF-IDF is a method used to evaluate the importance of a word in a document relative to a collection of documents. TF (Term Frequency) measures how frequently a term appears in a document, while IDF (Inverse Document Frequency) quantifies how rare the term is across all documents. A higher TF-IDF score indicates that the term is important in the document but appears infrequently in other documents.\n\n\n11.3.2 Term Frequency (TF)\nThe TF of a term measures how frequently a term occurs in a document. It is calculated as:\n\\[TF(t, d) = \\frac{\\text{Number of times term } t \\text{ appears in document } d}{\\text{Total number of terms in document } d}\\]\n\n\n11.3.3 Inverse Document Frequency (IDF)\nThe IDF of a term measures how important a term is in the whole corpus. Words that are common across many documents (e.g., “the”, “is”, etc.) receive a low score. The formula is:\n\\[\nIDF(t) = \\log\\left(\\frac{N}{1 + \\text{Number of documents containing term } t}\\right)\n\\] Where:\n\nN is the total number of documents.\nThe “+1” in the denominator is added to prevent division by zero.\n\n\n\n11.3.4 TF-IDF Score\nThe TF-IDF score is computed by multiplying TF and IDF:\n\\[ TF\\text{-}IDF(t, d) = TF(t, d) \\times IDF(t) \\]\n\n\n11.3.5 Example Python Code Using sklearn for TF-IDF\nLet’s calculate the TF-IDF scores for a small example corpus using Python.\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n# Sample corpus of documents\ndocuments = [\n    \"The cat sat on the mat\",\n    \"The dog sat on the log\",\n    \"Cats and dogs are pets\"\n]\n\n# Create the TF-IDF vectorizer\nvectorizer = TfidfVectorizer()\n\n# Fit the vectorizer on the sample documents\ntfidf_matrix = vectorizer.fit_transform(documents)\n\n# Get the feature names (terms)\nterms = vectorizer.get_feature_names_out()\n\n# Convert the TF-IDF matrix into a Pandas DataFrame for better readability\nimport pandas as pd\ntfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=terms)\n\nprint(tfidf_df)\n\nThis approach helps measure the relative importance of words in the documents and can be used for various NLP tasks like text classification, document clustering, etc.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Text Representation Based on Counts</span>"
    ]
  },
  {
    "objectID": "11.Text_Representation.html#comparison-of-dtm-and-tf-idf",
    "href": "11.Text_Representation.html#comparison-of-dtm-and-tf-idf",
    "title": "11  Text Representation Based on Counts",
    "section": "11.4 Comparison of DTM and TF-IDF",
    "text": "11.4 Comparison of DTM and TF-IDF\n\nDTM simply counts how many times each word appears in the document, without considering how common the word is in the entire document collection.\nTF-IDF combines frequency information with the rarity of the word across all documents, thus highlighting words that are important in one document but uncommon in the overall corpus.\n\nBoth methods are useful for tasks like calculating document similarity, extracting keywords, and analyzing the importance of specific terms in a corpus.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Text Representation Based on Counts</span>"
    ]
  },
  {
    "objectID": "11.Text_Representation.html#conclusion",
    "href": "11.Text_Representation.html#conclusion",
    "title": "11  Text Representation Based on Counts",
    "section": "11.5 Conclusion",
    "text": "11.5 Conclusion\nCount-based word representation methods like DTM and TF-IDF are fundamental tools for converting text into numerical formats, making it easier to apply statistical approaches to natural language data. In this chapter, we explored the concepts and Python implementations of both methods, providing a foundation for more complex NLP tasks.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Text Representation Based on Counts</span>"
    ]
  },
  {
    "objectID": "12.Word_Embedding.html",
    "href": "12.Word_Embedding.html",
    "title": "12  Word Embedding and Relational Similarity",
    "section": "",
    "text": "12.1 Word Embedding",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Word Embedding and Relational Similarity</span>"
    ]
  },
  {
    "objectID": "12.Word_Embedding.html#word-embedding",
    "href": "12.Word_Embedding.html#word-embedding",
    "title": "12  Word Embedding and Relational Similarity",
    "section": "",
    "text": "12.1.1 Overview\nWord Embedding refers to a technique where words are mapped to vectors of real numbers, typically in a lower-dimensional space. These vectors capture the semantic meaning of words. Unlike TF-IDF, embeddings capture semantic similarity—words with similar meanings will have closer vectors.\nPopular models for word embeddings include:\n\nWord2Vec: Predicts context words given a target word (CBOW) or predicts the target word given the context (Skip-gram).\nGloVe: Focuses on matrix factorization, capturing global statistical information about word co-occurrences.\nFastText: Extends Word2Vec by considering subword information, which helps model morphology.\n\n\n\n12.1.2 What is Word2Vec?\nWord2Vec is a model used to learn the relationships between words based on their co-occurrence in a corpus. It creates a dense vector representation of words such that words with similar meanings are closer in the vector space.\nThere are two main architectures for Word2Vec:\n\nCBOW (Continuous Bag of Words): Predict the current word from surrounding words.\nSkip-gram: Predict surrounding words given the current word.\n\n\n\n\n12.1.3 Example in Python (Using Gensim Word2Vec)\nimport gensim\nfrom gensim.models import Word2Vec\n\n# Sample sentences\nsentences = [\n    ['natural', 'language', 'processing', 'is', 'fascinating'],\n    ['word', 'embedding', 'and', 'tf-idf', 'are', 'techniques'],\n    ['text', 'similarity', 'can', 'be', 'measured', 'using', 'vector', 'similarity']\n]\n\n# Train Word2Vec model\nmodel = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)\n\n# Check vector for a word\nword_vector = model.wv['similarity']\nprint(f\"Vector for 'similarity':\\n{word_vector}\")\n\n# Find similar words to 'text'\nsimilar_words = model.wv.most_similar('text')\nprint(f\"Words similar to 'text': {similar_words}\")\n\nIn this example, a simple Word2Vec model is trained on a small dataset. After training, we can retrieve word vectors and find similar words based on vector proximity.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Word Embedding and Relational Similarity</span>"
    ]
  },
  {
    "objectID": "12.Word_Embedding.html#step-by-step-example-using-gensims-word2vec",
    "href": "12.Word_Embedding.html#step-by-step-example-using-gensims-word2vec",
    "title": "12  Word Embedding and Relational Similarity",
    "section": "12.2 Step-by-Step Example Using Gensim’s Word2Vec",
    "text": "12.2 Step-by-Step Example Using Gensim’s Word2Vec\n\n12.2.1 Step 1: Installing Gensim\nTo use Word2Vec in Python, you will need to install Gensim. You can install it using pip:\npip install gensim\n\n\n12.2.2 Step 2: Import Required Libraries\nWe will import the necessary libraries for training the Word2Vec model.\nimport gensim\nfrom gensim.models import Word2Vec\nimport nltk\nfrom nltk.corpus import brown\nnltk.download('brown')  # We will use the Brown corpus for this example\n\n\n12.2.3 Step 3: Preparing the Data\nWe will use the Brown corpus provided by NLTK, which contains a variety of text genres. First, let’s tokenize the sentences into words.\n# Load the Brown corpus sentences\nsentences = brown.sents()\n\n# Print a sample sentence\nprint(sentences[0])\n\n\n12.2.4 Step 4: Training the Word2Vec Model\nNow that we have the tokenized sentences, we can train the Word2Vec model using Gensim. The important parameters are:\n\nsentences: The training data (list of tokenized sentences).\nvector_size: The dimensionality of the word vectors.\nwindow: The maximum distance between the current and predicted word.\nmin_count: Ignores all words with a total frequency lower than this.\n\n# Train the Word2Vec model\nmodel = Word2Vec(sentences, vector_size=100, window=5, min_count=5, workers=4)\n\n# Save the model\nmodel.save(\"word2vec_brown.model\")\n\n\n12.2.5 Step 5: Exploring the Word Embeddings\nOnce the model is trained, you can explore the word embeddings. You can find words similar to a given word, check the cosine similarity between words, and more.\n\n12.2.5.1 Finding the Most Similar Words\n# Load the trained model\nmodel = Word2Vec.load(\"word2vec_brown.model\")\n\n# Find words most similar to 'king'\nsimilar_words = model.wv.most_similar('king')\nprint(similar_words)\n\n\n12.2.5.2 Word Vector Representation\nEach word is represented by a vector. You can see the vector representation of a specific word.\n# Get the vector for a specific word\nvector = model.wv['king']\nprint(vector)\n\n\n12.2.5.3 Cosine Similarity Between Words\nCosine similarity is commonly used to measure the similarity between word vectors.\n# Calculate cosine similarity between two words\nsimilarity = model.wv.similarity('king', 'queen')\nprint(f\"Cosine similarity between 'king' and 'queen': {similarity}\")\n\n\n\n12.2.6 Step 6: Visualizing Word Embeddings (Optional)\nTo visualize word embeddings in a 2D space, we can use dimensionality reduction techniques like PCA (Principal Component Analysis) or t-SNE. Below is an example using PCA.\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\n\n# Select a few words to visualize\nwords = ['king', 'queen', 'man', 'woman', 'dog', 'cat']\nword_vectors = [model.wv[word] for word in words]\n\n# Reduce the dimensionality of word vectors using PCA\npca = PCA(n_components=2)\nword_vectors_2d = pca.fit_transform(word_vectors)\n\n# Plot the 2D word vectors\nplt.figure(figsize=(10, 5))\nfor i, word in enumerate(words):\n    plt.scatter(word_vectors_2d[i, 0], word_vectors_2d[i, 1])\n    plt.text(word_vectors_2d[i, 0] + 0.05, word_vectors_2d[i, 1] + 0.05, word)\nplt.show()\nOutput:\n\n\n\n12.2.7 Conclusion\nIn this tutorial, we explored the Word2Vec model using Gensim in Python. We trained a Word2Vec model using the Brown corpus, explored word embeddings, and visualized the vectors in a 2D space. Word embeddings are a powerful tool for capturing the semantic relationships between words and are widely used in natural language processing tasks.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Word Embedding and Relational Similarity</span>"
    ]
  },
  {
    "objectID": "12.Word_Embedding.html#vector-similarity",
    "href": "12.Word_Embedding.html#vector-similarity",
    "title": "12  Word Embedding and Relational Similarity",
    "section": "12.3 Vector Similarity",
    "text": "12.3 Vector Similarity\nOnce text data is transformed into vectors (using methods like TF-IDF or word embeddings), we can compute similarity between documents or words. A common method for measuring similarity is Cosine Similarity, which measures the cosine of the angle between two vectors.\n\\[\\text{cosine similarity}(A, B) = \\frac{A \\cdot B}{\\|A\\| \\|B\\|} \\]\nWhere:\n\n\\(A \\cdot B\\) is the dot product of the vectors.\n\\(\\|A\\|\\) and \\(\\|B\\|\\) are the magnitudes (norms) of vectors (A) and (B).\n\n\n12.3.1 Example in Python (Cosine Similarity)\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport numpy as np\n\n# Example vectors (from TF-IDF or word embedding)\nvector_a = np.array([0.1, 0.3, 0.7])\nvector_b = np.array([0.2, 0.4, 0.5])\n\n# Compute cosine similarity\nsimilarity = cosine_similarity([vector_a], [vector_b])\nprint(f\"Cosine Similarity between vector_a and vector_b: {similarity[0][0]}\")\nOutput:\nCosine Similarity between vector_a and vector_b: 0.9509634325746505",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Word Embedding and Relational Similarity</span>"
    ]
  },
  {
    "objectID": "12.Word_Embedding.html#using-pre-trained-word-vectors-using-spacy",
    "href": "12.Word_Embedding.html#using-pre-trained-word-vectors-using-spacy",
    "title": "12  Word Embedding and Relational Similarity",
    "section": "12.4 Using Pre-trained Word Vectors (using spaCy)",
    "text": "12.4 Using Pre-trained Word Vectors (using spaCy)\nBelow is an example of how to compute document similarity using spaCy, which is a powerful NLP library. This code uses spaCy’s pre-trained word vectors to compute similarity between documents.\n\n12.4.1 Install spaCy and Download Language Model\nIf you haven’t already installed spaCy and its language model, you can do so using the following commands:\npip install spacy\npython -m spacy download en_core_web_md  # Medium-sized English model with word vectors\n\n\n12.4.2 Example Code: Document Similarity Using spaCy\nHere is a Python script that demonstrates how to compute similarity between two documents using spaCy:\nimport spacy\n\n# Load the spaCy language model (with word vectors)\nnlp = spacy.load('en_core_web_md')\n\n# Define two sample documents\ndoc1 = nlp(\"Natural language processing is a fascinating field of AI that focuses on the interaction between humans and computers through language.\")\ndoc2 = nlp(\"Machine learning is a part of AI that allows computers to learn from data without being explicitly programmed.\")\n\n# Compute similarity between the two documents\nsimilarity = doc1.similarity(doc2)\n\nprint(f\"Similarity between doc1 and doc2: {similarity:.4f}\")\n\n# Compare additional documents for similarity\ndoc3 = nlp(\"Understanding how humans speak is the first step in improving human-computer interaction.\")\nsimilarity_with_doc3 = doc1.similarity(doc3)\nprint(f\"Similarity between doc1 and doc3: {similarity_with_doc3:.4f}\")\n\ndoc4 = nlp(\"I love hiking in the mountains.\")\nsimilarity_with_doc4 = doc1.similarity(doc4)\nprint(f\"Similarity between doc1 and doc4: {similarity_with_doc4:.4f}\")\n\n\n12.4.3 Explanation:\n\nLoading the Model: We load the medium-sized en_core_web_md model in spaCy, which contains word vectors necessary for computing similarity.\nCreating Documents: We create document objects (doc1, doc2, etc.) by passing strings to the nlp object, which processes them.\nComputing Similarity: The doc1.similarity(doc2) method computes the similarity between two documents using word vectors. This similarity score is a number between 0 and 1, where 1 indicates perfect similarity.\nAdditional Comparisons: We also compare doc1 with doc3 and doc4 to see how similar different texts are.\n\n\n\n12.4.4 Output:\nYou will get an output that shows similarity scores for each pair of documents. A higher similarity score means the documents are more closely related in terms of their content.\nSimilarity between doc1 and doc2: 0.9172\nSimilarity between doc1 and doc3: 0.9096\nSimilarity between doc1 and doc4: 0.7760\n\n\n12.4.5 Notes:\n\nSimilarity Score: This is computed based on the semantic content of the text using pre-trained word vectors. It considers the meaning of words, so two documents that use different words but are semantically similar should have a high similarity score.\nWord Vector Model: You could also use a larger or smaller language model depending on your needs (en_core_web_sm for a small model, or en_core_web_lg for a large model).\n\n\n\n12.4.6 Summary:\nThis code computes the similarity between different documents using spaCy’s pre-trained word vectors. You can use this method to compare documents in various NLP tasks, such as document clustering, summarization, or recommendation systems.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Word Embedding and Relational Similarity</span>"
    ]
  },
  {
    "objectID": "12.Word_Embedding.html#spacys-vs.-gensims-word2vec",
    "href": "12.Word_Embedding.html#spacys-vs.-gensims-word2vec",
    "title": "12  Word Embedding and Relational Similarity",
    "section": "12.5 spaCy’s vs. Gensim’s Word2Vec",
    "text": "12.5 spaCy’s vs. Gensim’s Word2Vec\n\n12.5.0.1 Pre-trained vs Custom Training\n\nspaCy’s Pre-trained Word Vectors:\n\nPre-trained: spaCy comes with pre-trained word vectors as part of its language models, such as en_core_web_md and en_core_web_lg. These vectors have already been trained on large datasets (like Common Crawl) and are ready to use out of the box.\nCannot Retrain Easily: spaCy is designed primarily for loading and using pre-trained models. While it is possible to integrate custom vectors into spaCy, it’s not built for training word vectors from scratch.\n\nGensim Word2Vec:\n\nCustomizable Training: Gensim’s Word2Vec is a flexible tool that allows you to train word vectors on your own corpus. You can adjust the training parameters (e.g., window, vector_size, etc.) to suit your specific needs.\nNo Pre-trained Models: Gensim provides the implementation of Word2Vec, but you need to either train the model on your data or use pre-trained vectors from external sources like Google News.\n\n\n\n\n12.5.0.2 Training Algorithm\n\nspaCy:\n\nSpaCy uses pre-trained models that are typically trained using Word2Vec-like or FastText-like algorithms, but the specifics may vary depending on the source of the vectors.\nSpaCy’s pre-trained models also include other components such as dependency parsers, named entity recognizers, etc., in addition to word vectors.\n\nGensim Word2Vec:\n\nGensim implements the original Word2Vec algorithm as described by Mikolov et al. It offers two main algorithms:\n\nSkip-gram: Predicts surrounding words given a target word.\nCBOW (Continuous Bag of Words): Predicts a target word from surrounding words.\n\nIt is a more specific tool aimed at training word vectors.\n\n\n\n\n12.5.0.3 Vector Quality and Size\n\nspaCy:\n\nSpaCy offers models like en_core_web_sm, en_core_web_md, and en_core_web_lg:\n\nsm (small model): Does not contain word vectors and only includes word embeddings based on context.\nmd (medium model): Includes word vectors, but they are smaller and less fine-grained (300 dimensions).\nlg (large model): Contains more detailed word vectors and a larger vocabulary (300 dimensions).\n\nThe quality of vectors depends on the size of the model you load.\n\nGensim Word2Vec:\n\nGensim allows you to specify the vector size, the window size, the min_count (to ignore infrequent words), and other hyperparameters during training. This means you can control the trade-off between model complexity and the quality of word embeddings.\nGensim also allows for training on domain-specific corpora, which can give higher-quality word embeddings for specific use cases (e.g., medical, legal text).\n\n\n\n\n12.5.0.4 Use Case Differences\n\nspaCy:\n\nBest for Pre-trained NLP Pipelines: SpaCy is primarily used for building NLP pipelines and includes tools for tokenization, part-of-speech tagging, dependency parsing, named entity recognition, and more. Word vectors are just one part of spaCy’s broader functionality.\nOut-of-the-Box Similarity: If you need to compute similarities between documents or words quickly without training your own model, spaCy’s pre-trained vectors are an excellent choice.\n\nGensim Word2Vec:\n\nBest for Custom Training on Specific Domains: If you have a large corpus specific to your domain (e.g., legal, medical, or technical text) and you want to train your own word embeddings, Gensim’s Word2Vec is the tool to use.\nFlexible and Customizable: Gensim is ideal when you want to experiment with different training parameters or create embeddings for specialized applications.\n\n\n\n\n12.5.0.5 Performance and Speed\n\nspaCy:\n\nSpaCy is optimized for high performance. When using pre-trained vectors, it’s designed to be fast, especially for inference tasks like similarity comparisons.\n\nGensim Word2Vec:\n\nGensim is optimized for large-scale training and can work with very large corpora efficiently. However, training from scratch on large datasets can take some time depending on the size of your data and chosen parameters.\n\n\n\n\n12.5.0.6 Additional Features\n\nspaCy:\n\nSpaCy’s models are multi-functional, including components for parsing, tagging, named entity recognition, etc. Word vectors are just one feature among many.\nSpaCy focuses on providing an end-to-end pipeline for various NLP tasks, making it a comprehensive tool for many tasks, but less specialized in word vector training.\n\nGensim Word2Vec:\n\nGensim is a specialized tool for generating word embeddings and working with them. While it does not provide the full NLP pipeline like spaCy, it excels in generating custom word embeddings and performing tasks like topic modeling, document similarity, and more.\n\n\nIn summary, spaCy’s pre-trained word vectors are great for general-purpose applications and fast deployment, while Gensim’s Word2Vec is better for training custom embeddings on specialized data.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Word Embedding and Relational Similarity</span>"
    ]
  },
  {
    "objectID": "12.Word_Embedding.html#relational-similarity",
    "href": "12.Word_Embedding.html#relational-similarity",
    "title": "12  Word Embedding and Relational Similarity",
    "section": "12.6 Relational Similarity",
    "text": "12.6 Relational Similarity\nThe famous example (see https://arxiv.org/abs/1509.01692) of “King” - “man” + “woman” = “Queen” demonstrates how word embeddings can capture semantic relationships between words. This is often referred to as vector arithmetic or analogy in word embeddings.\nYou can use both Gensim’s Word2Vec and spaCy to demonstrate this kind of analogy. Below is an example using both libraries:\n\n12.6.1 Using Gensim Word2Vec\nIf you already have a pre-trained Word2Vec model (like Google News vectors or one trained on your own corpus), you can use Gensim to perform the analogy.\nimport gensim.downloader as api\n\n# Load pre-trained Word2Vec model (Google News vectors)\n# This might take a few minutes to download (around 1.6GB)\nmodel = api.load(\"word2vec-google-news-300\")\n\n# Perform the vector arithmetic: King - Man + Woman = ?\nresult = model.most_similar(positive=['king', 'woman'], negative=['man'])\n\nprint(f\"Result: {result[0]}\")\nExplanation:\n\nPositive Words: Words that should be added (king and woman).\nNegative Words: Words that should be subtracted (man).\nResult: This will return the top word that matches the resulting vector after the arithmetic operation.\n\nExample Output:\nResult: ('queen', 0.7118192911148071)\n\nThis indicates that the word closest to the result of the vector arithmetic “King” - “man” + “woman” is “Queen” with a similarity score of 0.71.\n\n\n12.6.2 Using spaCy\nIf you are using spaCy with a pre-trained model, such as en_core_web_md or en_core_web_lg, you can perform similar vector arithmetic.\nimport spacy\n\n# Load spaCy medium or large model (that includes word vectors)\n#nlp = spacy.load(\"en_core_web_md\")\nnlp = spacy.load(\"en_core_web_lg\")\n\n# Get vectors for words\nking = nlp.vocab['king'].vector\nman = nlp.vocab['man'].vector\nwoman = nlp.vocab['woman'].vector\n\n# Perform vector arithmetic: King - Man + Woman\nresult_vector = king - man + woman\n\n# Find the word closest to the result vector\nsimilar_word = None\nmax_similarity = -1\n\nfor word in nlp.vocab:\n    if word.has_vector and word.is_lower and word.is_alpha:\n        similarity = result_vector.dot(word.vector)\n        if similarity &gt; max_similarity:\n            max_similarity = similarity\n            similar_word = word\n\nprint(f\"Result: {similar_word.text}, Similarity: {max_similarity}\")\nExplanation:\n\nVector Operations: We manually subtract and add the word vectors for king, man, and woman.\nFinding Similar Words: We compare the resulting vector to all other word vectors in the model’s vocabulary using a dot product to find the most similar word.\nPerformance: This might take a bit longer depending on your system since it loops through the entire vocabulary.\n\nExample Output:\nResult: woman, Similarity: 69.10123443603516\nBoth methods achieve the same goal of showing that “King” - “man” + “woman” results in a vector closest to “Queen”.\nEnd.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Word Embedding and Relational Similarity</span>"
    ]
  },
  {
    "objectID": "13.NLP_toolkit.html",
    "href": "13.NLP_toolkit.html",
    "title": "13  Word Embedding Activities",
    "section": "",
    "text": "13.0.1 Lexical Semantics and Word Meaning",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Word Embedding Activities</span>"
    ]
  },
  {
    "objectID": "13.NLP_toolkit.html#lexical-semantics-and-word-meaning-1",
    "href": "13.NLP_toolkit.html#lexical-semantics-and-word-meaning-1",
    "title": "13  Word Embedding Activities",
    "section": "13.1 Lexical Semantics and Word Meaning",
    "text": "13.1 Lexical Semantics and Word Meaning\nTo conduct synonym detection and word sense disambiguation using word embeddings in Python, we can use popular libraries like gensim, spaCy, or transformers that provide pre-trained word embeddings. Below are step-by-step examples for both synonym detection and word sense disambiguation.\n\n13.1.1 Synonym Detection Using Word Embeddings\nWe can detect synonyms by checking the similarity between word vectors in the embedding space. Here’s an example using the gensim library with the pre-trained Word2Vec model.\n\n13.1.1.1 Steps:\n\nInstall necessary libraries:\npip install gensim spacy\npython -m spacy download en_core_web_sm  # Download the English model for spaCy\nLoad pre-trained word embeddings and find similar words:\nimport gensim.downloader as api\n\n# Load a pre-trained Word2Vec model from Gensim\nmodel = api.load(\"word2vec-google-news-300\")  # A popular pre-trained word2vec model\n\n# Example word for synonym detection\nword = \"happy\"\n\n# Get top 5 most similar words to the target word\nsimilar_words = model.most_similar(word, topn=5)\n\nprint(f\"Top 5 synonyms for '{word}':\")\nfor similar_word, similarity_score in similar_words:\n    print(f\"{similar_word} ({similarity_score})\")\n\nThis will output the top 5 words that are most similar to “happy” based on their proximity in the embedding space.\nSample Output:\nTop 5 synonyms for 'happy':\njoyful (0.714)\ncheerful (0.701)\ncontent (0.689)\ndelighted (0.678)\nelated (0.665)\nTo filter out words that share the same part-of-speech (POS) as the target word when performing synonym detection, we need to combine the word embedding approach with POS tagging. This ensures that the similar words returned are not only semantically related but also belong to the same grammatical category (e.g., noun, verb, adjective).\nWe can achieve this by using a POS tagger from a library like spaCy, which allows us to tag words and filter out only those with the same POS as the target word.\nPython Code to Show All POS Tags in spaCy:\nimport spacy\nfrom spacy.symbols import POS\n\n# Load the spaCy English model\nnlp = spacy.load(\"en_core_web_sm\")\n\n# List all available POS tags in spaCy with their explanations\npos_tags = nlp.get_pipe(\"tagger\").labels\n\nprint(\"All available POS tags in spaCy:\")\nfor pos in pos_tags:\n    print(f\"{pos}: {spacy.explain(pos)}\")\nOutput:\nAll available POS tags in spaCy:\n$: symbol, currency\n'': closing quotation mark\n,: punctuation mark, comma\n-LRB-: left round bracket\n-RRB-: right round bracket\n.: punctuation mark, sentence closer\n:: punctuation mark, colon or ellipsis\nADD: email\nAFX: affix\nCC: conjunction, coordinating\nCD: cardinal number\nDT: determiner\nEX: existential there\nFW: foreign word\nHYPH: punctuation mark, hyphen\nIN: conjunction, subordinating or preposition\nJJ: adjective (English), other noun-modifier (Chinese)\nJJR: adjective, comparative\nJJS: adjective, superlative\nLS: list item marker\nMD: verb, modal auxiliary\nNFP: superfluous punctuation\nNN: noun, singular or mass\nNNP: noun, proper singular\nNNPS: noun, proper plural\nNNS: noun, plural\nPDT: predeterminer\nPOS: possessive ending\nPRP: pronoun, personal\nPRP$: pronoun, possessive\nRB: adverb\nRBR: adverb, comparative\nRBS: adverb, superlative\nRP: adverb, particle\nSYM: symbol\nTO: infinitival \"to\"\nUH: interjection\nVB: verb, base form\nVBD: verb, past tense\nVBG: verb, gerund or present participle\nVBN: verb, past participle\nVBP: verb, non-3rd person singular present\nVBZ: verb, 3rd person singular present\nWDT: wh-determiner\nWP: wh-pronoun, personal\nWP$: wh-pronoun, possessive\nWRB: wh-adverb\nXX: unknown\n_SP: whitespace\n``: opening quotation mark\n\nHere’s the revised version of the code, where the “word” and “pos” assignment are handled separately.\n\n\n13.1.1.2 Revised Python Code:\nimport gensim.downloader as api\nimport spacy\n\n# Load pre-trained Word2Vec model from Gensim\nmodel = api.load(\"word2vec-google-news-300\")\n\n# Load spaCy POS tagger\nnlp = spacy.load(\"en_core_web_sm\")\n# Define a function to get the POS tag of a word\ndef get_pos(word):\n    doc = nlp(word)\n    return doc[0].pos_  # Returns the POS tag of the word\n\n# Function to find synonyms with the same POS\ndef find_synonyms_with_same_pos(word, topn=10):\n    try:\n        # Get the POS of the target word\n        word_pos = get_pos(word)\n\n        # Get the most similar words from the model\n        similar_words = model.most_similar(word, topn=topn)\n\n        # Filter similar words by POS tag\n        filtered_words = [\n            (w, sim) for w, sim in similar_words if get_pos(w) == word_pos\n        ]\n\n        return filtered_words\n    except KeyError:\n        print(f\"Word '{word}' not found in the model vocabulary.\")\n        return []\nSeparate input box for word and POS tagging\nword = \"happy\"  # Define the target word\npos = get_pos(word)  # Get the POS tag for the target word\n\n# Find synonyms with the same POS\nsynonyms_with_same_pos = find_synonyms_with_same_pos(word, topn=10)\n\n# Output the result\nprint(f\"Synonyms for '{word}' with the same POS ({pos}):\")\nfor synonym, similarity in synonyms_with_same_pos:\n    print(f\"{synonym} ({similarity})\")\nExample Output:\nFor word = \"happy\", the output will be something like:\nSynonyms for 'happy' with the same POS (ADJ):\njoyful (0.714)\ncheerful (0.701)\ndelighted (0.678)\ncontent (0.689)\necstatic (0.662)\n\n\n\n13.1.2 Word Sense Disambiguation Using Contextual Embeddings\nWord sense disambiguation (WSD) can be done by using contextual word embeddings, where the meaning of a word is determined by its context. Here’s an example using the transformers library (BERT embeddings) from Hugging Face.\n\n13.1.2.1 Steps:\n\nInstall necessary libraries:\npip install transformers torch\nUse BERT to generate contextual embeddings:\nfrom transformers import BertTokenizer, BertModel\n# BertModel: This is the actual pre-trained BERT model used to generate embeddings.\n\nimport torch\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n# Load pre-trained BERT model and tokenizer\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n# BertTokenizer.from_pretrained('bert-base-uncased'): Loads a pre-trained tokenizer for BERT. The bert-base-uncased model is a smaller, lower-cased version of BERT (where all text is converted to lowercase).\nmodel = BertModel.from_pretrained('bert-base-uncased')\n# BertModel.from_pretrained('bert-base-uncased'): Loads the pre-trained BERT model. This model outputs hidden states (embeddings) that can be used for various NLP tasks.\n\n\n# Sentences with the ambiguous word \"bank\"\nsentence_1 = \"He went to the bank to deposit money.\"\nsentence_2 = \"The river bank was full of fish.\"\n\n# Tokenize and get embeddings for both sentences\ninputs_1 = tokenizer(sentence_1, return_tensors=\"pt\")\ninputs_2 = tokenizer(sentence_2, return_tensors=\"pt\")\n\n# tokenizer(sentence_1, return_tensors=\"pt\"): This tokenizes the sentences into BERT's format and returns a PyTorch tensor (pt stands for PyTorch). BERT needs input to be tokenized into a numerical form (token IDs) that it can process.\n# It converts each word into subwords (tokens) and creates corresponding token IDs.\n# The result is a tensor, which is an array containing the numerical representation of each token.\n\n\nwith torch.no_grad():\n    outputs_1 = model(**inputs_1)\n    outputs_2 = model(**inputs_2)\n\n# torch.no_grad(): This disables gradient calculations (used for training models). Here, it saves memory and speeds up computations since we only need forward passes through the model to get the embeddings.\n# outputs_1 = model(**inputs_1): This runs the tokenized input through the BERT model. The model outputs hidden states or embeddings for each token in the sentence.\n# The hidden state captures the meaning of each word in the context of the entire sentence.\n\nembedding_1 = outputs_1.last_hidden_state[0, 4, :]  # Word \"bank\" in sentence 1\nembedding_2 = outputs_2.last_hidden_state[0, 2, :]  # Word \"bank\" in sentence 2\n\n# Extract the embeddings for the word \"bank\" (assuming the word is at index 5 in both cases)\n# The output hidden states are of shape (batch_size, sequence_length, hidden_size), we take the last hidden state\n# outputs_1.last_hidden_state: The output of BERT contains hidden states for all tokens in the sentence. This has the shape (batch_size, sequence_length, hidden_size) where:\n## batch_size: The number of sentences (in this case, it's 1).\n## sequence_length: The number of tokens in the sentence.\n## hidden_size: The size of the hidden state vector (768 dimensions for BERT).\n# [0, 5, :]: We access the embedding for the token at index 5 in both sentences. BERT generates embeddings for each token in the sentence, and this line assumes that the word \"bank\" is at index 5. The : means that we're extracting all the 768 dimensions of the embedding.\n# Note: Token indexes might differ depending on tokenization, so in a real application, you should find the correct index of the word \"bank\".\n\n# Compute cosine similarity between embeddings\nsimilarity = cosine_similarity(embedding_1.unsqueeze(0), embedding_2.unsqueeze(0))\n\n# cosine_similarity(embedding_1.unsqueeze(0), embedding_2.unsqueeze(0)): This computes the cosine similarity between the two embeddings. Cosine similarity is a measure of similarity between two vectors based on their orientation (not magnitude). It ranges from -1 (completely opposite) to 1 (exactly the same), with 0 indicating no similarity.\n# unsqueeze(0): This adds an extra dimension to the embedding to make it a 2D tensor, as cosine_similarity expects the input to be 2D.\n\n\nprint(f\"Similarity between 'bank' in two different contexts: {similarity[0][0]}\")\n\nIn this example:\n\nWe take the word “bank” in two different contexts: one financial (bank to deposit money) and one geographical (river bank).\nBERT creates embeddings for the word based on its surrounding context.\nCosine similarity is computed between these embeddings to determine how similar the meanings of “bank” are in both contexts.\n\nSample Output:\nSimilarity between 'bank' in two different contexts: 0.37\nThis low similarity score suggests that the word “bank” has different meanings in these two contexts (financial institution vs. riverside).\n\nTo revise the above code and ensure that the correct index of the word “bank” is used in both sentences, we need to account for the way BERT tokenizes the input. BERT uses subword tokenization, meaning that words can sometimes be split into multiple tokens. To ensure we find the correct index of “bank”, we need to first tokenize the sentences, then search for the token ID that corresponds to “bank” within the tokenized input.\nHere’s how to revise the code:\n\n\n\n13.1.3 Revised Python Code:\nfrom transformers import BertTokenizer, BertModel\nimport torch\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n# Load pre-trained BERT model and tokenizer\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nmodel = BertModel.from_pretrained('bert-base-uncased')\n\n# Sentences with the ambiguous word \"bank\"\nsentence_1 = \"He went to the bank to deposit money.\"\nsentence_2 = \"The river bank was full of fish.\"\n\n# Tokenize and get embeddings for both sentences\ninputs_1 = tokenizer(sentence_1, return_tensors=\"pt\")\ninputs_2 = tokenizer(sentence_2, return_tensors=\"pt\")\n\n# Tokenized input with subword tokens\ntokens_1 = tokenizer.tokenize(sentence_1)\ntokens_2 = tokenizer.tokenize(sentence_2)\n\n# Find the index of the token \"bank\" in both tokenized sentences\nindex_1 = tokens_1.index(\"bank\")\nindex_2 = tokens_2.index(\"bank\")\nprint(index_1)\nprint(index_2)\n\n# Convert to tensor input for BERT\ninputs_1 = tokenizer(sentence_1, return_tensors=\"pt\")\ninputs_2 = tokenizer(sentence_2, return_tensors=\"pt\")\n\nwith torch.no_grad():\n    outputs_1 = model(**inputs_1)\n    outputs_2 = model(**inputs_2)\n\n# Extract the embeddings for the word \"bank\" using the correct index\nembedding_1 = outputs_1.last_hidden_state[0, index_1 + 1, :]  # +1 due to [CLS] token at index 0\nembedding_2 = outputs_2.last_hidden_state[0, index_2 + 1, :]  # +1 due to [CLS] token at index 0\n\n# Compute cosine similarity between embeddings\nsimilarity = cosine_similarity(embedding_1.unsqueeze(0), embedding_2.unsqueeze(0))\n\nprint(f\"Similarity between 'bank' in two different contexts: {similarity[0][0]}\")\nKey Changes:\n\nTokenization:\n\ntokenizer.tokenize(sentence_1): This tokenizes each sentence into subword tokens.\ntokens_1.index(\"bank\"): Finds the correct index of the word “bank” in the tokenized input.\n\nCorrect Index Adjustment:\n\nIn BERT’s input format, the sequence starts with a [CLS] token at index 0, so the actual index of the word “bank” is index + 1. This is why we add 1 to the token index to get the correct location in the hidden states.\n\nEmbedding Extraction:\n\nThe embedding for the word “bank” is extracted based on the calculated index in each sentence.\n\n\nExample Output:\nThis code should give you the similarity score for the word “bank” in the two different contexts. If the meanings are different (as expected here), the similarity score will be low.\nFor example:\n4\n2\nSimilarity between 'bank' in two different contexts: 0.43\nThis low similarity score indicates that the word “bank” has different meanings in these contexts (financial institution vs. riverside).",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Word Embedding Activities</span>"
    ]
  },
  {
    "objectID": "13.NLP_toolkit.html#corpus-linguistics-with-word-embeddings",
    "href": "13.NLP_toolkit.html#corpus-linguistics-with-word-embeddings",
    "title": "13  Word Embedding Activities",
    "section": "13.2 Corpus Linguistics with Word Embeddings",
    "text": "13.2 Corpus Linguistics with Word Embeddings\nIn corpus linguistics, word embeddings can be applied to two key tasks: collocation analysis and semantic similarity. Below are Python implementations for both activities, along with detailed explanations.\n\n13.2.1 Collocation Analysis Using Word Embeddings\nCollocations are word pairings that frequently occur together in a language and exhibit specific patterns. Word embeddings can help identify semantically related word pairs based on their proximity in vector space.\n\n13.2.1.1 Steps:\n\nLoad pre-trained word embeddings (such as Word2Vec).\nExtract word pairs (collocations) based on their co-occurrence and proximity in embedding space.\nSort the word pairs by their similarity score to identify common collocations.\n\n\n\n13.2.1.2 Python Code for Collocation Analysis:\nimport gensim.downloader as api\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n# Load a pre-trained Word2Vec model (Google News Vectors)\nmodel = api.load(\"word2vec-google-news-300\")\n\n# List of word pairs you want to analyze for collocation\nword_pairs = [\n    ('quick', 'fox'),\n    ('lazy', 'dog'),\n    ('king', 'queen'),\n    ('strong', 'weak'),\n    ('bank', 'money')\n]\n\n# Function to calculate cosine similarity between two words\ndef get_similarity(word1, word2):\n    try:\n        vec1 = model[word1]\n        vec2 = model[word2]\n        similarity = cosine_similarity([vec1], [vec2])[0][0]\n        return similarity\n    except KeyError:\n        return None  # If word not in vocabulary\n\n# Find collocations by calculating similarity\ncollocations = []\nfor word1, word2 in word_pairs:\n    similarity = get_similarity(word1, word2)\n    if similarity is not None:\n        collocations.append((word1, word2, similarity))\n\n# Sort by similarity score\ncollocations.sort(key=lambda x: x[2], reverse=True)\n\n# Display the collocations and their similarity scores\nprint(\"Collocations and their similarity scores:\")\nfor word1, word2, sim in collocations:\n    print(f\"{word1} - {word2}: {sim:.3f}\")\nExplanation:\n\nModel: This code uses a pre-trained Word2Vec model (word2vec-google-news-300), which contains embeddings for millions of words.\nCosine Similarity: The similarity between two word vectors is calculated using cosine similarity. This measures how closely two words are related based on their context.\nWord Pairs: The list word_pairs contains sample word pairs for which collocations are analyzed. You can modify this list to include more word pairs.\n\nSample Output:\nCollocations and their similarity scores:\nking - queen: 0.651\nquick - fox: 0.341\nlazy - dog: 0.295\nbank - money: 0.519\nstrong - weak: -0.012\n\nThe word pair “king” and “queen” shows a high similarity, indicating they are often collocates in contexts related to royalty or power.\nThe pair “strong” and “weak” has a very low (and even negative) similarity, suggesting that these are antonyms rather than collocates.\n\n\n\n\n13.2.2 2. Semantic Similarity in Corpora\nSemantic similarity is used to measure how similar two words, phrases, or sentences are in meaning. This can be used to compare texts across corpora (e.g., learner vs. native speaker corpora).\n\n13.2.2.1 Steps:\n\nUse word embeddings to compute similarity scores between words or phrases in two different corpora.\nAggregate the similarities to compare the semantic variation between the corpora.\n\n\n\n13.2.2.2 Python Code for Semantic Similarity in Corpora:\nfrom transformers import BertTokenizer, BertModel\nimport torch\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n# Load pre-trained BERT model and tokenizer for contextual embeddings\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nmodel = BertModel.from_pretrained('bert-base-uncased')\n\n# Example sentences from different corpora (Learner vs Native)\nsentence_learner = \"The cat sat on the mat.\"\nsentence_native = \"A feline rested on a carpet.\"\n\n# Tokenize the sentences and create input tensors for BERT\ninputs_learner = tokenizer(sentence_learner, return_tensors='pt')\ninputs_native = tokenizer(sentence_native, return_tensors='pt')\n\n# Pass the sentences through BERT to get hidden states\nwith torch.no_grad():\n    outputs_learner = model(**inputs_learner)\n    outputs_native = model(**inputs_native)\n\n# Extract the last hidden states for sentence embeddings\nembedding_learner = outputs_learner.last_hidden_state.mean(dim=1)  # Mean pooling for sentence embedding\nembedding_native = outputs_native.last_hidden_state.mean(dim=1)\n\n# Compute cosine similarity between the sentence embeddings\nsimilarity = cosine_similarity(embedding_learner, embedding_native)[0][0]\n\nprint(f\"Semantic similarity between the sentences: {similarity:.3f}\")\nExplanation:\n\nBERT Model: We use a pre-trained BERT model (bert-base-uncased) to compute contextual word embeddings. BERT captures the meaning of a word or sentence in the context of surrounding words.\nSentence Embedding: BERT outputs embeddings for each token in the sentence. We use mean pooling to combine these token embeddings into a single vector representing the entire sentence.\nCosine Similarity: Cosine similarity is used to compute how similar the two sentences are in meaning.\n\nSample Output:\nSemantic similarity between the sentences: 0.781\n\nThe two sentences “The cat sat on the mat.” (learner corpus) and “A feline rested on a carpet.” (native corpus) have a high similarity score (0.781), showing that despite lexical differences, their meanings are quite similar.\n\n\n\n\n13.2.3 Use Cases for Both Tasks in Corpus Linguistics:\n\nCollocation Analysis:\n\nLexicography: Identify common collocations for dictionary creation or teaching materials.\nLanguage Teaching: Help learners understand frequent word pairings and idiomatic expressions.\n\nSemantic Similarity in Corpora:\n\nLearner Corpora: Compare learner-generated texts with native speaker texts to assess the semantic proximity and linguistic variation.\nTextual Analysis: Measure how similar different versions of texts are, or compare writing from different authors or genres.\n\n\nBy applying these techniques, researchers can study patterns in natural language usage, how meanings vary across corpora, and how words co-occur in different contexts.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Word Embedding Activities</span>"
    ]
  },
  {
    "objectID": "13.NLP_toolkit.html#discourse-and-pragmatics-using-word-embeddings-python-code-and-explanations",
    "href": "13.NLP_toolkit.html#discourse-and-pragmatics-using-word-embeddings-python-code-and-explanations",
    "title": "13  Word Embedding Activities",
    "section": "13.3 Discourse and Pragmatics Using Word Embeddings: Python Code and Explanations",
    "text": "13.3 Discourse and Pragmatics Using Word Embeddings: Python Code and Explanations\nIn discourse and pragmatics, topic modeling and contextual meaning and coherence are key tasks. Below are Python implementations for each task, focusing on using word embeddings to uncover topics and assess coherence in text.\n\n13.3.1 Topic Modeling Using Embedding-Based Approaches\nTopic modeling is a technique for identifying hidden themes or topics within a collection of documents. Embedding-based models, like BERTopic, can generate clusters of semantically related words that represent underlying topics.\n\n13.3.1.1 Steps:\n\nPreprocess a collection of documents.\nUse a pre-trained embedding model to transform text into vectors.\nApply topic modeling using an embedding-based approach like BERTopic.\n\n\n\n13.3.1.2 Python Code for Topic Modeling Using BERTopic:\nYou need to install BERTopic and sentence-transformers first:\npip install bertopic sentence-transformers\nNow the Python code:\nfrom bertopic import BERTopic\nfrom sklearn.datasets import fetch_20newsgroups\n\n# Fetch sample data (20 newsgroups dataset for topic modeling)\ndata = fetch_20newsgroups(subset='all')['data']\n\n# Initialize BERTopic model (uses embedding-based topic modeling)\ntopic_model = BERTopic()\n\n# Fit the topic model on the dataset\ntopics, probabilities = topic_model.fit_transform(data)\n\n# Display the top 5 topics\ntopic_info = topic_model.get_topic_info()\nprint(topic_info.head())\n\n# Get the top words for a specific topic\ntopic_id = 0  # You can change this to explore different topics\ntop_words = topic_model.get_topic(topic_id)\nprint(f\"Top words for topic {topic_id}: {top_words}\")\nExplanation:\n\nBERTopic: BERTopic is a topic modeling library that leverages pre-trained sentence embeddings to find topics in large corpora.\nEmbedding Transformation: It uses embeddings to capture the semantic meaning of each document and then clusters these embeddings to identify topics.\nOutput:\ntopic_info: This provides a list of all topics discovered by the model, along with the size of each topic (i.e., how many documents are classified under each topic).\nget_topic(): This function returns the top words for a particular topic, providing insights into the core vocabulary related to that topic.\n\nSample Output:\nTop 5 topics:\n   Topic  Count\n0     -1   7285\n1      0   1121\n2      1    874\n3      2    797\n4      3    726\n\nTop words for topic 0: [('space', 0.03), ('nasa', 0.02), ('launch', 0.015), ('mission', 0.014), ('orbit', 0.013)]\nIn this example, Topic 0 might be related to space exploration, as evidenced by the most prominent words: “space”, “nasa”, “launch”, “mission”, “orbit”.\n\n\n\n13.3.2 Contextual Meaning and Coherence Assessment\nTo assess coherence in a text, we can analyze the semantic similarity between consecutive sentences. Cohesive and coherent texts tend to have sentences that are contextually related, whereas disjointed texts may exhibit lower similarity scores between sentences.\n\n13.3.2.1 Steps:\n\nPreprocess the text into sentences.\nUse pre-trained sentence embeddings (e.g., BERT) to compute sentence vectors.\nCalculate the similarity between consecutive sentences to assess coherence.\n\n\n\n13.3.2.2 Python Code for Contextual Meaning and Coherence Using BERT:\nYou need to install transformers for sentence embeddings:\npip install transformers torch\nNow the Python code:\nfrom transformers import BertTokenizer, BertModel\nimport torch\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport nltk\n\n# Download the NLTK sentence tokenizer\nnltk.download('punkt')\n\n# Load pre-trained BERT model and tokenizer\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nmodel = BertModel.from_pretrained('bert-base-uncased')\n\n# Sample text for coherence analysis\ntext = \"\"\"\nThe cat sat on the mat. It was a sunny day. The dog barked at the cat. The mat was clean and soft.\nThe weather changed abruptly. There was a sudden storm, and everyone rushed inside.\n\"\"\"\n\n# Tokenize the text into sentences\nsentences = nltk.sent_tokenize(text)\n\n# Function to get the sentence embedding from BERT\ndef get_sentence_embedding(sentence):\n    inputs = tokenizer(sentence, return_tensors='pt')\n    with torch.no_grad():\n        outputs = model(**inputs)\n    # Mean pooling of the hidden states to get sentence embedding\n    sentence_embedding = outputs.last_hidden_state.mean(dim=1)\n    return sentence_embedding\n\n# Compute embeddings for all sentences\nsentence_embeddings = [get_sentence_embedding(sentence) for sentence in sentences]\n\n# Compute cosine similarity between consecutive sentences\ncoherence_scores = []\nfor i in range(len(sentence_embeddings) - 1):\n    similarity = cosine_similarity(sentence_embeddings[i], sentence_embeddings[i + 1])[0][0]\n    coherence_scores.append(similarity)\n\n# Display coherence scores\nfor i, score in enumerate(coherence_scores):\n    print(f\"Coherence between sentence {i+1} and {i+2}: {score:.3f}\")\nExplanation:\n\nSentence Tokenization: The text is split into individual sentences using nltk.sent_tokenize().\nBERT Sentence Embedding: Each sentence is passed through BERT to obtain a sentence embedding, which is a dense vector representation capturing the semantic meaning of the entire sentence.\nCoherence Measurement: The coherence between consecutive sentences is measured using cosine similarity between their embeddings. A high similarity score means that the two sentences are contextually coherent, while a low score indicates a break in coherence.\n\nSample Output:\nCoherence between sentence 1 and 2: 0.721\nCoherence between sentence 2 and 3: 0.695\nCoherence between sentence 3 and 4: 0.891\nCoherence between sentence 4 and 5: 0.462\nCoherence between sentence 5 and 6: 0.853\nIn this example: - High coherence is observed between sentence pairs 3 & 4, and 5 & 6, indicating they are contextually related. - Lower coherence between sentences 4 & 5 suggests a possible topic shift or break in coherence, which could be a signal of abrupt transitions in discourse.\n\n\n\n13.3.3 Use Cases for Both Tasks in Discourse Analysis:\n\nTopic Modeling:\n\nDiscourse Studies: Identify themes in large-scale conversations (e.g., analyzing debates, interviews, or discussions).\nGenre Analysis: Discover the key topics or themes within specific genres of text (e.g., scientific articles, novels, news articles).\n\nContextual Meaning and Coherence:\n\nAutomatic Essay Scoring: Coherence scores can be used to evaluate how well a student’s essay flows from one sentence or paragraph to the next.\nDiscourse Analysis: Researchers can measure the cohesion within a text to better understand how well ideas are connected or if there are any sudden shifts in the narrative.\n\n\nThese tools offer a powerful way to apply word embeddings to uncover the structure and meaning within discourse, making them useful for both academic and practical applications in language analysis.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Word Embedding Activities</span>"
    ]
  },
  {
    "objectID": "13.NLP_toolkit.html#sociolinguistics-using-word-embeddings",
    "href": "13.NLP_toolkit.html#sociolinguistics-using-word-embeddings",
    "title": "13  Word Embedding Activities",
    "section": "13.4 Sociolinguistics Using Word Embeddings",
    "text": "13.4 Sociolinguistics Using Word Embeddings\nIn sociolinguistics, word embeddings can be used to analyze variation and change over time and across dialects or sociolects. Below are Python implementations for both tasks, along with detailed explanations.\n\n13.4.1 Variation and Change in Word Usage\nResearchers can track changes in word meanings or usage across time periods or linguistic communities by training word embeddings on different subsets of data (e.g., text from different decades or regions). By comparing embeddings of the same word across different time periods or regions, researchers can observe how word meanings shift.\n\n13.4.1.1 Steps:\n\nDivide the text corpus into different time periods (or other sociolinguistic factors like regions or social groups).\nTrain or load pre-trained word embeddings for each time period.\nCompare the embeddings of a target word across the time periods to detect changes in meaning.\n\n\n\n13.4.1.2 Python Code for Tracking Variation and Change:\n\n13.4.1.2.1 Download Pre-Trained Word2Vec Models\nIf you do not have the word2vec-google-news-300.bin file, you can use pre-trained word embeddings from gensim. You can load models like the Google News Word2Vec embeddings, which are available through gensim.\nHere’s an example of how to load a pre-trained model from Gensim’s downloader:\nimport gensim.downloader as api\n\n# Load a pre-trained Word2Vec model (e.g., Google News embeddings)\nmodel_google = api.load(\"word2vec-google-news-300\")  # Use this for the 1990s model\nmodel_wiki = api.load(\"fasttext-wiki-news-subwords-300\")  # Use the same for modern-day comparison\nimport gensim\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport gensim.downloader as api\nprint(list(api.info()['models'].keys()))\n['fasttext-wiki-news-subwords-300', 'conceptnet-numberbatch-17-06-300', 'word2vec-ruscorpora-300', 'word2vec-google-news-300', 'glove-wiki-gigaword-50', 'glove-wiki-gigaword-100', 'glove-wiki-gigaword-200', 'glove-wiki-gigaword-300', 'glove-twitter-25', 'glove-twitter-50', 'glove-twitter-100', 'glove-twitter-200', '__testing_word2vec-matrix-synopsis']\n# Load a pre-trained Word2Vec model (e.g., Google News embeddings)\nmodel_google = api.load(\"word2vec-google-news-300\")\nmodel_wiki = api.load(\"fasttext-wiki-news-subwords-300\")  \n\n# Function to compare word embeddings across context\ndef compare_word_across_time(word, model1, model2):\n    try:\n        # Get embeddings for the word from both time periods\n        vector1 = model1[word]\n        vector2 = model2[word]\n        \n        # Calculate cosine similarity to see how the word meaning has changed\n        similarity = cosine_similarity([vector1], [vector2])[0][0]\n        return similarity\n    except KeyError:\n        return f\"'{word}' not found in one of the models.\"\n\n# Example usage: Track how the meaning of the word 'cloud' is different between Google and Twitter\nword = 'cloud'\nsimilarity_score = compare_word_across_time(word, model_google, model_wiki)\n\nprint(f\"Semantic similarity for '{word}' between Google and Wiki: {similarity_score:.3f}\")\nSample Output:\nSemantic similarity for 'cloud' between Google and Wiki: -0.035\nIn this case, the similarity score suggests a moderate shift in the meaning of the word “cloud.”\n\n\n13.4.1.2.2 Load a pre-trained model\nEnsure that the file 'word2vec-google-news-300.bin' exists and the path is correctly specified. If the file is stored in a different directory, make sure to provide the absolute path to the file.\nFor example:\nmodel_google = gensim.models.KeyedVectors.load_word2vec_format('/path/to/your/word2vec-google-news-300.bin', binary=True)\n\n\n\n13.4.1.3 Train Your Own Embedding Model (Optional)\nIf you want to specifically train word embeddings on corpora from different time periods, you can use Gensim’s Word2Vec to train models on your own text data from Google and Twitter.\nHere’s a basic example of how to train a Word2Vec model:\nfrom gensim.models import Word2Vec\n\n# Assuming `Google` and `Twitter` are lists of tokenized sentences from your corpus\n# Example: sentences_Google = [[\"word1\", \"word2\", \"word3\"], [\"word4\", \"word5\"]]\n\n# Train Word2Vec models on your corpus\nmodel_google = Word2Vec(sentences_google, vector_size=300, window=5, min_count=1, workers=4)\nmodel_wiki = Word2Vec(sentences_wiki, vector_size=300, window=5, min_count=1, workers=4)\n\n# Save the models\nmodel_google.wv.save_word2vec_format('model_google.bin', binary=True)\nmodel_wiki.wv.save_word2vec_format('model_wiki.bin', binary=True)\nYou will need large corpora for Google and Wiki to train accurate embeddings. Once trained, you can load and use these models as in your original code.\n\n\n\n13.4.2 Dialectology Using Word Embeddings\nIn dialectology, researchers can compare the embeddings of words across dialects or sociolects to quantify linguistic similarities and differences. By training word embeddings on corpora representing different dialects, the embeddings for the same word can be compared to reveal how meanings or usages differ.\n\n13.4.2.1 Steps:\n\nTrain or load pre-trained word embeddings for different dialects or sociolects.\nCompare the embeddings of the same word across dialects to measure their semantic similarity.\n\n\n\n13.4.2.2 Python Code for Comparing Dialects Using Word Embeddings:\nimport gensim\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n# Load or train embeddings for different dialects (example from two dialect corpora)\nmodel_us_english = gensim.models.KeyedVectors.load_word2vec_format('word2vec_us_english.bin', binary=True)\nmodel_uk_english = gensim.models.KeyedVectors.load_word2vec_format('word2vec_uk_english.bin', binary=True)\n\n# Function to compare word embeddings across dialects\ndef compare_word_across_dialects(word, model_dialect1, model_dialect2):\n    try:\n        # Get embeddings for the word from both dialects\n        vector_dialect1 = model_dialect1[word]\n        vector_dialect2 = model_dialect2[word]\n        \n        # Calculate cosine similarity to compare the meanings across dialects\n        similarity = cosine_similarity([vector_dialect1], [vector_dialect2])[0][0]\n        return similarity\n    except KeyError:\n        return f\"'{word}' not found in one of the models.\"\n\n# Example usage: Compare the meaning of 'boot' in US English and UK English\nword = 'boot'\nsimilarity_score = compare_word_across_dialects(word, model_us_english, model_uk_english)\n\nprint(f\"Semantic similarity for '{word}' between US and UK English: {similarity_score:.3f}\")\nExplanation:\n\nDifferent Dialect Embedding Models: Two Word2Vec models (model_us_english and model_uk_english) are trained on corpora from two different English dialects: American and British.\nCosine Similarity: The similarity between the word embeddings in different dialects indicates how similarly the word is used or understood. A high similarity score indicates that the word is used similarly, while a low score suggests a difference in usage or meaning.\nUsage Example: The word “boot” has different meanings in US English (referring to footwear) and UK English (referring to the trunk of a car).\n\nSample Output:\nSemantic similarity for 'boot' between US and UK English: 0.421\nIn this example, the word “boot” has a lower similarity score, reflecting its different meanings in the two dialects.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Word Embedding Activities</span>"
    ]
  },
  {
    "objectID": "13.NLP_toolkit.html#sentiment-and-politeness-analysis-using-word-embeddings",
    "href": "13.NLP_toolkit.html#sentiment-and-politeness-analysis-using-word-embeddings",
    "title": "13  Word Embedding Activities",
    "section": "13.5 Sentiment and Politeness Analysis Using Word Embeddings:",
    "text": "13.5 Sentiment and Politeness Analysis Using Word Embeddings:\nWord embeddings can be used to analyze both politeness or formality levels and sentiment analysis. These tasks are important for understanding the pragmatic and affective aspects of language in different contexts, such as social interactions or cross-cultural communication.\n\n13.5.1 Politeness or Formality Levels Using Word Embeddings\nIn this task, we aim to assess the politeness or formality of text by measuring how closely words or phrases align with known politeness/formality markers in the embedding space. We can create a word embedding-based model to compare the text with words commonly associated with politeness or formality.\n\n13.5.1.1 Steps:\n\nCreate or load word embeddings (Word2Vec, BERT).\nDefine a set of words or phrases commonly associated with politeness or formality (e.g., “please”, “thank you”, “sir”).\nCompute the similarity between the words in the input text and the politeness markers.\n\n\n\n13.5.1.2 Python Code for Politeness/Formality Level Detection:\nimport gensim.downloader as api\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n# Load pre-trained Word2Vec model\nmodel = api.load(\"word2vec-google-news-300\")\n\n# Define a list of politeness or formality markers\npoliteness_markers = [\"please\", \"thank you\", \"sir\", \"madam\", \"kindly\", \"would you\", \"may I\"]\n\n# Function to assess the politeness of a given sentence\ndef assess_politeness(sentence, markers, model):\n    # Tokenize the sentence (simplified)\n    words = sentence.lower().split()\n    \n    # Check similarity to politeness markers\n    politeness_score = 0\n    for word in words:\n        try:\n            word_vector = model[word]\n            # Calculate similarity with each politeness marker\n            for marker in markers:\n                marker_vector = model[marker]\n                similarity = cosine_similarity([word_vector], [marker_vector])[0][0]\n                politeness_score += similarity\n        except KeyError:\n            # Word not in the model vocabulary\n            pass\n    \n    # Normalize politeness score by the number of words\n    return politeness_score / len(words) if len(words) &gt; 0 else 0\n\n# Example usage\nsentence = \"Would you kindly help me with this task, sir?\"\npoliteness_score = assess_politeness(sentence, politeness_markers, model)\n\nprint(f\"Politeness score for the sentence: {politeness_score:.3f}\")\nExplanation:\n\nPoliteness Markers: We define a list of words or phrases typically associated with politeness or formality.\nCosine Similarity: For each word in the input sentence, we compute its similarity to each politeness marker using cosine similarity. The higher the similarity score, the more polite or formal the sentence is likely to be.\nPoliteness Score: We aggregate the similarities across all words in the sentence to compute a “politeness score.”\n\nSample Output:\nPoliteness score for the sentence: 0.219\n\n\n\n13.5.2 Sentiment Analysis Using Word Embeddings\nSentiment analysis involves classifying the emotional tone of a text (e.g., positive, negative, neutral). By leveraging word embeddings, we can calculate the semantic similarity between words in the input text and words that are commonly associated with positive or negative sentiments.\n\n13.5.2.1 Steps:\n\nLoad pre-trained word embeddings.\nDefine a set of words associated with positive and negative sentiment.\nCompute the similarity between the words in the input text and the sentiment markers.\n\n\n\n13.5.2.2 Python Code for Sentiment Analysis:\n# Define a list of positive and negative sentiment markers\npositive_markers = [\"good\", \"happy\", \"joy\", \"love\", \"excellent\", \"amazing\"]\nnegative_markers = [\"bad\", \"sad\", \"angry\", \"hate\", \"terrible\", \"horrible\"]\n\n# Function to assess the sentiment of a given sentence\ndef assess_sentiment(sentence, pos_markers, neg_markers, model):\n    # Tokenize the sentence (simplified)\n    words = sentence.lower().split()\n\n    # Initialize sentiment scores\n    positive_score = 0\n    negative_score = 0\n\n    # Check similarity to sentiment markers\n    for word in words:\n        try:\n            word_vector = model[word]\n            # Compare with positive markers\n            for pos_word in pos_markers:\n                pos_vector = model[pos_word]\n                similarity = cosine_similarity([word_vector], [pos_vector])[0][0]\n                positive_score += similarity\n            # Compare with negative markers\n            for neg_word in neg_markers:\n                neg_vector = model[neg_word]\n                similarity = cosine_similarity([word_vector], [neg_vector])[0][0]\n                negative_score += similarity\n        except KeyError:\n            # Word not in the model vocabulary\n            pass\n    \n    # Determine overall sentiment based on which score is higher\n    sentiment = \"Positive\" if positive_score &gt; negative_score else \"Negative\"\n    return sentiment, positive_score, negative_score\n\n# Example usage\nsentence = \"I love this amazing product!\"\nsentiment, pos_score, neg_score = assess_sentiment(sentence, positive_markers, negative_markers, model)\n\nprint(f\"Sentiment: {sentiment}\")\nprint(f\"Positive score: {pos_score:.3f}, Negative score: {neg_score:.3f}\")\nSample Output:\nSentiment: Positive\nPositive score: 7.896, Negative score: 6.310\nIn this example, the sentence “I love this amazing product!” is classified as positive based on its higher similarity to positive sentiment markers like “love” and “amazing.”\nBy leveraging word embeddings, we can analyze both the politeness and sentiment of text in a nuanced way, providing insights into how language is used to convey emotions, politeness, and formality across different contexts.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Word Embedding Activities</span>"
    ]
  },
  {
    "objectID": "13.NLP_toolkit.html#translation-and-bilingual-word-embeddings-1",
    "href": "13.NLP_toolkit.html#translation-and-bilingual-word-embeddings-1",
    "title": "13  Word Embedding Activities",
    "section": "13.6 Translation and Bilingual Word Embeddings",
    "text": "13.6 Translation and Bilingual Word Embeddings\nBilingual word embeddings enable us to map words from different languages into a shared semantic space, facilitating cross-linguistic analysis and error detection in translation. These embeddings allow words in different languages that have similar meanings to be located close to each other in the shared space, which is useful for tasks like machine translation and semantic analysis across languages.\nIn cross-linguistic analysis, we use bilingual embeddings to map words from two different languages into a shared semantic space. This allows us to study how similar words in different languages are semantically related.\n\n13.6.0.1 Steps:\n\nLoad bilingual word embeddings for two languages.\nCompare the embeddings of words from different languages in the shared space.\nCalculate similarity between words to identify cross-linguistic semantic similarity.\n\nHere’s the revised Python code for both cross-linguistic analysis and translation error detection using English and Korean word embeddings:\n\n\n13.6.1 Cross-Linguistic Analysis Between English and Korean Using Bilingual Word Embeddings\n# First, install fastText if not already installed\npip install fasttext\nimport fasttext\nimport fasttext.util\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n# Load pre-trained fastText models for English and Korean\nfasttext.util.download_model('en', if_exists='ignore')  # English\nfasttext.util.download_model('ko', if_exists='ignore')  # Korean\nmodel_en = fasttext.load_model('cc.en.300.bin')\nmodel_ko = fasttext.load_model('cc.ko.300.bin')\n\n# Function to compare cross-linguistic similarity between English and Korean words\ndef cross_linguistic_similarity(word_en, word_ko, model_en, model_ko):\n    # Get word embeddings\n    vec_en = model_en.get_word_vector(word_en)\n    vec_ko = model_ko.get_word_vector(word_ko)\n    \n    # Compute cosine similarity\n    similarity = cosine_similarity([vec_en], [vec_ko])[0][0]\n    return similarity\n# Example usage: Compare 'apple' (English) and '사과' (Korean)\nword_en = 'apple'\nword_ko = '사과'  # 사과 (sagwa) means \"apple\" in Korean\nsimilarity_score = cross_linguistic_similarity(word_en, word_ko, model_en, model_ko)\n\nprint(f\"Similarity between '{word_en}' (English) and '{word_ko}' (Korean): {similarity_score:.3f}\")\nExplanation:\n\nfastText Models: We load pre-trained fastText models for English and Korean. These models map words from both languages into a shared 300-dimensional space.\nCross-Linguistic Similarity: The function calculates cosine similarity between the embeddings of an English word and its Korean translation.\n\nSample Output:\nSimilarity between 'apple' (English) and '사과' (Korean): 0.811\nIn this example, ‘apple’ (English) and ‘사과’ (Korean) have a high similarity score, indicating they are semantically related across languages.\n\n\n13.6.2 Translation Error Detection Between English and Korean\nWe can use bilingual word embeddings to detect translation errors by comparing the embeddings of English words with their Korean translations. If the similarity is below a threshold, the translation may be incorrect.\n\n13.6.2.1 Revised Python Code for Translation Error Detection Between English and Korean:\n# Function to detect errors in translation using bilingual word embeddings\ndef detect_translation_error(source_word, target_word, model_source, model_target, threshold=0.6):\n    # Get embeddings for source and target words\n    vec_source = model_source.get_word_vector(source_word)\n    vec_target = model_target.get_word_vector(target_word)\n    \n    # Compute cosine similarity\n    similarity = cosine_similarity([vec_source], [vec_target])[0][0]\n    \n    # Determine if there is a potential translation error\n    if similarity &lt; threshold:\n        return f\"Potential translation error: '{source_word}' and '{target_word}' have low similarity ({similarity:.3f}).\"\n    else:\n        return f\"Translation seems correct: '{source_word}' and '{target_word}' are semantically similar ({similarity:.3f}).\"\n\n# Example usage: Detecting a possible translation error between 'car' (English) and '자동차' (Korean)\nsource_word = 'car'\ntarget_word = '자동차'  # 자동차 (jadongcha) means \"car\" in Korean\nerror_message = detect_translation_error(source_word, target_word, model_en, model_ko)\n\nprint(error_message)\n\n# Example usage: Detecting a potential translation error between 'car' (English) and '책' (Korean)\nsource_word = 'car'\ntarget_word = '책'  # 책 (chaek) means \"book\" in Korean (incorrect translation)\nerror_message = detect_translation_error(source_word, target_word, model_en, model_ko)\n\nprint(error_message)\nExplanation:\n\nTranslation Error Detection: This function compares the similarity between an English word and its Korean translation. If the similarity is below a given threshold (e.g., 0.6), the translation is flagged as potentially incorrect.\nThreshold: The threshold helps to determine the cutoff for acceptable similarity. A higher threshold will be more strict in detecting errors.\n\nSample Output:\nTranslation seems correct: 'car' and '자동차' are semantically similar (0.874).\nPotential translation error: 'car' and '책' have low similarity (0.218).\nIn the first example, ‘car’ (English) and ‘자동차’ (Korean) are correctly translated, whereas ‘car’ and ‘책’ (book) are not semantically similar, indicating a translation error.\nBy using bilingual word embeddings, we can perform effective cross-linguistic analysis and translation error detection between English and Korean, improving translation systems and understanding the semantic relationships between languages.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Word Embedding Activities</span>"
    ]
  },
  {
    "objectID": "A.assignments.html",
    "href": "A.assignments.html",
    "title": "14  Assignment #01",
    "section": "",
    "text": "14.1 Assignment 01: Web Scraping and YouTube Subtitle Extraction",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Assignment #01</span>"
    ]
  },
  {
    "objectID": "A.assignments.html#assignment-01-web-scraping-and-youtube-subtitle-extraction",
    "href": "A.assignments.html#assignment-01-web-scraping-and-youtube-subtitle-extraction",
    "title": "14  Assignment #01",
    "section": "",
    "text": "14.1.1 Total Marks: 20\n\n\n14.1.2 Objective:\nIn this assignment, you will use Python to perform web scraping and extract subtitles from YouTube playlists. You will scrape text from a website over multiple pages, process the text, and save it in an Excel file. Additionally, you will extract and process subtitles from videos in a YouTube playlist, saving the results in Excel format.\n\n\n14.1.3 Assignment Instructions:",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Assignment #01</span>"
    ]
  },
  {
    "objectID": "A.assignments.html#task-1-web-scraping-10-marks",
    "href": "A.assignments.html#task-1-web-scraping-10-marks",
    "title": "14  Assignment #01",
    "section": "14.2 Task 1: Web Scraping (10 Marks)",
    "text": "14.2 Task 1: Web Scraping (10 Marks)\n\n14.2.1 Step 1: Select a Website for Crawling\nSelect a website from which you will scrape data. The website must have multiple pages that can be crawled.\nExplain why you selected this website (e.g., relevance, data availability, structure).\n\n\n14.2.2 Step 2: Crawl Data from More than 2 Pages (4 Marks)\nWrite a Python script using requests, BeautifulSoup, or other relevant libraries to scrape data from at least two pages of the selected website. Ensure that your script can navigate between pages (pagination).\n\n\n14.2.3 Step 3: Preprocess the Scraped Text and Save to Excel (xlsx) (4 Marks)\n\nPreprocess the scraped text data (e.g., remove unnecessary characters, clean formatting).\nSave the processed text in an Excel file (.xlsx format) using pandas or openpyxl.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Assignment #01</span>"
    ]
  },
  {
    "objectID": "A.assignments.html#task-2-youtube-playlist-subtitle-extraction-10-marks",
    "href": "A.assignments.html#task-2-youtube-playlist-subtitle-extraction-10-marks",
    "title": "14  Assignment #01",
    "section": "14.3 Task 2: YouTube Playlist Subtitle Extraction (10 Marks)",
    "text": "14.3 Task 2: YouTube Playlist Subtitle Extraction (10 Marks)\n\n14.3.1 Step 1: Select a YouTube Playlist (2 Marks)\n\nSelect a YouTube playlist that contains videos with subtitles.\nExplain why you chose this playlist (e.g., topic relevance, video variety).\n\n\n\n14.3.2 Step 2: Extract Subtitles from More than 2 Videos (4 Marks)\n\nWrite a Python script using pytube, youtube-transcript-api, or other relevant libraries to extract subtitles from at least two videos in the playlist.\nPrint or log the extracted subtitles.\n\n\n\n14.3.3 Step 3: Preprocess Subtitles and Save to Excel (xlsx) (4 Marks)\n\nPreprocess the extracted subtitles (e.g., remove unnecessary characters, clean formatting, timestamp adjustments).\nSave the cleaned subtitles in an Excel file (.xlsx format) using pandas or openpyxl.\n\n\n\n\n14.3.4 Submission Guidelines:\n\nTask 1:\n\nSubmit the Python script used for web scraping and text processing. The .ipynb file should include an explanation of why you chose the website using markdown.\nSubmit the Excel file containing the processed text data.\n\nTask 2:\n\nSubmit the Python script used for YouTube subtitle extraction and processing. The .ipynb file should include an explanation of why you chose the playlist using markdown.\nSubmit the Excel file containing the processed subtitles.\n\n\n\n\n14.3.5 Grading Criteria:\n\n\n\nTask\nMarks\n\n\n\n\nTask 1: Website Selection & Explanation\n2\n\n\nTask 1: Crawl Data from Multiple Pages\n4\n\n\nTask 1: Preprocess and Save to Excel\n4\n\n\nTask 2: YouTube Playlist Selection & Explanation\n2\n\n\nTask 2: Extract Subtitles from Multiple Videos\n4\n\n\nTask 2: Preprocess Subtitles and Save to Excel\n4\n\n\nTotal\n20\n\n\n\n\n\n14.3.6 Submission Deadline:\n\nPlease submit your assignment by Oct. 10, 2024 via the course portal.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Assignment #01</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Books\n\n파이썬 텍스트 마이닝 완벽 가이드 (개정판) https://wikibook.co.kr/textmining-rev/\n딥 러닝을 이용한 자연어 처리 입문 https://wikidocs.net/book/2155\n\n\n\nWebsites\n\nhttps://regexr.com/",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Basic AI Programming",
    "section": "",
    "text": "Preface\nThis material has been compiled using ChatGPT-4o for the AI Programming class of IGSE in Fall 2024. It is primarily targeted at linguistics students.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "14.Semantic_Network.html",
    "href": "14.Semantic_Network.html",
    "title": "14  Semantic Network Analysis",
    "section": "",
    "text": "14.1 Overview",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Semantic Network Analysis</span>"
    ]
  },
  {
    "objectID": "14.Semantic_Network.html#overview",
    "href": "14.Semantic_Network.html#overview",
    "title": "14  Semantic Network Analysis",
    "section": "",
    "text": "14.1.1 Key Concepts of Semantic Network Analysis:\n\nNodes and Edges:\n\nThe nodes in the network represent the concepts or terms you are analyzing.\nThe edges represent the connections between these terms. These connections can be based on co-occurrences, shared meanings, or even statistical relationships like cosine similarity in word vectors.\n\nCentrality:\n\nCentrality measures like degree centrality or betweenness centrality indicate which terms or concepts are most connected or important in the network. These terms can serve as key concepts or hubs that link many other terms together.\n\nCommunities or Clusters:\n\nSNA can help identify clusters or communities within the network, revealing groups of related terms or concepts that form cohesive sub-networks. These clusters can indicate topics or themes within a set of documents.\n\nApplications:\n\nLinguistics: It can be used to understand the structure of language and how words are related.\nSocial Media Analysis: Identifies key themes, discussions, and influencers.\nKnowledge Management: Visualizes how knowledge is structured in documents or research articles.\n\n\n\n\n14.1.2 Steps in Semantic Network Analysis:\n\nData Collection: You collect a dataset where relationships between concepts are apparent. This could be a corpus of text or a set of documents.\nText Preprocessing: This involves cleaning the text by removing stop words, punctuation, and irrelevant data. Stemming or lemmatization might also be performed to reduce words to their base forms.\nConcept Extraction: You identify the key concepts or terms in the dataset, typically using techniques like term frequency-inverse document frequency (TF-IDF) or keyword extraction algorithms.\nBuilding the Network: A co-occurrence matrix is built, where rows and columns represent words or concepts, and the cells represent the frequency with which those terms co-occur in the same context.\nNetwork Visualization: You visualize the network using graph tools, where terms are represented as nodes, and their relationships (e.g., co-occurrences) are represented by edges.\nAnalysis: You can calculate network metrics (e.g., centrality, clustering) and examine sub-networks to derive insights into how terms are related and what the key concepts are.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Semantic Network Analysis</span>"
    ]
  },
  {
    "objectID": "14.Semantic_Network.html#components",
    "href": "14.Semantic_Network.html#components",
    "title": "14  Semantic Network Analysis",
    "section": "14.2 Components",
    "text": "14.2 Components\n\n14.2.1 What is an Adjacency Matrix?\nAn Adjacency Matrix is a matrix representation of a graph, where the rows and columns represent the nodes (vertices) of the graph, and the matrix entries indicate whether pairs of nodes are adjacent (connected by an edge). It is a common way to represent graph data in mathematics, computer science, and network theory.\nFor a graph ( G = (V, E) ) where ( V ) is the set of vertices and ( E ) is the set of edges:\n\nIf there is an edge between vertex ( i ) and vertex ( j ), the entry ( A[i][j] ) in the adjacency matrix is 1.\nIf there is no edge between ( i ) and ( j ), ( A[i][j] ) is 0.\n\n\n14.2.1.1 Types of Adjacency Matrices:\n\nFor undirected graphs: The matrix is symmetric, i.e., if there is an edge between vertex ( i ) and vertex ( j ), both ( A[i][j] ) and ( A[j][i] ) are 1.\nFor directed graphs: The matrix is not necessarily symmetric. ( A[i][j] = 1 ) only if there is an edge from vertex ( i ) to vertex ( j ).\nWeighted graphs: Instead of just 0 and 1, the entries in the matrix can represent the weight of the edges between vertices.\n\n\n\n\n14.2.2 Example of an Adjacency Matrix\nConsider the following simple undirected graph:\n  A -- B\n  |  /\n  C\nThe graph has three nodes: A, B, and C. There are edges between A and B, A and C, and B and C. The adjacency matrix for this undirected graph would be:\n    A  B  C\nA  [0, 1, 1]\nB  [1, 0, 1]\nC  [1, 1, 0]\n\n\n14.2.3 Python Code to Create and Display an Adjacency Matrix\nBelow is a Python example to create and display an adjacency matrix using the numpy and networkx libraries.\nimport numpy as np\nimport networkx as nx\nimport matplotlib.pyplot as plt\n\n# Create a graph\nG = nx.Graph()\n\n# Add nodes\nG.add_nodes_from(['A', 'B', 'C'])\n\n# Add edges (undirected graph)\nG.add_edges_from([('A', 'B'), ('A', 'C'), ('B', 'C')])\n\n# Get the adjacency matrix\nadj_matrix = nx.adjacency_matrix(G).todense()\n\n# Display the adjacency matrix\nprint(\"Adjacency Matrix:\")\nprint(adj_matrix)\n\n# Visualize the graph\nnx.draw(G, with_labels=True, node_color='skyblue', edge_color='gray', node_size=1500, font_size=16)\nplt.show()\nOutput:\nAdjacency Matrix:\n[[0 1 1]\n [1 0 1]\n [1 1 0]]\nAnd the corresponding graph visualization will show the relationships between A, B, and C.\n\n\n\n14.2.4 Modifying for Directed or Weighted Graphs\n\nDirected Graph: You can use nx.DiGraph() instead of nx.Graph() to create a directed graph. In a directed graph, the edges will only be represented in one direction.\nG = nx.DiGraph()\nG.add_edges_from([('A', 'B'), ('C', 'A')])  # Directed edges\n\n# Get the adjacency matrix\nadj_matrix = nx.adjacency_matrix(G).todense()\n\n# Display the adjacency matrix\nprint(\"Adjacency Matrix:\")\nprint(adj_matrix)\n\n# Visualize the graph\nnx.draw(G, with_labels=True, node_color='skyblue', edge_color='gray', node_size=1500, font_size=16)\nplt.show()\n\nOutput:\nAdjacency Matrix:\n[[0 1 0]\n [0 0 0]\n [1 0 0]]\n\n\nWeighted Graph: To handle weighted graphs, you can assign weights to the edges, and the adjacency matrix will reflect these weights.\nimport networkx as nx\nimport matplotlib.pyplot as plt\n\n# Create a MultiGraph\nG = nx.MultiGraph()\n\n# Add nodes\nG.add_nodes_from(['A', 'B', 'C'])\n\n# Add multiple edges between nodes\nG.add_edge('A', 'B', weight=2)\nG.add_edge('A', 'B', weight=3)  # Multiple edge between A and B\nG.add_edge('A', 'C', weight=4)\nG.add_edge('B', 'C', weight=1)\n\n# Print the edges along with their attributes\nprint(\"Edges in MultiGraph with attributes:\")\nprint(G.edges(data=True))\n\n# Visualize the MultiGraph\npos = nx.spring_layout(G)\nnx.draw(G, pos, with_labels=True, node_color='lightblue', node_size=1500, font_size=16)\n\n# Create custom edge labels\nedge_labels = {}\nfor (u, v, data) in G.edges(data=True):\n    if (u, v) not in edge_labels:\n        edge_labels[(u, v)] = str(data['weight'])\n    else:\n        edge_labels[(u, v)] += f\",{data['weight']}\"\n\n# Draw edge labels\nnx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_size=12)\n\nplt.title(\"MultiGraph with Edge Weights\", fontsize=16)\nplt.axis('off')\nplt.tight_layout()\nplt.show()\nIn a weighted adjacency matrix, instead of 0 and 1, the matrix values will represent the weights of the edges.\n\nOutput:\nEdges in MultiGraph with attributes:\n[('A', 'B', {'weight': 2}), ('A', 'B', {'weight': 3}), ('A', 'C', {'weight': 4}), ('B', 'C', {'weight': 1})]\n\n\n\n14.2.5 Centrality Measures in Graphs\nCentrality measures are important in network analysis because they help identify the most important or influential nodes in a graph. Different centrality measures capture different notions of importance based on various graph properties. Below, we’ll explore some of the most common centrality measures, including Python code to calculate each one using the networkx library.\n\nDegree Centrality: How many direct connections a node has.\nBetweenness Centrality: How often a node lies on the shortest path between other nodes.\nCloseness Centrality: How close a node is to all other nodes in the network.\nEigenvector Centrality: The influence of a node in the network, considering the centrality of its neighbors.\nPageRank: A variant of eigenvector centrality that ranks nodes based on incoming links from other important nodes.\n\n\n14.2.5.1 Degree Centrality\nDegree Centrality is the simplest form of centrality. It counts the number of edges connected to a node. In an undirected graph, this is simply the number of neighbors.\n\nIn a directed graph, it can be divided into:\n\nIn-degree centrality: The number of incoming edges.\nOut-degree centrality: The number of outgoing edges.\n\n\\[\\text{Degree Centrality of node } i = \\frac{\\text{Number of edges connected to node } i}{\\text{Total number of possible edges}}\\]\nPython Code for Degree Centrality:\n### Python Code for Degree Centrality:\n\nimport networkx as nx\nimport matplotlib.pyplot as plt\n\n# Create a sample graph (based on your data or a custom graph)\nG = nx.Graph()\n\n# Add edges between nodes (example graph)\nedges = [\n    ('A', 'B'), \n    ('B', 'C'), \n    ('C', 'D'), ('C', 'E'),\n    ('D', 'E')\n]\nG.add_edges_from(edges)\n# Calculate degree centrality\ndegree_centrality = nx.degree_centrality(G)\nprint(\"Degree Centrality:\", degree_centrality)\n\n# Visualize the graph with node size proportional to degree centrality\nnode_size = [v * 3000 for v in degree_centrality.values()]\npos = nx.spring_layout(G)\nnx.draw(G, pos, with_labels=True, node_size=node_size, node_color=\"lightblue\")\nplt.show()\nOutput:\nDegree Centrality: {'A': 0.25, 'B': 0.5, 'C': 0.75, 'D': 0.5, 'E': 0.5}\n\n\n\n14.2.5.2 Betweenness Centrality\nBetweenness Centrality measures the number of times a node lies on the shortest path between other nodes. Nodes with high betweenness centrality can control the flow of information in a network.\n\\[\\text{Betweenness Centrality of node } i = \\sum{\\substack{s \\neq i \\neq t}} \\frac{\\sigma_{st}(i)}{\\sigma_{st}}\\]\nWhere:\n\n\\(\\sigma_{st}\\) is the total number of shortest paths from node \\(s\\) to node \\(t\\).\n\\(\\sigma_{st}(i)\\)is the number of shortest paths that pass through node \\(i\\).\n\n\nPython Code for Betweenness Centrality:\n# Calculate betweenness centrality\nbetweenness_centrality = nx.betweenness_centrality(G)\nprint(\"Betweenness Centrality:\", betweenness_centrality)\n\n# Visualize the graph with node size proportional to betweenness centrality\nnode_size = [v * 3000 for v in betweenness_centrality.values()]\nnx.draw(G, pos, with_labels=True, node_size=node_size, node_color=\"lightgreen\")\nplt.show()\nOutput:\nBetweenness Centrality: {'A': 0.0, 'B': 0.5, 'C': 0.6666666666666666, 'D': 0.0, 'E': 0.0}\n\n\n14.2.5.3 \n\n\n14.2.5.4 Closeness Centrality\nCloseness Centrality measures how close a node is to all other nodes in the graph. Nodes with high closeness centrality can quickly interact with all other nodes in the network. Closeness Centrality is a centrality measure used to determine how efficiently information spreads from a given node to all other nodes in the network.\n\\[\\text{Closeness Centrality of node } i = \\frac{\\text{number of nodes - 1}}{\\sum{\\text{distance from node $i$ to all other nodes}}}\\]\n\nPython Code for Closeness Centrality:\nmport networkx as nx\n\n# Create a sample graph (based on your data or a custom graph)\nG = nx.Graph()\n\n# Add edges between nodes (example graph)\nedges = [\n    ('A', 'B'), \n    ('B', 'C'), \n    ('C', 'D'), ('C', 'E'),\n    ('D', 'E')\n]\nG.add_edges_from(edges)\n\n# Compute closeness centrality for each node\ncloseness_centrality = nx.closeness_centrality(G)\n\n# Display the closeness centrality values\nfor node, centrality in closeness_centrality.items():\n    print(f\"Closeness Centrality of node {node}: {centrality:.4f}\")\n    \n# Visualize the graph with node size proportional to closeness centrality\nnode_size = [v * 3000 for v in closeness_centrality.values()]\npos = nx.spring_layout(G)\nnx.draw(G, pos, with_labels=True, node_size=node_size, node_color=\"lightgreen\")\nplt.show()\nOutput:\nCloseness Centrality of node A: 0.4444\nCloseness Centrality of node B: 0.6667\nCloseness Centrality of node C: 0.8000\nCloseness Centrality of node D: 0.5714\nCloseness Centrality of node E: 0.5714\n\n\n\n14.2.5.5 Eigenvector Centrality\nEigenvector Centrality measures the influence of a node in a network. Unlike degree centrality, which simply counts the number of connections, eigenvector centrality assigns more value to nodes that are connected to other highly connected nodes.\n\nWhy Use Eigenvector Centrality?\nEigenvector centrality is useful when you need to consider not just how many connections a node has, but also how influential its neighbors are. It is commonly used in applications like:\n\nSocial Networks: Identifying influential individuals who are connected to other influential individuals.\nWeb Search Algorithms: PageRank, a variant of eigenvector centrality, is used to rank web pages based on their importance.\nBiological Networks: Identifying key genes or proteins based on their interactions with other important molecules.\n\nFor a node \\(v\\), the eigenvector centrality \\(x_v\\) is given by the following equation:\n\\[x_v = \\frac{1}{\\lambda} \\sum_{u* \\in N(v)}A_{vu}x_u\\]\nWhere:\n\n\\(x_v\\) is the eigenvector centrality of node \\(v\\).\n\\(\\lambda\\) is a constant (the largest eigenvalue of the adjacency matrix).\n\\(N(v)\\) is the set of neighbors of node ( v ).\n\\(A_{vu}\\) is the adjacency matrix where \\(A_{vu} = 1\\) if there is an edge between \\(v\\) and \\(u\\), and 0 otherwise.\n\nEigenvector centrality is a more nuanced measure of influence compared to simpler metrics like degree centrality, making it ideal for applications where connections to other influential nodes are particularly important.\nPython Code for Eigenvector Centrality:\nimport networkx as nx\nimport matplotlib.pyplot as plt\n\n# Create a sample graph\nG = nx.Graph()\n\n# Add edges between nodes (example graph)\nedges = [\n    ('A', 'C'), \n    ('B', 'C'), \n    ('C', 'D'), \n    ('D', 'E'), \n    ('D', 'F'), \n    ('D', 'G')\n]\nG.add_edges_from(edges)\n\n# Compute eigenvector centrality for each node\neigenvector_centrality = nx.eigenvector_centrality(G)\n\n# Display the eigenvector centrality values\nfor node, centrality in eigenvector_centrality.items():\n    print(f\"Eigenvector Centrality of node {node}: {centrality:.4f}\")\n\n# Visualize the graph with node size proportional to eigenvector centrality\nnode_size = [v * 3000 for v in eigenvector_centrality.values()]\npos = nx.spring_layout(G)\nnx.draw(G, pos, with_labels=True, node_size=node_size, node_color=\"lightblue\")\nplt.show()\nOutput:\nEigenvector Centrality of node A: 0.2299\nEigenvector Centrality of node C: 0.5000\nEigenvector Centrality of node B: 0.2299\nEigenvector Centrality of node D: 0.6280\nEigenvector Centrality of node E: 0.2887\nEigenvector Centrality of node F: 0.2887\nEigenvector Centrality of node G: 0.2887\n\n\n\n14.2.5.6 PageRank Centrality\nPageRank is a centrality measure originally developed by Larry Page and Sergey Brin to rank web pages in Google’s search engine. It is a variant of Eigenvector Centrality, but with the addition of a damping factor, typically set to 0.85, that models the probability that a user will stop following links at any given point. This allows PageRank to rank the importance of nodes (e.g., web pages) not just based on the number of inbound links, but also based on the importance of the linking pages.\nWhy Use PageRank?\nPageRank is a powerful measure of centrality that accounts for both the quantity and quality of connections to a node. It’s particularly useful in situations where:\n\nWeb Search: Ranking web pages based on the importance of the pages linking to them.\nCitation Networks: Determining influential papers based on how often they are cited by other important papers.\nSocial Networks: Identifying influential individuals based on connections from other influential people.\n\nDifferences from Other Centrality Measures:\n\nDegree Centrality only counts the number of links to a node, whereas PageRank considers both the number and quality (importance) of those links.\nEigenvector Centrality is similar to PageRank but lacks the damping factor, which prevents dead-end nodes from absorbing all the rank.\n\nPageRank Centrality Formula\nThe PageRank of a node \\(v\\) is given by:\n\\[PR(v) = \\frac{1-d}{N} + d \\sum_{u \\in M(v)} \\frac{PR(u)}{L(u)}\\]\nWhere:\n\n\\(PR(v)\\)is the PageRank of node \\(v\\).\n\\(d\\) is the damping factor (usually set to 0.85), representing the probability that a random walker will continue following links.\n\\(N\\) is the total number of nodes in the graph.\n\\(M(v)\\) is the set of nodes that link to node \\(v\\) (inbound links).\n\\(PR(u)\\) is the PageRank of node \\(u\\), which links to \\(v\\).\n\\(L(u)\\) is the number of outbound links from node \\(u\\).\n\nThe PageRank algorithm is an iterative process, where the initial PageRank values are distributed equally across all nodes. It then updates these values based on the formula above until the values converge (i.e., until they stop changing significantly).\nDetailed Explanation of Components:\n\nDamping Factor \\(d\\): This factor accounts for the possibility that a random surfer on the network (or web) will jump to a random page rather than following links. Typically, \\(d\\) is set to 0.85, meaning there’s an 85% chance the surfer will follow links and a 15% chance they will jump to a random page.\nTerm \\[\\frac{1-d}{N}\\]: This is the probability that the random surfer jumps to any node randomly. It distributes a small amount of rank equally to all nodes to prevent dead ends (i.e., nodes with no outbound links) from absorbing all the rank.\nSummation Term \\[\\sum_{u \\in M(v)} \\frac{PR(u)}{L(u)}\\]: This term gives the contribution of the rank from each node \\(u\\) that links to \\(v\\). The rank from node \\(u\\) is divided by the number of outbound links from \\(u\\), so the more links \\(u\\) has, the less rank it passes to each linked node.\n\nPython Code for PageRank\nUsing the networkx library, we can easily compute PageRank for a graph. Below is a Python implementation:\nimport networkx as nx\nimport matplotlib.pyplot as plt\n\n# Create a directed graph (since PageRank works on directed graphs)\nG = nx.DiGraph()\n\n# Add edges (this is a simple example, you can add any structure you want)\nedges = [\n    ('A', 'B'), \n    ('B', 'C'), \n    ('C', 'A'), \n    ('A', 'D'), \n    ('D', 'C'), \n    ('E', 'D'), \n    ('F', 'D'), \n    ('E', 'F'), \n    ('F', 'E')\n]\nG.add_edges_from(edges)\n\n# Compute PageRank\npagerank = nx.pagerank(G, alpha=0.85)  # Damping factor alpha=0.85\n\n# Display the PageRank values\nfor node, rank in pagerank.items():\n    print(f\"PageRank of node {node}: {rank:.4f}\")\n\n# Visualize the graph with node size proportional to PageRank\nnode_size = [v * 3000 for v in pagerank.values()]\npos = nx.spring_layout(G)\nnx.draw(G, pos, with_labels=True, node_size=node_size, node_color=\"lightblue\", arrows=True)\nplt.show()\nOutput:\nPageRank of node A: 0.2827\nPageRank of node B: 0.1451\nPageRank of node C: 0.3031\nPageRank of node D: 0.1821\nPageRank of node E: 0.0435\nPageRank of node F: 0.0435\n\nInterpretation of Output:\nThe PageRank algorithm has been applied to a directed graph where nodes and edges represent relationships (e.g., links between websites, connections in a network, etc.). Below is a detailed interpretation of both the graph and the PageRank values that were calculated in the provided output.\nPageRank Scores:\n\nNode A (0.2827):\n\nNode A has the second-highest PageRank score. It is an important node in the cycle (A → B → C → A) and also has an outbound connection to D. The cycle A-B-C boosts its importance since it is connected to other central nodes.\n\nNode B (0.1451):\n\nNode B has a relatively lower PageRank compared to A and C. While it is part of the cycle, it doesn’t have any additional outbound connections like node A, which might reduce its importance compared to its neighbors.\n\nNode C (0.3031):\n\nNode C has the highest PageRank score, indicating it is the most central and influential node in the graph. This is likely because it not only participates in the cycle (A → B → C → A) but also receives an additional edge from node D, giving it more inbound influence.\n\nNode D (0.1821):\n\nNode D has a moderately high PageRank score, primarily because it is linked to the important cycle (via A and C). Although it is not part of the cycle, its proximity to influential nodes boosts its score.\n\nNode E (0.0435) and Node F (0.0435):\n\nBoth E and F have the lowest PageRank scores. These nodes are peripheral in the network, with their primary connections being reciprocal links between themselves and links to node D. Since D is the only pathway for their influence, their importance is limited compared to other nodes directly involved in the central cycle.\n\n\nVisual Representation:\nIn the provided graph (as visualized), node sizes are proportional to their PageRank scores:\n\nLarger nodes (A, C, and D) are more important, with C being the largest node, reflecting its highest PageRank score.\nSmaller nodes (E and F) are less influential and are positioned on the periphery, reflecting their lower PageRank.\n\nSummary:\n\nCentrality of Cycle Nodes: Nodes A, B, and C form a strongly connected component (cycle), making them highly influential. C is the most central because it benefits from both the cycle and its connection to D.\nPeripheral Influence: Nodes E and F are peripheral and have limited influence because they rely on node D to connect to the rest of the network.\nNode D’s Role: Node D plays an intermediary role between the central cycle and the peripheral nodes E and F, making it somewhat important, though not as central as nodes in the cycle itself.\n\nThe graph and PageRank scores together show how the structure of a network determines the relative importance of each node, with highly interconnected cycles typically having the most influence.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Semantic Network Analysis</span>"
    ]
  }
]