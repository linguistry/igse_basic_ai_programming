[
  {
    "objectID": "01.setup.html",
    "href": "01.setup.html",
    "title": "1  Setting Up Environment",
    "section": "",
    "text": "1.1 Setting Up VSCode for Data Analysis\nVisual Studio Code (VSCode) is a free, open-source code editor developed by Microsoft. It supports multiple programming languages, extensions, and integrations, making it an excellent choice for data analysis.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Setting Up Environment</span>"
    ]
  },
  {
    "objectID": "01.setup.html#setting-up-vscode-for-data-analysis",
    "href": "01.setup.html#setting-up-vscode-for-data-analysis",
    "title": "1  Setting Up Environment",
    "section": "",
    "text": "1.1.1 Installing VSCode\n\nDownload VSCode: Visit the VSCode official website and download the installer for your operating system (Windows, macOS, or Linux).\nInstall VSCode: Run the installer and follow the installation instructions.\n\n\n\n1.1.2 Setting Up Python in VSCode\n\nInstall Python: Ensure Python is installed on your system. You can download it from the official Python website.\nInstall Python Extension for VSCode:\n\nOpen VSCode and go to the Extensions view by clicking on the Extensions icon in the sidebar or pressing Ctrl+Shift+X.\nSearch for “Python” and install the extension provided by Microsoft.\n\nVerify Python Installation:\n\nOpen a terminal in VSCode (`Ctrl+``) and type:\npython --version\nYou should see the installed Python version. If not, check your installation path.\n\nSet Up a Virtual Environment (Optional but recommended):\n\nCreate a virtual environment to manage dependencies for your project:\npython -m venv myenv\nActivate the virtual environment:\n\nWindows: myenv\\Scripts\\activate\nmacOS/Linux: source myenv/bin/activate\n\n\nInstall Essential Libraries:\n\nUse pip to install essential libraries like pandas, NumPy, and matplotlib:\npip install pandas numpy matplotlib seaborn\n\nCreate a Python File:\n\nOpen a new file, save it with a .py extension, and write your first Python code. For example:\nimport pandas as pd\nimport numpy as np\n\n# Create a simple DataFrame\ndata = {'Name': ['Alice', 'Bob', 'Charlie'], 'Age': [25, 30, 35]}\ndf = pd.DataFrame(data)\n\nprint(df)\n\nRunning Python Code:\n\nRun the code by clicking the “Run” button at the top right or by pressing F5.\n\n\n\n\n1.1.3 Additional Tips\n\nJupyter Notebook in VSCode: You can also use Jupyter notebooks within VSCode by installing the Jupyter extension.\nLinting and Formatting: Use extensions like Pylint or Black to maintain code quality.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Setting Up Environment</span>"
    ]
  },
  {
    "objectID": "01.setup.html#setting-up-google-colab-for-data-analysis",
    "href": "01.setup.html#setting-up-google-colab-for-data-analysis",
    "title": "1  Setting Up Environment",
    "section": "1.2 Setting Up Google Colab for Data Analysis",
    "text": "1.2 Setting Up Google Colab for Data Analysis\nGoogle Colab is a free, cloud-based platform that provides Jupyter Notebook environments. It allows you to write and execute Python code in your browser, making it a great tool for data analysis, especially when dealing with large datasets and GPU-based computations.\n\n1.2.1 Getting Started with Google Colab\n\nAccess Google Colab:\n\nGo to Google Colab using your Google account.\n\nCreating a New Notebook:\n\nClick on “New Notebook” to create a new notebook. This will open a new page with a code cell ready to run.\n\nInstalling and Importing Libraries:\n\nUse pip to install any additional libraries you need directly within a cell:\n# Install pandas and numpy\n!pip install pandas numpy\n\nWriting and Running Code:\n\nWrite your code in the code cells and run it by clicking the play button on the left side of the cell or pressing Shift+Enter.\nimport pandas as pd\nimport numpy as np\n\n# Creating a DataFrame\ndata = {'Product': ['Apples', 'Bananas', 'Cherries'], 'Price': [1.2, 0.8, 2.5]}\ndf = pd.DataFrame(data)\n\n# Displaying the DataFrame\nprint(df)\n\n\n\n\n1.2.2 Uploading Files and Connecting to Google Drive\n\nUploading Files: You can upload files directly to the Colab environment by using the file upload button on the left sidebar.\nConnecting to Google Drive: If you have data stored on Google Drive, you can easily access it by mounting your drive.\nfrom google.colab import drive\ndrive.mount('/content/drive')\nThis code will prompt you to authorize access to your Google Drive.\n\n\n\n1.2.3 Using GPU or TPU for Accelerated Computing\nGoogle Colab allows you to use GPUs or TPUs to speed up your computations, especially useful for machine learning tasks.\n\nChanging Runtime Type:\n\nGo to Runtime &gt; Change runtime type.\nSelect “GPU” or “TPU” under “Hardware accelerator” and click “Save”.\n\n\n\n\n1.2.4 Saving and Exporting Your Work\n\nSaving Notebooks: Colab automatically saves your work to Google Drive, but you can also save a copy manually by selecting File &gt; Save a copy in Drive.\nExporting Notebooks: You can download your notebook as a .ipynb or .py file by selecting File &gt; Download.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Setting Up Environment</span>"
    ]
  },
  {
    "objectID": "01.setup.html#summary",
    "href": "01.setup.html#summary",
    "title": "1  Setting Up Environment",
    "section": "1.3 Summary",
    "text": "1.3 Summary\n\nVSCode is great for local development with rich support for extensions, virtual environments, and debugging.\nGoogle Colab is ideal for cloud-based development, especially when you need quick access to computational resources like GPUs and TPUs.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Setting Up Environment</span>"
    ]
  },
  {
    "objectID": "01.setup.html#both-environments-are-powerful-tools-for-data-analysis-and-the-choice-depends-on-your-specific-needs-and-workflow-preferences.",
    "href": "01.setup.html#both-environments-are-powerful-tools-for-data-analysis-and-the-choice-depends-on-your-specific-needs-and-workflow-preferences.",
    "title": "1  Setting Up Environment",
    "section": "1.4 Both environments are powerful tools for data analysis, and the choice depends on your specific needs and workflow preferences.",
    "text": "1.4 Both environments are powerful tools for data analysis, and the choice depends on your specific needs and workflow preferences.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Setting Up Environment</span>"
    ]
  },
  {
    "objectID": "02.intro.html",
    "href": "02.intro.html",
    "title": "2  Constants and Variables in Python",
    "section": "",
    "text": "2.1 Variables in Python\nA variable is a named location in memory used to store data that can change during the execution of a program. Variables can hold different types of data, such as numbers, strings, or more complex data structures.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Constants and Variables in Python</span>"
    ]
  },
  {
    "objectID": "02.intro.html#variables-in-python",
    "href": "02.intro.html#variables-in-python",
    "title": "2  Constants and Variables in Python",
    "section": "",
    "text": "2.1.1 Declaring Variables\nTo declare a variable in Python, you simply assign a value to a name using the assignment operator =. Python is dynamically typed, meaning you do not need to declare the type of the variable explicitly.\n# Variable declaration\nx = 10           # An integer variable\nname = \"Alice\"   # A string variable\npi = 3.14159     # A float variable\n\nprint(x)         # Output: 10\nprint(name)      # Output: Alice\nprint(pi)        # Output: 3.14159\n# Updating variable values\nx = 20           # Reassigning a new value to x\nname = \"Bob\"     # Changing the value of name\n\nprint(x)         # Output: 20\nprint(name)      # Output: Bob",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Constants and Variables in Python</span>"
    ]
  },
  {
    "objectID": "02.intro.html#constants-in-python",
    "href": "02.intro.html#constants-in-python",
    "title": "2  Constants and Variables in Python",
    "section": "2.2 Constants in Python",
    "text": "2.2 Constants in Python\nA constant is a value that does not change during the execution of a program. Python does not have a built-in way to define constants explicitly, but naming conventions are used to indicate that a variable should be treated as a constant.\n\n2.2.1 Defining Constants\nPython does not have a specific syntax for defining constants. However, by convention, constants are written in all uppercase letters, and these variables are not supposed to be modified.\n# Defining constants (by convention)\nMAX_SPEED = 120          # A constant integer\nGRAVITY = 9.81           # A constant float\nWELCOME_MESSAGE = \"Hello\" # A constant string\n\nprint(MAX_SPEED)         # Output: 120\nprint(GRAVITY)           # Output: 9.81\nprint(WELCOME_MESSAGE)   # Output: Hello",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Constants and Variables in Python</span>"
    ]
  },
  {
    "objectID": "02.intro.html#numeric-data-types",
    "href": "02.intro.html#numeric-data-types",
    "title": "2  Constants and Variables in Python",
    "section": "3.1 Numeric Data Types",
    "text": "3.1 Numeric Data Types\n\n3.1.1 Integer (int)\nIntegers are whole numbers, positive or negative, without decimals.\n# Integer example\nage = 25\nprint(age)        # Output: 25\nprint(type(age))  # Output: &lt;class 'int'&gt;\n\n\n3.1.2 Float (float)\nFloats represent numbers with decimal points.\n# Float example\nheight = 5.9\nprint(height)        # Output: 5.9\nprint(type(height))  # Output: &lt;class 'float'&gt;\n\n\n3.1.3 Complex (complex)\nComplex numbers consist of a real and an imaginary part.\n# Complex number example\nz = 3 + 4j\nprint(z)            # Output: (3+4j)\nprint(type(z))      # Output: &lt;class 'complex'&gt;",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Constants and Variables in Python</span>"
    ]
  },
  {
    "objectID": "02.intro.html#sequence-data-types",
    "href": "02.intro.html#sequence-data-types",
    "title": "2  Constants and Variables in Python",
    "section": "3.2 Sequence Data Types",
    "text": "3.2 Sequence Data Types\n\n3.2.1 String (str)\nStrings are sequences of characters enclosed in quotes.\n# String example\nname = \"Python\"\nprint(name)        # Output: Python\nprint(type(name))  # Output: &lt;class 'str'&gt;\n\n\n3.2.2 List (list)\nLists are ordered, mutable collections of items that can hold mixed data types.\n# List example\nfruits = [\"apple\", \"banana\", \"cherry\"]\nprint(fruits)        # Output: ['apple', 'banana', 'cherry']\nprint(type(fruits))  # Output: &lt;class 'list'&gt;\n\n3.2.2.1 List Indexing and Slicing\nIndexing allows you to access individual elements in a list using their position, while slicing lets you extract a sublist from a list.\n\n3.2.2.1.1 Indexing\nIndexing starts at 0. Negative indices can be used to access elements from the end of the list.\n# List indexing example\nfruits = ['apple', 'banana', 'cherry']\n\n# Accessing elements\nprint(fruits[0])  # Output: apple\nprint(fruits[-1]) # Output: cherry\n\n\n3.2.2.1.2 Slicing\nSlicing allows you to extract a range of elements. The syntax is list[start:end], where start is inclusive and end is exclusive.\n# List slicing example\nnumbers = [0, 1, 2, 3, 4, 5]\n\n# Extracting a sublist\nprint(numbers[1:4])   # Output: [1, 2, 3]\nprint(numbers[:3])    # Output: [0, 1, 2] (from the start to index 2)\nprint(numbers[3:])    # Output: [3, 4, 5] (from index 3 to the end)\nprint(numbers[-3:])   # Output: [3, 4, 5] (last three elements)\n\n\n\n3.2.2.2 Adding Elements to a List\nYou can add elements to a list using methods like append(), insert(), and extend().\n\n3.2.2.2.1 Using append()\nappend() adds an element to the end of the list.\n# Using append() to add an element\nnumbers = [1, 2, 3]\nnumbers.append(4)\nprint(numbers)  # Output: [1, 2, 3, 4]\n\n\n3.2.2.2.2 Using insert()\ninsert() adds an element at a specified position.\n# Using insert() to add an element at a specific position\nnumbers = [1, 3, 4]\nnumbers.insert(1, 2)  # Insert 2 at index 1\nprint(numbers)  # Output: [1, 2, 3, 4]\n\n\n3.2.2.2.3 Using extend()\nextend() adds multiple elements to the end of the list.\n# Using extend() to add multiple elements\nnumbers = [1, 2]\nnumbers.extend([3, 4, 5])\nprint(numbers)  # Output: [1, 2, 3, 4, 5]\n\n\n\n3.2.2.3 Modifying and Deleting List Elements\nYou can modify or remove elements from a list using index assignments, remove(), pop(), or del.\n\n3.2.2.3.1 Modifying Elements\nAssign a new value to an index to modify an element.\n# Modifying an element in the list\nnumbers = [1, 2, 3]\nnumbers[1] = 20\nprint(numbers)  # Output: [1, 20, 3]\n\n\n3.2.2.3.2 Deleting Elements\nUse remove() to delete an element by value, pop() to remove by index, or del to delete a slice.\n# Using remove() to delete by value\nnumbers = [1, 2, 3, 4]\nnumbers.remove(3)\nprint(numbers)  # Output: [1, 2, 4]\n\n# Using pop() to delete by index\nnumbers = [1, 2, 3, 4]\nremoved_element = numbers.pop(1)\nprint(numbers)  # Output: [1, 3, 4]\nprint(removed_element)  # Output: 2\n\n# Using del to delete by index or slice\nnumbers = [1, 2, 3, 4, 5]\ndel numbers[1:3]\nprint(numbers)  # Output: [1, 4, 5]\n\n\n\n3.2.2.4 Sorting Lists\nUse sort() to sort a list in place, or sorted() to return a new sorted list.\n# Sorting a list in ascending order\nnumbers = [3, 1, 4, 1, 5]\nnumbers.sort()\nprint(numbers)  # Output: [1, 1, 3, 4, 5]\n\n# Sorting a list in descending order\nnumbers.sort(reverse=True)\nprint(numbers)  # Output: [5, 4, 3, 1, 1]\n\n# Using sorted() to return a new sorted list\nnew_numbers = sorted(numbers)\nprint(new_numbers)  # Output: [1, 1, 3, 4, 5]\n\n\n\n3.2.3 3. Tuple (tuple)\nA tuple is an ordered, immutable collection of elements. Tuples are similar to lists but cannot be modified after creation.\n# Creating and accessing tuples\ncoordinates = (10, 20)\nprint(coordinates)        # Output: (10, 20)\nprint(coordinates[0])     # Output: 10\n\n# Tuples are immutable; attempting to modify will raise an error\n# coordinates[0] = 30  # This would cause an error",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Constants and Variables in Python</span>"
    ]
  },
  {
    "objectID": "02.intro.html#mapping-data-type",
    "href": "02.intro.html#mapping-data-type",
    "title": "2  Constants and Variables in Python",
    "section": "3.3 Mapping Data Type",
    "text": "3.3 Mapping Data Type\n\n3.3.1 Dictionary (dict)\nA dictionary is a collection of key-value pairs, where each key is unique. Dictionaries are mutable, allowing for dynamic updates.\n# Dictionary example\nperson = {\"name\": \"Alice\", \"age\": 30}\nprint(person)        # Output: {'name': 'Alice', 'age': 30}\nprint(type(person))  # Output: &lt;class 'dict'&gt;\n\n3.3.1.1 Accessing Values Using Keys\nAccess values in a dictionary using keys.\n# Accessing values using keys\nname = person[\"name\"]\nage = person.get(\"age\")\nprint(name)  # Output: Alice\nprint(age)   # Output: 30\n\n\n3.3.1.2 Adding and Removing Key-Value Pairs\nYou can add new key-value pairs or remove existing ones using methods like update(), del, or pop().\n\n3.3.1.2.1 Adding Key-Value Pairs\n# Adding new key-value pairs\nperson[\"city\"] = \"New York\"\nprint(person)  # Output: {'name': 'Alice', 'age': 30, 'city': 'New York'}\n\n\n3.3.1.2.2 Removing Key-Value Pairs\n# Removing a key-value pair using pop()\nremoved_value = person.pop(\"age\")\nprint(person)        # Output: {'name': 'Alice', 'city': 'New York'}\nprint(removed_value) # Output: 30\n\n# Removing a key-value pair using del\ndel person[\"city\"]\nprint(person)  # Output: {'name': 'Alice'}",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Constants and Variables in Python</span>"
    ]
  },
  {
    "objectID": "02.intro.html#set-data-types",
    "href": "02.intro.html#set-data-types",
    "title": "2  Constants and Variables in Python",
    "section": "3.4 Set Data Types",
    "text": "3.4 Set Data Types\n\n3.4.1 Set (set)\nSets are unordered collections of unique items.\n# Set example\nnumbers = {1, 2, 3, 4}\nprint(numbers)        # Output: {1, 2, 3, 4}\nprint(type(numbers))  # Output: &lt;class 'set'&gt;",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Constants and Variables in Python</span>"
    ]
  },
  {
    "objectID": "02.intro.html#boolean-data-type",
    "href": "02.intro.html#boolean-data-type",
    "title": "2  Constants and Variables in Python",
    "section": "3.5 Boolean Data Type",
    "text": "3.5 Boolean Data Type\n\n3.5.1 Boolean (bool)\nBooleans represent one of two values: True or False.\n# Boolean example\nis_active = True\nprint(is_active)        # Output: True\nprint(type(is_active))  # Output: &lt;class 'bool'&gt;",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Constants and Variables in Python</span>"
    ]
  },
  {
    "objectID": "02.intro.html#f-string-formatting-in-python",
    "href": "02.intro.html#f-string-formatting-in-python",
    "title": "2  Constants and Variables in Python",
    "section": "3.6 f-string Formatting in Python",
    "text": "3.6 f-string Formatting in Python\nf-strings, introduced in Python 3.6, are a modern and powerful way to format strings. They offer a concise and readable syntax, allowing you to embed expressions directly inside string literals by prefixing the string with the letter f or F.\nf-strings make string formatting simpler and more intuitive compared to older methods such as % formatting or the str.format() method. They allow for inline expression evaluation, formatting of numbers, and easy manipulation of string data.\n\n3.6.1 Embedding Variables\nTo use f-strings, simply place an f before the opening quote of the string and include expressions inside curly braces {}.\n# Basic usage of f-string\nname = \"Alice\"\nage = 30\n\n# Embedding variables in a string\ngreeting = f\"Hello, {name}! You are {age} years old.\"\nprint(greeting)  # Output: Hello, Alice! You are 30 years old.\n\n\n3.6.2 Inline Expressions\nf-strings allow you to include any valid Python expression inside the curly braces.\n# Using expressions inside f-strings\nx = 10\ny = 5\n\n# Inline arithmetic expression\nresult = f\"{x} + {y} = {x + y}\"\nprint(result)  # Output: 10 + 5 = 15",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Constants and Variables in Python</span>"
    ]
  },
  {
    "objectID": "02.intro.html#advanced-formatting-with-f-strings",
    "href": "02.intro.html#advanced-formatting-with-f-strings",
    "title": "2  Constants and Variables in Python",
    "section": "3.7 Advanced Formatting with f-strings",
    "text": "3.7 Advanced Formatting with f-strings\n\n3.7.1 Number Formatting\nf-strings provide options to format numbers in various ways, such as controlling decimal places, adding commas, or displaying percentages.\n\n3.7.1.1 Formatting Floats\nYou can specify the number of decimal places by using .nf, where n is the number of decimal places.\n# Formatting a float to 2 decimal places\npi = 3.14159265358979\nformatted_pi = f\"Pi rounded to 2 decimal places: {pi:.2f}\"\nprint(formatted_pi)  # Output: Pi rounded to 2 decimal places: 3.14\n\n\n3.7.1.2 Adding Commas to Large Numbers\nUse the :, format specifier to include commas as thousand separators.\n# Adding commas to large numbers\nlarge_number = 1000000\nformatted_number = f\"The number is: {large_number:,}\"\nprint(formatted_number)  # Output: The number is: 1,000,000\n\n\n3.7.1.3 Displaying Percentages\nTo display a number as a percentage, use the % format specifier.\n# Displaying a percentage\nsuccess_rate = 0.85\nformatted_rate = f\"Success rate: {success_rate:.2%}\"\nprint(formatted_rate)  # Output: Success rate: 85.00%\n\n\n\n3.7.2 Embedding Dictionary Values\nf-strings can also be used to format values from dictionaries.\n# Formatting dictionary values\nperson = {\"name\": \"Bob\", \"age\": 40}\nformatted_string = f\"{person['name']} is {person['age']} years old.\"\nprint(formatted_string)  # Output: Bob is 40 years old.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Constants and Variables in Python</span>"
    ]
  },
  {
    "objectID": "03.control.html",
    "href": "03.control.html",
    "title": "3  Control Structures in Python",
    "section": "",
    "text": "3.1 Control Structures",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Control Structures in Python</span>"
    ]
  },
  {
    "objectID": "03.control.html#control-structures",
    "href": "03.control.html#control-structures",
    "title": "3  Control Structures in Python",
    "section": "",
    "text": "3.1.1 if Statement\nThe if statement is used to execute a block of code only if a specified condition is true. You can use if, elif (else if), and else to build more complex conditions.\n\n3.1.1.1 Basic if Statement\n# Basic if statement\nage = 18\nif age &gt;= 18:\n    print(\"You are an adult.\")  # Output: You are an adult.\n\n\n3.1.1.2 if-elif-else Structure\nYou can chain multiple conditions using elif and provide a default action using else.\n# if-elif-else example\nscore = 85\nif score &gt;= 90:\n    print(\"Grade: A\")\nelif score &gt;= 80:\n    print(\"Grade: B\")  # Output: Grade: B\nelif score &gt;= 70:\n    print(\"Grade: C\")\nelse:\n    print(\"Grade: F\")\n\n\n3.1.1.3 Nested if Statements\nif statements can be nested within each other to handle complex conditions.\n# Nested if statement\nnumber = 10\nif number &gt; 0:\n    print(\"Positive number\")  # Output: Positive number\n    if number % 2 == 0:\n        print(\"Even number\")  # Output: Even number\n\n\n\n3.1.2 while Loop\nThe while loop repeatedly executes a block of code as long as the specified condition is true. It is used when the number of iterations is not known in advance.\n\n3.1.2.1 Basic while Loop\n# Basic while loop\ncount = 1\nwhile count &lt;= 5:\n    print(count)  # Output: 1 2 3 4 5\n    count += 1\n\n\n3.1.2.2 while Loop with break and continue\n\nbreak: Exits the loop immediately.\ncontinue: Skips the current iteration and moves to the next one.\n\n# Using break and continue in a while loop\nnum = 0\nwhile num &lt; 10:\n    num += 1\n    if num == 5:\n        continue  # Skip the number 5\n    if num == 8:\n        break  # Exit the loop when num is 8\n    print(num)  # Output: 1 2 3 4 6 7\n\n\n\n3.1.3 for Loop\nThe for loop iterates over a sequence (such as a list, tuple, string, or range). It is used when the number of iterations is known.\n\n3.1.3.1 Basic for Loop\n# Basic for loop\nfor i in range(5):\n    print(i)  # Output: 0 1 2 3 4\n\n\n3.1.3.2 Iterating Over a List\n# Iterating over a list\nfruits = ['apple', 'banana', 'cherry']\nfor fruit in fruits:\n    print(fruit)  # Output: apple banana cherry\n\n\n3.1.3.3 for Loop with break and continue\n# Using break and continue in a for loop\nfor num in range(1, 10):\n    if num == 4:\n        continue  # Skip number 4\n    if num == 7:\n        break  # Stop the loop when num is 7\n    print(num)  # Output: 1 2 3 5 6\n\n\n\n3.1.4 Exception Handling\nException handling allows you to manage errors gracefully without crashing your program. Use try, except, else, and finally blocks to handle exceptions.\n\n3.1.4.1 Basic Exception Handling with try and except\n# Basic try-except block\ntry:\n    result = 10 / 0  # This will raise a ZeroDivisionError\nexcept ZeroDivisionError:\n    print(\"Cannot divide by zero!\")  # Output: Cannot divide by zero!\n\n\n3.1.4.2 Multiple Exceptions\nYou can handle multiple exceptions by specifying different exception types.\n# Handling multiple exceptions\ntry:\n    value = int(\"abc\")  # This will raise a ValueError\nexcept ValueError:\n    print(\"Invalid number format.\")  # Output: Invalid number format.\nexcept ZeroDivisionError:\n    print(\"Division by zero is not allowed.\")\n\n\n3.1.4.3 Using else and finally\n\nelse: Executes if no exception occurs.\nfinally: Executes no matter what, useful for cleanup actions.\n\n# Using else and finally with try-except\ntry:\n    num = 5\n    result = num / 1\nexcept ZeroDivisionError:\n    print(\"Cannot divide by zero.\")\nelse:\n    print(\"Division successful:\", result)  # Output: Division successful: 5.0\nfinally:\n    print(\"Execution complete.\")  # Output: Execution complete.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Control Structures in Python</span>"
    ]
  },
  {
    "objectID": "04.function.html",
    "href": "04.function.html",
    "title": "4  Functions and Packages in Python",
    "section": "",
    "text": "4.1 Functions in Python\nFunctions are reusable blocks of code that perform specific tasks. They help in organizing code, making it modular, readable, and easy to maintain. Python allows you to define your own functions, and it also comes with many built-in functions.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Functions and Packages in Python</span>"
    ]
  },
  {
    "objectID": "04.function.html#functions-in-python",
    "href": "04.function.html#functions-in-python",
    "title": "4  Functions and Packages in Python",
    "section": "",
    "text": "4.1.1 What is a Function?\nA function is a block of organized, reusable code that is used to perform a single, related action. Functions provide better modularity for your application and allow code reuse.\n\n4.1.1.1 Key Points:\n\nModularity: Functions divide complex problems into smaller, manageable tasks.\nReusability: Functions allow the code to be used repeatedly without rewriting.\nReadability: Well-defined functions make the code clearer and easier to understand.\n\n\n\n\n4.1.2 Defining a Function\nFunctions are defined using the def keyword, followed by the function name and parentheses () containing any parameters. The function body is indented and contains the code that runs when the function is called.\n\n4.1.2.1 Basic Syntax\n# Defining a simple function\ndef function_name(parameters):\n    \"\"\"\n    Optional docstring describing the function.\n    \"\"\"\n    # Function body\n    # Code to execute\n    return value  # Optional return statement\n\n\n4.1.2.2 Example of a Simple Function\n# A function to add two numbers\ndef add(a, b):\n    return a + b\n\n# Calling the function\nresult = add(3, 5)\nprint(result)  # Output: 8\n\n\n\n4.1.3 Function Parameters and Arguments\n\n4.1.3.1 Positional Arguments\nArguments passed to a function in the order they are defined.\n# Function with positional arguments\ndef greet(name, message):\n    print(f\"{message}, {name}!\")\n\ngreet(\"Alice\", \"Hello\")  # Output: Hello, Alice!\n\n\n4.1.3.2 Default Arguments\nDefault arguments allow parameters to have default values, which are used if no argument is provided.\n# Function with default arguments\ndef greet(name, message=\"Hello\"):\n    print(f\"{message}, {name}!\")\n\ngreet(\"Bob\")           # Output: Hello, Bob!\ngreet(\"Charlie\", \"Hi\") # Output: Hi, Charlie!\n\n\n4.1.3.3 Keyword Arguments\nKeyword arguments are passed by explicitly specifying the parameter names, allowing you to change the order of arguments.\n# Function with keyword arguments\ndef describe_pet(animal_type, pet_name):\n    print(f\"I have a {animal_type} named {pet_name}.\")\n\ndescribe_pet(pet_name=\"Max\", animal_type=\"dog\")  # Output: I have a dog named Max.\n\n\n4.1.3.4 Variable-Length Arguments (*args and **kwargs)\n\n*args allows a function to accept any number of positional arguments, which are passed as a tuple.\n**kwargs allows a function to accept any number of keyword arguments, which are passed as a dictionary.\n\n# Using *args\ndef sum_all(*args):\n    return sum(args)\n\nprint(sum_all(1, 2, 3, 4))  # Output: 10\n\n# Using **kwargs\ndef print_info(**kwargs):\n    for key, value in kwargs.items():\n        print(f\"{key}: {value}\")\n\nprint_info(name=\"Alice\", age=30)  \n# Output:\n# name: Alice\n# age: 30\n\n\n\n4.1.4 Return Statement\nThe return statement is used to exit a function and return a value. If no return statement is used, the function will return None by default.\n# Function with a return statement\ndef square(x):\n    return x * x\n\nresult = square(5)\nprint(result)  # Output: 25\n\n\n4.1.5 Lambda Functions\nA lambda function is a small, anonymous function defined using the lambda keyword. Lambda functions can have any number of arguments but only one expression. They are often used for short, simple operations that are used once or as arguments to other functions.\n\n4.1.5.1 Basic Syntax of a Lambda Function\n# Syntax of a lambda function\n# lambda arguments: expression\n\n# Example of a lambda function to add two numbers\nadd = lambda x, y: x + y\nprint(add(3, 5))  # Output: 8\n\n\n4.1.5.2 Use Cases of Lambda Functions\n\nUsing with map(), filter(), and reduce(): Lambda functions are commonly used with these built-in functions for quick operations.\n\n# Using lambda with map() to square each number in a list\nnumbers = [1, 2, 3, 4]\nsquared = list(map(lambda x: x**2, numbers))\nprint(squared)  # Output: [1, 4, 9, 16]\n\nSorting by a Custom Key: You can use lambda functions as the key argument in sorting.\n\n# Sorting a list of tuples by the second element using a lambda function\npoints = [(2, 3), (1, 2), (3, 1)]\nsorted_points = sorted(points, key=lambda x: x[1])\nprint(sorted_points)  # Output: [(3, 1), (1, 2), (2, 3)]\n\n\n4.1.5.3 Limitations of Lambda Functions\n\nLimited to a single expression.\nLess readable for complex operations compared to regular functions.\nCannot contain statements or annotations.\n\n\n\n\n4.1.6 Scope and Lifetime of Variables\nVariables defined inside a function are local to that function and cannot be accessed outside. Variables defined outside functions are global.\n# Scope example\ndef my_function():\n    local_var = \"I am local\"\n    print(local_var)\n\nmy_function()            # Output: I am local\n# print(local_var)       # This would raise an error: NameError\n\n\n4.1.7 Docstrings\nDocstrings are used to document functions. They provide a way to describe what the function does, its parameters, and its return value.\n# Function with a docstring\ndef multiply(a, b):\n    \"\"\"\n    Multiply two numbers.\n\n    Parameters:\n    a (int or float): The first number.\n    b (int or float): The second number.\n\n    Returns:\n    int or float: The product of a and b.\n    \"\"\"\n    return a * b\n\nhelp(multiply)  # This will display the function's docstring",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Functions and Packages in Python</span>"
    ]
  },
  {
    "objectID": "04.function.html#packages",
    "href": "04.function.html#packages",
    "title": "4  Functions and Packages in Python",
    "section": "4.2 Packages",
    "text": "4.2 Packages\nA package is a collection of Python modules grouped together within a directory. Packages help organize and distribute reusable code across multiple projects. They make it easy to manage and share code.\n\n4.2.1 Importing a Package\nYou can import specific modules from a package or import the entire package.\n# Importing a specific module from a package\nfrom mypackage import module1\n\n# Importing a specific function from a module within a package\nfrom mypackage.module1 import my_function\n\n\n4.2.2 Using Standard Packages\nPython has a rich standard library with many built-in packages, such as math, os, and random. You can also install third-party packages using pip.\n# Using a function from the math package\nimport math\n\nresult = math.sqrt(16)\nprint(result)  # Output: 4.0",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Functions and Packages in Python</span>"
    ]
  },
  {
    "objectID": "04.function.html#difference-between-functions-and-methods",
    "href": "04.function.html#difference-between-functions-and-methods",
    "title": "4  Functions and Packages in Python",
    "section": "4.3 Difference Between Functions and Methods",
    "text": "4.3 Difference Between Functions and Methods\n\n4.3.1 Functions\n\nDefinition: A function is a block of reusable code that performs a specific task. It can be defined using the def keyword and can be called independently of objects.\nUsage: Functions are generally used to perform a task, and they can accept parameters and return values.\n\n# Example of a function\ndef greet(name):\n    return f\"Hello, {name}!\"\n\nprint(greet(\"Alice\"))  # Output: Hello, Alice!\n\n\n4.3.2 Methods\n\nDefinition: A method is a function that is associated with an object. It is called on an object and usually works with the data contained in that object.\nUsage: Methods are used to perform operations on objects and are called using the dot notation.\n\n# Example of a method in a class\nclass Person:\n    def __init__(self, name):\n        self.name = name\n\n    def greet(self):\n        return f\"Hello, {self.name}!\"\n\n# Creating an object and calling the greet method\nperson = Person(\"Bob\")\nprint(person.greet())  # Output: Hello, Bob!\n\n\n4.3.3 Key Differences\n\nAssociation: Functions are standalone, while methods are associated with objects.\nCalling: Functions are called directly, while methods are called on objects using dot notation.\n4.4 First Parameter: Methods often have self as their first parameter, referring to the object itself, whereas functions do not.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Functions and Packages in Python</span>"
    ]
  },
  {
    "objectID": "05.pandas.html",
    "href": "05.pandas.html",
    "title": "5  Data Analysis in Python",
    "section": "",
    "text": "5.1 Series\nA Series is a one-dimensional array-like object provided by pandas that can hold data of any type (integers, floats, strings, Python objects, etc.). It is similar to a column in an Excel spreadsheet or a single-column DataFrame in pandas. Each element in a Series has an associated index, which is used to access individual elements.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Analysis in Python</span>"
    ]
  },
  {
    "objectID": "05.pandas.html#series",
    "href": "05.pandas.html#series",
    "title": "5  Data Analysis in Python",
    "section": "",
    "text": "5.1.1 Creating a Series\nYou can create a Series in pandas using various data structures such as lists, dictionaries, or NumPy arrays. You can also specify the index manually.\n\n5.1.1.1 Creating a Series from a List\nThe simplest way to create a Series is by passing a list to the pd.Series() constructor.\nimport pandas as pd\n\n# Creating a Series from a list\ndata = [10, 20, 30, 40]\nseries = pd.Series(data)\nprint(series)\nOutput:\n0    10\n1    20\n2    30\n3    40\ndtype: int64\nIn this example, the default index starts at 0 and increments by 1.\n\n\n5.1.1.2 Creating a Series with Custom Index\nYou can specify custom indices when creating a Series, making it more descriptive.\n# Creating a Series with custom indices\ndata = [100, 200, 300]\nindex = ['a', 'b', 'c']\ncustom_series = pd.Series(data, index=index)\nprint(custom_series)\nOutput:\na    100\nb    200\nc    300\ndtype: int64\n\n\n5.1.1.3 Creating a Series from a Dictionary\nYou can also create a Series from a dictionary, where keys become the indices, and values become the Series values.\n# Creating a Series from a dictionary\ndata_dict = {'apple': 50, 'banana': 30, 'cherry': 20}\nfruit_series = pd.Series(data_dict)\nprint(fruit_series)\nOutput:\napple     50\nbanana    30\ncherry    20\ndtype: int64\n\n\n\n5.1.2 Selecting Elements\nSelecting elements from a Series can be done using indexing and slicing, similar to Python lists and NumPy arrays. The indexing can be done using integer-based indexing or label-based indexing if a custom index is set.\n\n5.1.2.1 1.2.1 Accessing Single Elements\nYou can access single elements by using their index position or label.\n# Accessing elements by index position\nprint(series[0])    # Output: 10\n\n# Accessing elements by index label\nprint(custom_series['b'])  # Output: 200\n\n\n5.1.2.2 Slicing a Series\nSlicing allows you to select a range of elements from a Series.\n# Slicing using index positions\nprint(series[1:3])  # Output: \n# 1    20\n# 2    30\n# dtype: int64\n\n# Slicing using index labels\nprint(custom_series['a':'b'])  # Output:\n# a    100\n# b    200\n# dtype: int64\n\n\n\n5.1.3 Series Operations\nSeries in pandas support vectorized operations, which means you can perform operations on entire Series without writing loops. These operations are element-wise, and pandas will align data using the index.\n\n5.1.3.1 Arithmetic Operations\nYou can perform arithmetic operations like addition, subtraction, multiplication, and division directly on Series.\n# Arithmetic operations\nseries1 = pd.Series([1, 2, 3, 4])\nseries2 = pd.Series([10, 20, 30, 40])\n\n# Adding two Series\nsum_series = series1 + series2\nprint(sum_series)\n\n# Output:\n# 0    11\n# 1    22\n# 2    33\n# 3    44\n# dtype: int64\n\n\n5.1.3.2 Mathematical Functions\nYou can use mathematical functions like mean(), sum(), max(), and others on Series.\n# Using mathematical functions\nvalues = pd.Series([5, 10, 15, 20])\nprint(\"Mean:\", values.mean())   # Output: Mean: 12.5\nprint(\"Sum:\", values.sum())     # Output: Sum: 50\nprint(\"Max:\", values.max())     # Output: Max: 20\n\n\n5.1.3.3 Applying Functions with apply()\nThe apply() function allows you to apply a custom function to each element in the Series.\n# Applying a custom function to each element\ndef square(x):\n    return x * x\n\nsquared_series = values.apply(square)\nprint(squared_series)\n\n# Output:\n# 0     25\n# 1    100\n# 2    225\n# 3    400\n# dtype: int64\n\n\n5.1.3.4 Handling Missing Data\nSeries can handle missing data (NaN values), and you can manipulate these values using methods like fillna(), dropna(), and isna().\n# Series with missing values\ndata_with_nan = pd.Series([1, 2, None, 4])\nprint(data_with_nan)\n\n# Filling missing values\nfilled_series = data_with_nan.fillna(0)\nprint(filled_series)\n\n# Output:\n# 0    1.0\n# 1    2.0\n# 2    0.0\n# 3    4.0\n# dtype: float64\n\n\n\n5.1.4 Conclusion\nSeries in pandas is a powerful tool for one-dimensional data manipulation, allowing for efficient data access, arithmetic operations, and handling of missing data. Understanding Series is fundamental to working with more complex pandas data structures like DataFrames.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Analysis in Python</span>"
    ]
  },
  {
    "objectID": "05.pandas.html#dataframe",
    "href": "05.pandas.html#dataframe",
    "title": "5  Data Analysis in Python",
    "section": "5.2 DataFrame",
    "text": "5.2 DataFrame\nA DataFrame is a two-dimensional, size-mutable, and heterogeneous tabular data structure with labeled axes (rows and columns) in pandas. Think of it as a table of data, similar to an Excel spreadsheet or a SQL table, which can hold data of various types (strings, integers, floats, etc.).\n\n5.2.1 Creating and Modifying DataFrames\nDataFrames can be created from various sources, including lists, dictionaries, NumPy arrays, and even other pandas objects like Series.\n\n5.2.1.1 Creating a DataFrame from a Dictionary\nThe most common way to create a DataFrame is by using a dictionary, where the keys become column names, and the values are lists representing column data.\nimport pandas as pd\n\n# Creating a DataFrame from a dictionary\ndata = {\n    'Name': ['Alice', 'Bob', 'Charlie'],\n    'Age': [25, 30, 35],\n    'City': ['New York', 'Los Angeles', 'Chicago']\n}\ndf = pd.DataFrame(data)\nprint(df)\nOutput:\n      Name  Age         City\n0    Alice   25     New York\n1      Bob   30  Los Angeles\n2  Charlie   35      Chicago\n\n\n5.2.1.2 Creating a DataFrame from a List of Lists\nYou can create a DataFrame from a list of lists, specifying column names separately.\n# Creating a DataFrame from a list of lists\ndata = [\n    ['Alice', 25, 'New York'],\n    ['Bob', 30, 'Los Angeles'],\n    ['Charlie', 35, 'Chicago']\n]\ncolumns = ['Name', 'Age', 'City']\ndf = pd.DataFrame(data, columns=columns)\nprint(df)\n\n\n5.2.1.3 Creating a DataFrame from a NumPy Array\nDataFrames can also be created from NumPy arrays. You must specify column names as pandas does not infer them automatically.\nimport numpy as np\n\n# Creating a DataFrame from a NumPy array\narray_data = np.array([[1, 2], [3, 4], [5, 6]])\ndf = pd.DataFrame(array_data, columns=['Column1', 'Column2'])\nprint(df)\n\n\n5.2.1.4 Modifying a DataFrame\nYou can add, modify, or delete rows and columns in a DataFrame.\nAdding a Column:\n# Adding a new column to an existing DataFrame\ndf['Score'] = [90, 85, 88]\nprint(df)\nModifying a Column:\n# Modifying an existing column\ndf['Age'] = df['Age'] + 1\nprint(df)\nDeleting a Column:\n# Deleting a column\ndf.drop('Score', axis=1, inplace=True)\nprint(df)\n\n\n\n5.2.2 Selecting Rows and Columns\nSelecting data from a DataFrame can be done using indexing, loc, and iloc.\n\n5.2.2.1 Selecting Columns\nColumns can be selected using square brackets, dot notation, or loc.\n# Selecting a single column\nnames = df['Name']\nprint(names)\n\n# Selecting multiple columns\nsubset = df[['Name', 'City']]\nprint(subset)\n\n\n5.2.2.2 Selecting Rows\nRows can be selected by index position using iloc or by label using loc.\nUsing iloc (Index Location):\niloc selects rows and columns by integer index positions.\n# Selecting rows using iloc\nprint(df.iloc[0])  # First row\nprint(df.iloc[1:3])  # Slicing rows\nUsing loc (Label Location):\nloc selects rows and columns by label (index and column names).\n# Selecting rows using loc\ndf.set_index('Name', inplace=True)  # Setting 'Name' column as index\nprint(df.loc['Alice'])  # Select row with index 'Alice'\n\n\n\n5.2.3 Filtering Data in DataFrames\nFiltering allows you to extract specific rows based on conditions.\nFiltering Rows Based on Conditions:\n# Filtering rows where Age &gt; 30\nfiltered_df = df[df['Age'] &gt; 30]\nprint(filtered_df)\n\n\n5.2.4 Adding and Removing Rows\nYou can add rows to a DataFrame using the append() method and remove rows using drop().\n\n5.2.4.1 Adding Rows\n# Adding a new row using append() # will be expired.\nnew_row = pd.DataFrame([['David', 28, 'Seattle']], columns=['Name', 'Age', 'City'])\ndf = df.append(new_row, ignore_index=True)\nprint(df)\nHere’s the revised version using updated pandas methods such as pd.concat() instead of append():\nimport pandas as pd\n\n# Creating the initial DataFrame\ndata = {\n    'Name': ['Alice', 'Bob', 'Charlie'],\n    'Age': [25, 30, 35],\n    'City': ['New York', 'Los Angeles', 'Chicago']\n}\ndf = pd.DataFrame(data)\n\n# Adding a new row using pd.concat()\nnew_row = pd.DataFrame([['David', 28, 'Seattle']], columns=['Name', 'Age', 'City'])\ndf = pd.concat([df, new_row], ignore_index=True)\nprint(df)\n\n\n5.2.4.2 Removing Rows\n# Removing rows by index\ndf = df.drop(0)  # Removes the first row\nprint(df)\nIn this updated code: - Rows are added using pd.concat(), which is the recommended method in the newer versions of pandas. - The drop() method is still valid and is used to remove rows by their index.\n\n\n\n5.2.5 Handling Missing Data in DataFrames\nHandling missing data is essential for clean and accurate data analysis.\n\n5.2.5.1 Identifying Missing Data\nimport pandas as pd\nimport numpy as np\n\n# Creating a DataFrame with missing values\ndata = {\n    'Name': ['Alice', 'Bob', 'Charlie', 'David'],\n    'Age': [25, 30, np.nan, 40],\n    'Salary': [50000, 60000, np.nan, 80000]\n}\ndf = pd.DataFrame(data)\n\n# Checking for missing data\nprint(df.isna())  # Returns a DataFrame of booleans\nprint(df.isna().sum())  # Sum of missing values in each column\n\n\n5.2.5.2 Filling Missing Data\nYou can fill missing data with a specific value, such as 0 or the mean of the column.\n# Filling missing values with a specified value\ndf['Age'].fillna(df['Age'].mean(), inplace=True)\nThe warning you received is related to pandas’ behavior when using chained assignments. In future versions, using inplace=True with column-level operations might not work as expected. To fix this issue and ensure compatibility with future versions of pandas, you should avoid using inplace=True in this context. Instead, you can assign the result of the fillna() operation directly back to the DataFrame.\nHere’s the corrected code:\nimport pandas as pd\nimport numpy as np\n\n# Creating a DataFrame with missing values\ndata = {\n    'Name': ['Alice', 'Bob', 'Charlie', 'David'],\n    'Age': [25, 30, np.nan, 40],\n    'Salary': [50000, 60000, np.nan, 80000]\n}\ndf = pd.DataFrame(data)\n\n# Filling missing values in the 'Salary' column and assigning back to the original DataFrame\ndf['Salary'] = df['Salary'].fillna(df['Salary'].mean())\n\nprint(df)\n\n\n5.2.5.3 Dropping Missing Data\nRemove rows or columns that contain missing values.\n# Dropping rows with missing values\ndf.dropna(inplace=True)\ndf\n\n\n\n5.2.6 Sorting Data in DataFrames\nSorting can be performed based on one or multiple columns using the sort_values() method.\n# Sorting DataFrame by a column\nsorted_df = df.sort_values(by='Age', ascending=False)\nprint(sorted_df)\n\n\n5.2.7 Grouping Data (cf. MS Excel ‘PivotTable’)\nGrouping data is useful for performing aggregate operations on subsets of data.\n\n5.2.7.1 Option 1: Select Numeric Columns Before Grouping\n# Grouping by a column and calculating the mean\nimport pandas as pd\n\n# Creating a DataFrame\ndata = {\n    'Name': ['Alice', 'Bob', 'Charlie', 'David', 'Emma'],\n    'Age': [25, 30, 35, 40, 28],\n    'Salary': [50000, 60000, 70000, 80000, 55000],\n    'Department': ['HR', 'IT', 'IT', 'HR', 'HR']\n}\ndf = pd.DataFrame(data)\nprint(df)\n\n# Grouping by the 'Department' column and calculating the mean of only numeric columns\ngrouped = df.groupby('Department')[['Age', 'Salary']].mean()\n\nprint(grouped)\n\n\n5.2.7.2 Option 2: Use numeric_only=True (if available)\nimport pandas as pd\n\n# Creating a DataFrame\ndata = {\n    'Name': ['Alice', 'Bob', 'Charlie', 'David', 'Emma'],\n    'Age': [25, 30, 35, 40, 28],\n    'Salary': [50000, 60000, 70000, 80000, 55000],\n    'Department': ['HR', 'IT', 'IT', 'HR', 'HR']\n}\ndf = pd.DataFrame(data)\n\n# Grouping by the 'Department' column and calculating the mean, ignoring non-numeric columns\ngrouped = df.groupby('Department').mean(numeric_only=True)\n\nprint(grouped)\n\n\n\n5.2.8 Merging DataFrames\nMerging combines multiple DataFrames into one, similar to SQL JOIN operations.\n\n5.2.8.1 Using merge()\n# Merging two DataFrames on a common column\ndf1 = pd.DataFrame({'Name': ['Alice', 'Bob'], 'Score': [85, 90]})\ndf2 = pd.DataFrame({'Name': ['Alice', 'Bob'], 'Grade': ['A', 'B']})\n\nprint(df1)\nprint('---')\nprint(df2)\nprint('---')\n\nmerged_df = pd.merge(df1, df2, on='Name')\nprint(merged_df)\n\n\n5.2.8.2 Using concat()\n# Concatenating DataFrames \ndf3 = pd.concat([df1, df2], axis=1)\nprint(df3)\n\n\n\n5.2.9 Exporting DataFrames\nYou can export DataFrames to various file formats, including CSV, Excel, and JSON.\n# Install the correct library\n!pip install openpyxl\n\nimport pandas as pd\nimport openpyxl\n\n# Creating a DataFrame\ndata = {\n    'Name': ['Alice', 'Bob', 'Charlie', 'David', 'Emma'],\n    'Age': [25, 30, 35, 40, 28],\n    'Salary': [50000, 60000, 70000, 80000, 55000],\n    'Department': ['HR', 'IT', 'IT', 'HR', 'HR']\n}\ndf = pd.DataFrame(data)\n# Saving DataFrame to a CSV file\ndf.to_csv('output.csv', index=False)\n\n# Saving DataFrame to an Excel file\ndf.to_excel('output.xlsx', index=False)\n\n\n5.2.10 Conclusion\nDataFrames in pandas are versatile and powerful for handling tabular data in Python. They provide robust functionality for data manipulation, cleaning, and analysis, making them an essential tool for data scientists and analysts.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Analysis in Python</span>"
    ]
  },
  {
    "objectID": "05.pandas.html#loading-and-saving-data",
    "href": "05.pandas.html#loading-and-saving-data",
    "title": "5  Data Analysis in Python",
    "section": "5.3 Loading and Saving Data",
    "text": "5.3 Loading and Saving Data\nPandas provides powerful methods to load data from various file formats and save it back after manipulation. Common formats include CSV, Excel, JSON, and SQL databases.\n\n5.3.1 Loading Data\nPandas offers various functions to read data from external sources into DataFrames, allowing for quick data analysis and manipulation.\n\n5.3.1.1 Reading CSV Files\nThe read_csv() function reads data from a CSV file into a DataFrame. This is one of the most common methods to import data.\nimport pandas as pd\n\n# Reading a CSV file into a DataFrame\ndf = pd.read_csv('data.csv')\nprint(df.head())  # Display the first 5 rows\n\n\n5.3.1.2 Reading Excel Files\nTo read Excel files, use the read_excel() function. You can specify the sheet name if the Excel file contains multiple sheets.\n# Reading data from an Excel file\ndf = pd.read_excel('data.xlsx', sheet_name='Sheet1')\nprint(df.head())\n\n\n5.3.1.3 Reading JSON Files\nJSON files can be read using the read_json() function. JSON is commonly used for data exchange between systems.\n# Reading data from a JSON file\ndf = pd.read_json('data.json')\nprint(df.head())\n\n\n\n5.3.2 Saving Data\nOnce you have manipulated your data, you can save it back to your preferred format using pandas functions.\n\n5.3.2.1 Saving to CSV\nThe to_csv() function saves the DataFrame to a CSV file. You can choose whether to include the index or not.\n# Saving DataFrame to a CSV file\ndf.to_csv('output.csv', index=False)\n\n\n5.3.2.2 Saving to Excel\nTo save data to an Excel file, use the to_excel() function. You can also specify the sheet name.\n# Saving DataFrame to an Excel file\ndf.to_excel('output.xlsx', index=False, sheet_name='Results')\n\n\n5.3.2.3 Saving to JSON\nYou can save data to JSON format using the to_json() function, which is useful for data interchange between applications.\n# Saving DataFrame to a JSON file\ndf.to_json('output.json')",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Analysis in Python</span>"
    ]
  },
  {
    "objectID": "05.pandas.html#exploring-summary-information-and-statistical-values",
    "href": "05.pandas.html#exploring-summary-information-and-statistical-values",
    "title": "5  Data Analysis in Python",
    "section": "5.4 Exploring Summary Information and Statistical Values",
    "text": "5.4 Exploring Summary Information and Statistical Values\nExploring data is an essential step in understanding the structure, distribution, and characteristics of your dataset. Pandas provides several methods to inspect and summarize DataFrames.\n\n5.4.1 Viewing Basic Information\nThe info() method provides a concise summary of the DataFrame, including the index, column names, non-null counts, and data types.\nimport pandas as pd\n\n# Expanding the data with more people and overlapping cities\ndata = {\n    'Name': ['Alice', 'Bob', 'Charlie', 'David', 'Emma', 'Frank', 'Grace', 'Hannah', 'Ian', 'Jack'],\n    'Age': [25, 30, 35, 28, 22, 40, 33, 26, 29, 31],\n    'City': ['New York', 'Los Angeles', 'Chicago', 'New York', 'Los Angeles', 'San Francisco', 'Chicago', 'Boston', 'New York', 'Boston']\n}\n\n# Creating the DataFrame\ndf = pd.DataFrame(data)\ndf\n# Viewing basic information about the DataFrame\nprint(df.info())\n\n\n5.4.2 Viewing Statistical Summaries\nThe describe() method generates descriptive statistics that summarize the central tendency, dispersion, and shape of the data distribution, excluding NaN values.\n# Viewing statistical summary of numerical columns\nprint(df.describe())\nYou can also include non-numerical data by specifying include='all'.\n# Including all columns, including non-numeric\nprint(df.describe(include='all'))\n\n\n5.4.3 Counting Unique Values\nThe value_counts() function counts the unique values in a column, which is particularly useful for categorical data.\n# Counting unique values in a column\nprint(df['City'].value_counts())\n\n\n5.4.4 Displaying First and Last Rows\nYou can use head() and tail() to view the first and last few rows of the DataFrame.\n# Displaying the first 5 rows\nprint(df.head())\n\n# Displaying the last 5 rows\nprint(df.tail())",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Analysis in Python</span>"
    ]
  },
  {
    "objectID": "05.pandas.html#handling-missing-data-1",
    "href": "05.pandas.html#handling-missing-data-1",
    "title": "5  Data Analysis in Python",
    "section": "5.5 Handling Missing Data",
    "text": "5.5 Handling Missing Data\nHandling missing data is a crucial aspect of data cleaning, as missing values can lead to incorrect analysis results.\n\n5.5.1 Identifying Missing Data\nUse isna() or isnull() to detect missing values. These methods return a DataFrame of booleans indicating the presence of missing values.\n# Creating a DataFrame with missing values for demonstrating missing data handling\ndata_with_missing = {\n    'Name': ['Alice', 'Bob', 'Charlie', 'David', 'Emma', 'Frank', 'Grace', 'Hannah', 'Ian', 'Jack'],\n    'Age': [25, 30, None, 28, 22, 40, 33, None, 29, 31],  # Introducing missing values in 'Age'\n    'City': ['New York', 'Los Angeles', 'Chicago', None, 'Los Angeles', 'San Francisco', 'Chicago', 'Boston', 'New York', None]  # Introducing missing values in 'City'\n}\n\n# Creating the DataFrame with missing data\ndf_missing = pd.DataFrame(data_with_missing)\ndf_missing\n# Identifying missing data\nprint(df_missing.isna())\n\n# Counting missing values in each column\nprint(df_missing.isna().sum())\n\n\n5.5.2 Removing Missing Data\nYou can remove rows or columns with missing values using the dropna() method.\n# Dropping rows with any missing values\ndf_cleaned1 = df_missing.dropna()\nprint(df_cleaned1)\n\n# Dropping columns with missing values\ndf_cleaned2 = df_missing.dropna(axis=1)\nprint(df_cleaned2)\n\n\n5.5.3 Filling Missing Data\nInstead of dropping missing data, you can fill it with specified values using fillna(). Common strategies include filling with zeros, the mean, or forward/backward filling.\n# Filling missing values with a specified value\ndf_missing['Age'].fillna(0)\n\n# Filling missing values with the column mean\ndf_missing['Age'].fillna(df_missing['Age'].mean())\ndf_missing\n\n# Forward filling missing values\ndf_missing.fillna(method='ffill')",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Analysis in Python</span>"
    ]
  },
  {
    "objectID": "05.pandas.html#working-with-indices",
    "href": "05.pandas.html#working-with-indices",
    "title": "5  Data Analysis in Python",
    "section": "5.6 Working with Indices",
    "text": "5.6 Working with Indices\nThe index of a DataFrame is used to uniquely identify rows. Managing indices effectively allows for more efficient data manipulation and retrieval.\n\n5.6.1 Setting and Resetting Index\nYou can set a column as the index using set_index() and reset it back to the default integer index using reset_index().\n# Setting 'Name' as the index\ndf.set_index('Name', inplace=True)\nprint(df)\n\n# Resetting the index to default\ndf.reset_index(inplace=True)\nprint(df)\n\n\n5.6.2 Sorting by Index\nThe sort_index() method sorts the DataFrame by its index.\n# Sorting the DataFrame by index\ndf.sort_index(inplace=True)\nprint(df)\n\n\n5.6.3 Changing the Index Name\nYou can rename the index by assigning a new name directly.\n# Renaming the index\ndf.index.name = 'ID'\nprint(df)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Analysis in Python</span>"
    ]
  },
  {
    "objectID": "05.pandas.html#filtering-data",
    "href": "05.pandas.html#filtering-data",
    "title": "5  Data Analysis in Python",
    "section": "5.7 Filtering Data",
    "text": "5.7 Filtering Data\nFiltering allows you to select rows based on specific conditions, enabling targeted analysis and manipulation.\n\n5.7.1 Boolean Indexing\nBoolean indexing uses conditional statements to filter data.\n# Filtering rows where Age &gt; 30\nfiltered_df = df[df['Age'] &gt; 30]\nprint(filtered_df)\n\n\n5.7.2 Using isin() Method\nThe isin() method filters rows based on multiple values in a column.\n# Filtering rows where Name is Alice or Bob\nfiltered_df = df[df['Name'].isin(['Alice', 'Bob'])]\nprint(filtered_df)\n\n\n5.7.3 Filtering with Multiple Conditions\nYou can combine multiple conditions using & (and) or | (or).\n# Filtering with multiple conditions\nfiltered_df = df[(df['Age'] &gt; 30) & (df['City'] == 'Chicago')]\nprint(filtered_df)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Analysis in Python</span>"
    ]
  },
  {
    "objectID": "05.pandas.html#merging-dataframes-1",
    "href": "05.pandas.html#merging-dataframes-1",
    "title": "5  Data Analysis in Python",
    "section": "5.8 Merging DataFrames",
    "text": "5.8 Merging DataFrames\nMerging DataFrames is an essential operation when working with multiple datasets that need to be combined. Pandas offers several functions for merging, joining, and concatenating DataFrames.\n\n5.8.1 Using concat() Function\nThe concat() function concatenates DataFrames along rows (default) or columns. It is useful when you have DataFrames with the same columns and want to stack them.\nimport pandas as pd\n\n# Creating sample DataFrames\ndf1 = pd.DataFrame({'Name': ['Alice', 'Bob'], 'Age': [25, 30]})\ndf2 = pd.DataFrame({'Name': ['Charlie', 'David'], 'Age': [35, 40]})\n\n# Concatenating DataFrames vertically\ndf_concat = pd.concat([df1, df2], ignore_index=True)\nprint(df_concat)\n\n# Output:\n#       Name  Age\n# 0    Alice   25\n# 1      Bob   30\n# 2  Charlie   35\n# 3    David   40\nYou can also concatenate DataFrames horizontally by specifying axis=1.\n# Concatenating DataFrames horizontally\ndf_concat_horizontal = pd.concat([df1, df2], axis=1)\nprint(df_concat_horizontal)\n\n\n5.8.2 Using merge() Function\nThe merge() function combines two DataFrames based on a key column or index. It works similarly to SQL JOIN operations (inner, outer, left, right).\n# Creating sample DataFrames for merging\ndf1 = pd.DataFrame({'Name': ['Alice', 'Bob'], 'Score': [85, 90]})\ndf2 = pd.DataFrame({'Name': ['Alice', 'Bob'], 'Grade': ['A', 'B']})\n\n# Merging DataFrames on the 'Name' column\ndf_merged = pd.merge(df1, df2, on='Name', how='inner')\nprint(df_merged)\n\n# Output:\n#     Name  Score Grade\n# 0  Alice     85     A\n# 1    Bob     90     B\n\n5.8.2.1 SQL Join Methods\n\n5.8.2.1.1 1. Inner Join\nAn inner join returns rows when there is a match in both tables.\n\n\n5.8.2.1.2 2. Left Join (or Left Outer Join)\nA left join returns all rows from the left table, and the matched rows from the right table. If no match is found, the result is NULL on the right side.\n\n\n5.8.2.1.3 3. Right Join (or Right Outer Join)\nA right join returns all rows from the right table, and the matched rows from the left table. If no match is found, the result is NULL on the left side.\n\n\n5.8.2.1.4 4. Full Join (or Full Outer Join)\nA full join returns rows when there is a match in either table. It returns all rows from both tables and fills NULL where there is no match.\n\n\n\n\n5.8.2.2 Diagram of SQL Joins:\n1. INNER JOIN:\n+--------+     +--------+\n|  LEFT  |     |  RIGHT |\n+--------+     +--------+\n   +--------+     +--------+\n   | common |     | common |\n   +--------+     +--------+\n   \n2. LEFT JOIN:\n+--------+     +--------+\n|  LEFT  |     |  RIGHT |\n+--------+     +--------+\n   +--------+     +--------+\n   | all    |     | common |\n   | from   |     +--------+\n   | left   |\n   +--------+\n\n3. RIGHT JOIN:\n+--------+     +--------+\n|  LEFT  |     |  RIGHT |\n+--------+     +--------+\n   +--------+     +--------+\n   | common |     | all    |\n   +--------+     | from   |\n                  | right  |\n                  +--------+\n\n4. FULL JOIN:\n+--------+     +--------+\n|  LEFT  |     |  RIGHT |\n+--------+     +--------+\n   +--------+     +--------+\n   | all    |     | all    |\n   | from   |     | from   |\n   | both   |     | both   |\n   +--------+     +--------+\n\n\n\n5.8.2.3 Python Example with DataFrames\nLet’s create two DataFrames and perform different types of joins using the merge() function in pandas, which simulates SQL joins.\nimport pandas as pd\n\n# Creating two DataFrames\ndf1 = pd.DataFrame({\n    'Name': ['Alice', 'Bob', 'Charlie', 'David'],\n    'Age': [25, 30, 35, 40]\n})\n\ndf2 = pd.DataFrame({\n    'Name': ['Alice', 'Charlie', 'Eve'],\n    'City': ['New York', 'Chicago', 'Los Angeles']\n})\n\n# Display the DataFrames\nprint(\"DataFrame 1:\")\nprint(df1)\nprint(\"DataFrame 2:\")\nprint(df2)\n\n# Inner Join\ninner_join = pd.merge(df1, df2, on='Name', how='inner')\nprint(\"\\nInner Join:\")\nprint(inner_join)\n\n# Left Join\nleft_join = pd.merge(df1, df2, on='Name', how='left')\nprint(\"\\nLeft Join:\")\nprint(left_join)\n\n# Right Join\nright_join = pd.merge(df1, df2, on='Name', how='right')\nprint(\"\\nRight Join:\")\nprint(right_join)\n\n# Full Outer Join\nfull_join = pd.merge(df1, df2, on='Name', how='outer')\nprint(\"\\nFull Outer Join:\")\nprint(full_join)\n\n\n\n5.8.3 Using join() Function\nThe join() function is used to join two DataFrames based on their indices. It is particularly useful for merging DataFrames with overlapping indices.\n# Creating sample DataFrames with indices\ndf1 = pd.DataFrame({'Score': [85, 90]}, index=['Alice', 'Bob'])\ndf2 = pd.DataFrame({'Grade': ['A', 'B']}, index=['Alice', 'Bob'])\n\n# Joining DataFrames on indices\ndf_joined = df1.join(df2)\nprint(df_joined)\n\n# Output:\n#        Score Grade\n# Alice     85     A\n# Bob       90     B",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Analysis in Python</span>"
    ]
  },
  {
    "objectID": "05.pandas.html#reshaping-data",
    "href": "05.pandas.html#reshaping-data",
    "title": "5  Data Analysis in Python",
    "section": "5.9 Reshaping Data",
    "text": "5.9 Reshaping Data\nReshaping allows you to change the layout of a DataFrame, making it suitable for different types of analysis.\n\n5.9.1 Using melt() Function\nThe melt() function unpivots a DataFrame from a wide format to a long format, making it easier to analyze and visualize.\n# Creating a sample DataFrame\ndf = pd.DataFrame({\n    'Name': ['Alice', 'Bob'],\n    'Math': [90, 80],\n    'Science': [85, 95]\n})\n\n# Melting the DataFrame\ndf_melted = pd.melt(df, id_vars=['Name'], value_vars=['Math', 'Science'], \n                    var_name='Subject', value_name='Score')\nprint(df_melted)\n\n# Output:\n#     Name  Subject  Score\n# 0  Alice     Math     90\n# 1    Bob     Math     80\n# 2  Alice  Science     85\n# 3    Bob  Science     95\n\n\n5.9.2 Using pivot_table() Function\nThe pivot_table() function reshapes data by creating a new summary table, which is useful for aggregating data.\n# Creating a DataFrame for pivoting\ndf = pd.DataFrame({\n    'Name': ['Alice', 'Bob', 'Alice', 'Bob'],\n    'Subject': ['Math', 'Math', 'Science', 'Science'],\n    'Score': [90, 80, 85, 95]\n})\n\nprint(df)\nprint('---')\n\n# Pivoting the DataFrame\ndf_pivot = df.pivot_table(values='Score', index='Name', columns='Subject', \n                          aggfunc='mean')\nprint(df_pivot)\n\n# Output:\n# Subject  Math  Science\n# Name                  \n# Alice      90       85\n# Bob        80       95\n\n\n5.9.3 Using stack() and unstack()\nstack() compresses columns into rows, while unstack() does the opposite, expanding rows into columns.\n# Stacking and unstacking a DataFrame\ndf = pd.DataFrame({\n    'A': [1, 2],\n    'B': [3, 4]\n}, index=['X', 'Y'])\n\n# Stacking the DataFrame\nstacked = df.stack()\nprint(stacked)\n\n# Unstacking the stacked DataFrame\nunstacked = stacked.unstack()\nprint(unstacked)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Analysis in Python</span>"
    ]
  },
  {
    "objectID": "05.pandas.html#applying-functions-to-dataframes",
    "href": "05.pandas.html#applying-functions-to-dataframes",
    "title": "5  Data Analysis in Python",
    "section": "5.10 Applying Functions to DataFrames",
    "text": "5.10 Applying Functions to DataFrames\nApplying functions to DataFrames allows you to transform data easily, using built-in or custom functions.\n\n5.10.1 Applying Functions to Series\nThe apply() function applies a function along an axis of the DataFrame.\n# Creating a sample DataFrame\ndf = pd.DataFrame({\n    'A': [1, 2, 3],\n    'B': [4, 5, 6]\n})\n\n# Applying a function to each column\ndf_squared = df.apply(lambda x: x ** 2)\nprint(df_squared)\n\n# Output:\n#    A   B\n# 0  1  16\n# 1  4  25\n# 2  9  36\n\n# Applying a function to each column\n# def square(x):\n#     y = x ** 2\n#     return y\n\n# df_squared = df.apply(square)\n\n\n5.10.2 Applying Functions to Entire DataFrames\nThe applymap() function applies a function to each element of the DataFrame.\n# Applying a function to each element\ndf_negated = df.applymap(lambda x: -x)\nprint(df_negated)\n\n# Output:\n#    A  B\n# 0 -1 -4\n# 1 -2 -5\n# 2 -3 -6",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Analysis in Python</span>"
    ]
  },
  {
    "objectID": "06.arithmetic.html",
    "href": "06.arithmetic.html",
    "title": "6  Basic Arithmetic Operations in Python",
    "section": "",
    "text": "6.1 Simple Examples with Numbers",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Basic Arithmetic Operations in Python</span>"
    ]
  },
  {
    "objectID": "06.arithmetic.html#simple-examples-with-numbers",
    "href": "06.arithmetic.html#simple-examples-with-numbers",
    "title": "6  Basic Arithmetic Operations in Python",
    "section": "",
    "text": "6.1.1 Addition (+)\nThe addition operator adds two numbers together.\na = 10\nb = 5\nresult = a + b\nprint(result)  # Output: 15\n\n\n6.1.2 Subtraction (-)\nThe subtraction operator subtracts the second number from the first.\na = 10\nb = 5\nresult = a - b\nprint(result)  # Output: 5\n\n\n6.1.3 Multiplication (*)\nThe multiplication operator multiplies two numbers.\na = 10\nb = 5\nresult = a * b\nprint(result)  # Output: 50\n\n\n6.1.4 Division (/)\nThe division operator divides the first number by the second.\na = 10\nb = 5\nresult = a / b\nprint(result)  # Output: 2.0\n\n\n6.1.5 Modulus (%)\nThe modulus operator returns the remainder of the division of the first number by the second.\na = 10\nb = 3\nresult = a % b\nprint(result)  # Output: 1\n\n6.1.5.1 Example Explanation\nIn this example, 10 % 3 computes the remainder when 10 is divided by 3, which is 1.\n\n\n\n6.1.6 Exponentiation (**)\nThe exponentiation operator raises the first number to the power of the second.\na = 2\nb = 3\nresult = a ** b\nprint(result)  # Output: 8\n\n\n6.1.7 Floor Division (//)\nThe floor division operator divides and returns the largest integer less than or equal to the result.\na = 10\nb = 3\nresult = a // b\nprint(result)  # Output: 3\n\n\n6.1.8 divmod() Function\nThe divmod() function returns a tuple containing the quotient and the remainder when dividing two numbers.\na = 10\nb = 3\nquotient, remainder = divmod(a, b)\nprint(f\"Quotient: {quotient}, Remainder: {remainder}\")\n# Output: Quotient: 3, Remainder: 1\n\n6.1.8.1 Example Explanation\nHere, divmod(10, 3) returns (3, 1), where 3 is the quotient and 1 is the remainder when 10 is divided by 3.\n\n\n\n6.1.9 Summary Table\n\n\n\n\n\n\n\n\n\nOperator/Function\nDescription\nExample\nOutput\n\n\n\n\n+\nAddition\n10 + 5\n15\n\n\n-\nSubtraction\n10 - 5\n5\n\n\n*\nMultiplication\n10 * 5\n50\n\n\n/\nDivision\n10 / 5\n2.0\n\n\n%\nModulus (Remainder)\n10 % 3\n1\n\n\n**\nExponentiation\n2 ** 3\n8\n\n\n//\nFloor Division\n10 // 3\n3\n\n\ndivmod()\nQuotient and Remainder as a Tuple\ndivmod(10, 3)\n(3, 1)",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Basic Arithmetic Operations in Python</span>"
    ]
  },
  {
    "objectID": "06.arithmetic.html#advanced-examples-with-dataframes",
    "href": "06.arithmetic.html#advanced-examples-with-dataframes",
    "title": "6  Basic Arithmetic Operations in Python",
    "section": "6.2 Advanced Examples with DataFrames",
    "text": "6.2 Advanced Examples with DataFrames\nIn data analysis, we often work with tabular data. The pandas library provides a powerful DataFrame object to handle such data. Arithmetic operations can be applied to DataFrame objects directly, enabling efficient data manipulation.\n\n6.2.1 Setting Up the Environment\nFirst, install and import the necessary library:\n# Install pandas if not already installed\n# !pip install pandas\n\nimport pandas as pd\n\n\n6.2.2 Creating a DataFrame\nLet’s create a simple DataFrame for demonstration:\ndata = {\n    'A': [10, 20, 30],\n    'B': [1, 2, 3],\n    'C': [5, 4, 3]\n}\ndf = pd.DataFrame(data)\nprint(df)\nOutput:\n    A  B  C\n0  10  1  5\n1  20  2  4\n2  30  3  3\n\n\n6.2.3 Adding a Constant to a Column\nYou can add a constant value to an entire column:\ndf['A_plus_10'] = df['A'] + 10\nprint(df)\nOutput:\n    A  B  C  A_plus_10\n0  10  1  5         20\n1  20  2  4         30\n2  30  3  3         40\n\n\n6.2.4 Column-Wise Operations\nPerform arithmetic operations between columns:\ndf['A_minus_C'] = df['A'] - df['C']\nprint(df)\nOutput:\n    A  B  C  A_plus_10  A_minus_C\n0  10  1  5         20          5\n1  20  2  4         30         16\n2  30  3  3         40         27\n\n\n6.2.5 Using Modulus Operator with DataFrames\nApply the modulus operator to a DataFrame column to get the remainder of division element-wise:\ndf['A_mod_3'] = df['A'] % 3\nprint(df)\nOutput:\n    A  B  C  A_plus_10  A_minus_C  A_mod_3\n0  10  1  5         20          5        1\n1  20  2  4         30         16        2\n2  30  3  3         40         27        0\n\n6.2.5.1 Example Explanation\n\nFor row 0: 10 % 3 equals 1.\nFor row 1: 20 % 3 equals 2.\nFor row 2: 30 % 3 equals 0.\n\n\n\n\n6.2.6 Using divmod() with DataFrames\nWhile divmod() isn’t directly applicable to DataFrame columns, you can achieve similar results using apply along with a lambda function:\ndf[['Quotient', 'Remainder']] = df['A'].apply(lambda x: pd.Series(divmod(x, 3)))\nprint(df)\nOutput:\n    A  B  C  A_plus_10  A_minus_C  A_mod_3  Quotient  Remainder\n0  10  1  5         20          5        1         3          1\n1  20  2  4         30         16        2         6          2\n2  30  3  3         40         27        0        10          0\n\n6.2.6.1 Example Explanation\n\nFor row 0: divmod(10, 3) returns (3, 1).\nFor row 1: divmod(20, 3) returns (6, 2).\nFor row 2: divmod(30, 3) returns (10, 0).\n\n\n\n\n6.2.7 Row-Wise Operations\nUse the sum function to perform operations across rows:\ndf['Sum_A_B_C'] = df[['A', 'B', 'C']].sum(axis=1)\nprint(df)\nOutput:\n    A  B  C  A_plus_10  A_minus_C  A_mod_3  Quotient  Remainder  Sum_A_B_C\n0  10  1  5         20          5        1         3          1         16\n1  20  2  4         30         16        2         6          2         26\n2  30  3  3         40         27        0        10          0         36\n\n\n6.2.8 Applying Functions\nApply a function to a column:\ndf['A_squared'] = df['A'].apply(lambda x: x ** 2)\nprint(df)\nOutput:\n    A  B  C  A_plus_10  A_minus_C  A_mod_3  Quotient  Remainder  Sum_A_B_C  A_squared\n0  10  1  5         20          5        1         3          1         16        100\n1  20  2  4         30         16        2         6          2         26        400\n2  30  3  3         40         27        0        10          0         36        900\n\n\n6.2.9 Conditional Operations\nUse conditions to modify data:\ndf['A_gt_15'] = df['A'] &gt; 15\nprint(df)\nOutput:\n    A  B  C  A_plus_10  A_minus_C  A_mod_3  Quotient  Remainder  Sum_A_B_C  A_squared  A_gt_15\n0  10  1  5         20          5        1         3          1         16        100    False\n1  20  2  4         30         16        2         6          2         26        400     True\n2  30  3  3         40         27        0        10          0         36        900     True\n\n\n6.2.10 Handling Missing Data\nArithmetic operations handle missing data (NaN) gracefully:\ndf.loc[1, 'B'] = None  # Introduce a NaN value\ndf['B_plus_2'] = df['B'] + 2\nprint(df)\nOutput:\n    A    B  C  A_plus_10  A_minus_C  A_mod_3  Quotient  Remainder  Sum_A_B_C  A_squared  A_gt_15  B_plus_2\n0  10  1.0  5         20          5        1         3          1       16.0        100    False       3.0\n1  20  NaN  4         30         16        2         6          2        NaN        400     True       NaN\n2  30  3.0  3         40         27        0        10          0       36.0        900     True       5.0\nThis code snippet demonstrates how to introduce a missing value (NaN) into a DataFrame and perform an arithmetic operation on a column that contains NaN values. Let me break down the key points:\n\n\n6.2.11 Explanation of Code:\n\nIntroducing a Missing Value:\ndf.loc[1, 'B'] = None  # Introduce a NaN value\nThis line sets the value at row index 1 in column 'B' to None, which pandas treats as NaN (Not a Number). This introduces a missing value in the DataFrame.\nPerforming Arithmetic with NaN:\ndf['B_plus_2'] = df['B'] + 2\nThis line creates a new column, 'B_plus_2', by adding 2 to the values in column 'B'. When you perform arithmetic with NaN in pandas, the result for that specific operation is also NaN. This is why in the resulting DataFrame, the row with NaN in column 'B' also has NaN in the new column 'B_plus_2'.\n\n\n\n6.2.12 Grouped Operations\nPerform operations on grouped data:\ndf['Group'] = ['X', 'Y', 'X']\n\nprint(df)\nprint('---')\n\ngrouped = df.groupby('Group')\nsum_per_group = grouped['A'].sum()\nprint(sum_per_group)\nOutput:\nGroup\nX    40\nY    20\nName: A, dtype: int64",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Basic Arithmetic Operations in Python</span>"
    ]
  },
  {
    "objectID": "07.Regular_Expression.html",
    "href": "07.Regular_Expression.html",
    "title": "7  Regular Expressions (RegEx) in Python",
    "section": "",
    "text": "7.1 Basics of Regular Expressions\nHere are some basic symbols in regular expressions:",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Regular Expressions (RegEx) in Python</span>"
    ]
  },
  {
    "objectID": "07.Regular_Expression.html#basics-of-regular-expressions",
    "href": "07.Regular_Expression.html#basics-of-regular-expressions",
    "title": "7  Regular Expressions (RegEx) in Python",
    "section": "",
    "text": ". : Matches any single character except newline.\n^ : Asserts the start of a string.\n$ : Asserts the end of a string.\n[]: Defines a set of characters. For example, [abc] matches either “a”, “b”, or “c”.\n\\d: Matches any digit (equivalent to [0-9]).\n\\w: Matches any word character (alphanumeric and underscore, equivalent to [a-zA-Z0-9_]).\n\\s: Matches any whitespace character (spaces, tabs, etc.).\n*: Matches 0 or more occurrences of the preceding element.\n+: Matches 1 or more occurrences.\n?: Matches 0 or 1 occurrence.\n{n,m}: Matches between n and m occurrences of the preceding element.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Regular Expressions (RegEx) in Python</span>"
    ]
  },
  {
    "objectID": "07.Regular_Expression.html#findall-finding-all-matches",
    "href": "07.Regular_Expression.html#findall-finding-all-matches",
    "title": "7  Regular Expressions (RegEx) in Python",
    "section": "7.2 findall(): Finding All Matches",
    "text": "7.2 findall(): Finding All Matches\nThe findall() function in Python returns a list of all matches for a given pattern.\nimport re\n\ndata = \"1234 abc가나다ABC_555_6 13 43435 2213433577869 23ab\"\n\n# Finding single digits\nprint(re.findall(\"[0-9]\", data))  # ['1', '2', '3', '4', '5', '5', '5', ...]\n\n# Finding sequences of digits\nprint(re.findall(\"[0-9]+\", data))  # ['1234', '555', '6', '13', '43435', ...]\n\n# Finding exactly two-digit numbers\nprint(re.findall(\"[0-9]{2}\", data))  # ['12', '34', '55', '13', '43', ...]\n\n# Finding numbers with 2 to 6 digits\nprint(re.findall(\"[0-9]{2,6}\", data))  # ['1234', '555', '13', '43435', ...]",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Regular Expressions (RegEx) in Python</span>"
    ]
  },
  {
    "objectID": "07.Regular_Expression.html#matching-specific-patterns",
    "href": "07.Regular_Expression.html#matching-specific-patterns",
    "title": "7  Regular Expressions (RegEx) in Python",
    "section": "7.3 Matching Specific Patterns",
    "text": "7.3 Matching Specific Patterns\ndata = \"1234 abc가나다ABC_555_6 mbc kbs sbs 58672 newsline kkbc dreamair air airline air\"\n\n# Finding patterns that start with 'a' followed by two characters\nprint(re.findall(\"a..\", data))  # ['abc', 'air', 'air']\n\n# Finding strings that end with \"air\"\nprint(re.findall(\"air$\", data))  # ['air']",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Regular Expressions (RegEx) in Python</span>"
    ]
  },
  {
    "objectID": "07.Regular_Expression.html#finding-numeric-patterns",
    "href": "07.Regular_Expression.html#finding-numeric-patterns",
    "title": "7  Regular Expressions (RegEx) in Python",
    "section": "7.4 Finding Numeric Patterns",
    "text": "7.4 Finding Numeric Patterns\n\\d is used to represent any digit, and it is one of the most commonly used patterns.\ndata = \"johnson 80, Bong 100, David 50\"\nprint(re.findall(\"\\d\", data))  # ['8', '0', '1', '0', '0', '5', '0']\nprint(re.findall(\"\\d{2}\", data))  # ['80', '10', '00', '50']",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Regular Expressions (RegEx) in Python</span>"
    ]
  },
  {
    "objectID": "07.Regular_Expression.html#split-splitting-strings",
    "href": "07.Regular_Expression.html#split-splitting-strings",
    "title": "7  Regular Expressions (RegEx) in Python",
    "section": "7.5 split(): Splitting Strings",
    "text": "7.5 split(): Splitting Strings\nRegular expressions can be used to split strings based on various patterns.\n# Splitting based on space (default)\nprint(\"mbc,kbs sbs:ytn\".split())  # ['mbc,kbs', 'sbs:ytn']\n\n# Splitting based on commas\nprint(\"mbc,kbs sbs:ytn\".split(\",\"))  # ['mbc', 'kbs sbs:ytn']\n\n# Splitting using a regex pattern (\\W means non-alphanumeric characters)\nprint(re.split(\"\\W+\", \"mbc,kbs sbs:ytn\"))  # ['mbc', 'kbs', 'sbs', 'ytn']",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Regular Expressions (RegEx) in Python</span>"
    ]
  },
  {
    "objectID": "07.Regular_Expression.html#sub-substituting-strings",
    "href": "07.Regular_Expression.html#sub-substituting-strings",
    "title": "7  Regular Expressions (RegEx) in Python",
    "section": "7.6 sub(): Substituting Strings",
    "text": "7.6 sub(): Substituting Strings\nThe sub() function is used to replace parts of a string that match a pattern with another string.\nnumber = \"1234 abc가나다ABC_567_34234\"\nprint(number)  # Original string\n\n# Replacing any digits with '888'\nm = re.sub(\"[0-9]+\", \"888\", number)\nprint(m)  # '888 abc가나다ABC_888_888'",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Regular Expressions (RegEx) in Python</span>"
    ]
  },
  {
    "objectID": "07.Regular_Expression.html#text-preprocessing-examples",
    "href": "07.Regular_Expression.html#text-preprocessing-examples",
    "title": "7  Regular Expressions (RegEx) in Python",
    "section": "7.7 Text Preprocessing Examples",
    "text": "7.7 Text Preprocessing Examples\nThank you for sharing the image of your Excel data. To help you use this as an example in your regular expression textbook, we can focus on several key aspects of preprocessing this text. Here are some steps we could cover:\n\n7.7.1 Removing Special Characters\nThe text contains placeholders like “@@@” and non-standard line breaks, which can be removed or standardized. For instance:\n\nRegex Pattern: @@@|\\n\n\nThis pattern will match both the placeholder and newlines.\n\n\nimport re\n\ntext = \"Hello. I'm @@@.\\nToday I brought that bear doll.\\nThis is a bear doll that I like...\\n thank you. Thank you all.\"\nclean_text = re.sub(r\"@@@|\\n\", \"\", text)\nprint(clean_text)\n\n\n7.7.2 Lowercasing Text\nUniformity in text processing often requires converting all text to lowercase.\n\nRegex or Method: .lower()\n\nlower_text = clean_text.lower()\n\n\n7.7.3 Correcting Simple Typos\nThe text contains phrases like “thank you.” where punctuation might need correction. We can use regex to identify incorrect casing or punctuation.\n\nRegex Pattern: [Tt]hank you[.,]\n\nfixed_text = re.sub(r\"[Tt]hank you[.,]?\", \"Thank you.\", lower_text)\n\n\n7.7.4 Identifying Sentences or Key Phrases\nIf you want to extract specific phrases or sentences for further analysis:\n\nRegex Pattern: r\"\\b(?:brought|present|special)\\b\"\n\nmatches = re.findall(r\"\\b(?:brought|present|special)\\b\", fixed_text)\nprint(matches)  # Extracts words like 'brought', 'present', 'special'\n\n\n7.7.5 Sentence Tokenization\nYou can split sentences based on punctuation using regex:\n\nRegex Pattern: r\"[.!?]\\s+\"\n\nsentences = re.split(r\"[.!?]\\s+\", fixed_text)",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Regular Expressions (RegEx) in Python</span>"
    ]
  },
  {
    "objectID": "07.Regular_Expression.html#apply-regular-expression-in-dataframe",
    "href": "07.Regular_Expression.html#apply-regular-expression-in-dataframe",
    "title": "7  Regular Expressions (RegEx) in Python",
    "section": "7.8 Apply Regular Expression in DataFrame",
    "text": "7.8 Apply Regular Expression in DataFrame\n\n7.8.1 Step 1: Create a DataFrame\nFirst, we create a DataFrame from the sample data that was visible in the image.\nimport pandas as pd\n\n# Sample data based on the image\ndata = {\n    'NO': [1, 2, 3],\n    'Text_Typo': [\n        \"Hello. I'm @@@.\\nToday I brought that bear doll.\\nThis is a bear doll that I like.\\nThis bear doll is special to me because my aunt gave it to me.\\nI remember that it was my birthday present from my aunt.\\nI love my aunt.\\nThank you for listening to my presentation.\",\n        \"Hello, I'm @@@. Today I brought a baseball bat. This is a bat that makes me funny. This is special to me because I wanted to be a baseball player when I was a child. And I was a baseball player in baseball club. I remember that I played baseball with my friend. I was very happy at that time. Thank you.\",\n        \"Hello, I'm @@@. Let me show you this music box. It's special to me because it's my first music box. I was interested in the music.\"\n    ]\n}\n\n# Create DataFrame\ndf = pd.DataFrame(data)\nprint(df)\n\n\n7.8.2 Step 2: Apply Regular Expressions\nLet’s apply some regular expressions to clean up the text.\n\nRemove the placeholder “@@@” and extra line breaks \\n.\nLowercase the text for uniformity.\nReplace certain common typos like missing or incorrect punctuation around “thank you”.\nExtract key sentences using regular expressions.\n\n\n7.8.2.1 Applying Regular Expression to Each Row\nimport re\n\n# Function to clean text\ndef clean_text(text):\n    # Remove the placeholder and line breaks\n    text = re.sub(r\"@@@|\\n\", \"\", text)\n    \n    # Fix common typo with \"thank you\"\n    text = re.sub(r\"[Tt]hank you[.,]?\", \"Thank you.\", text)\n    \n    # Convert to lowercase\n    text = text.lower()\n    \n    return text\n\n# Apply the function to each row in the 'Text_Typo' column\ndf['Cleaned_Text'] = df['Text_Typo'].apply(clean_text)\n\n# Display the updated DataFrame\ndf[['NO', 'Text_Typo','Cleaned_Text']]\n\n\n\n7.8.3 Step 3: Extracting Specific Patterns\nIf you want to extract certain patterns like sentences containing the word “special” or phrases where the speaker brought an item, you can do this with regex as well.\n\n7.8.3.1 Extract Sentences with the Word “Special”\n# Function to extract sentences with the word \"special\"\ndef extract_special_sentences(text):\n    # Split sentences based on punctuation\n    sentences = re.split(r\"[.!?]\\s+\", text)\n    \n    # Find sentences containing the word \"special\"\n    special_sentences = [sent for sent in sentences if re.search(r\"\\bspecial\\b\", sent)]\n\n    ## FULL CODE ##\n    # special_sentences = []\n    # for sent in sentences:\n    #     if re.search(r\"\\bspecial\\b\", sent):\n    #         special_sentences.append(sent)    \n    \n    return special_sentences\n\n# Apply the function to extract sentences with \"special\"\ndf['Special_Sentences'] = df['Cleaned_Text'].apply(extract_special_sentences)\n\n# Display the DataFrame with extracted sentences\ndf[['NO', 'Text_Typo','Cleaned_Text', 'Special_Sentences']]\nThis DataFrame now contains cleaned text and a column with sentences that mention the word “special.”",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Regular Expressions (RegEx) in Python</span>"
    ]
  },
  {
    "objectID": "07.Regular_Expression.html#regular-expression-practice-website",
    "href": "07.Regular_Expression.html#regular-expression-practice-website",
    "title": "7  Regular Expressions (RegEx) in Python",
    "section": "7.9 Regular Expression Practice Website",
    "text": "7.9 Regular Expression Practice Website\nRegexr.com is an interactive website for practicing and learning regular expressions (regex). It offers tools to create and test regex patterns, along with explanations, a pattern library, and a reference guide. The site is suitable for both beginners and advanced users, providing real-time feedback and community-contributed examples to enhance learning.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Regular Expressions (RegEx) in Python</span>"
    ]
  },
  {
    "objectID": "08.Custom_Corpus.html",
    "href": "08.Custom_Corpus.html",
    "title": "8  Building a Custom Corpus",
    "section": "",
    "text": "8.1 Procedure",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Building a Custom Corpus</span>"
    ]
  },
  {
    "objectID": "08.Custom_Corpus.html#procedure",
    "href": "08.Custom_Corpus.html#procedure",
    "title": "8  Building a Custom Corpus",
    "section": "",
    "text": "8.1.1 Define the Purpose\nBefore starting, clearly define the purpose of your corpus. For instance:\n\nLanguage Analysis: Are you studying phonetics, syntax, or semantics?\nMachine Learning/NLP: Do you need a training dataset for sentiment analysis, text classification, or language modeling?\nDomain-specific Corpus: Are you focusing on medical, legal, academic, or informal conversational language?\n\n\n\n8.1.2 Decide on Data Sources\nDetermine where to source the data from:\n\nWeb scraping: Use tools like BeautifulSoup or Scrapy to extract text from websites.\nBooks, Papers, Articles: Digitize and preprocess text from publications.\nSocial Media/Forums: Use APIs like Twitter, Reddit, or specific domain forums.\nTranscriptions: If you need spoken language, consider using speech-to-text services to create transcripts.\nExisting Corpora: You can augment your corpus with publicly available ones like the Brown Corpus, Gutenberg Project, or specialized databases.\n\n\n\n8.1.3 Collect and Store the Data\n\nWeb Scraping/Downloading: Use web crawlers, APIs, or manual downloads to gather the raw text data. Make sure to follow ethical guidelines, including copyright considerations.\nDatabase/Spreadsheet: Store collected texts in an organized format (e.g., CSV, JSON, or a relational database).\nTagging Metadata: Include useful metadata (e.g., author, publication date, text genre, and source) to allow for targeted corpus queries.\n\n\n\n8.1.4 Data Cleaning & Preprocessing\nRaw text data needs to be cleaned and prepared for analysis:\n\nTokenization: Break the text into sentences and words.\nLowercasing: Convert all text to lowercase (if case sensitivity isn’t required).\nRemove Stopwords/Punctuation: You may want to exclude common words (like ‘and’, ‘the’) or punctuation, depending on the goal.\nNormalization: Convert numbers, dates, and other elements into consistent formats.\nHandling Special Characters: Replace or remove non-ASCII characters, HTML tags, or other artifacts from the data source.\nLemmatization/Stemming: Reduce words to their base forms for consistency (e.g., running -&gt; run).\n\n\n\n8.1.5 Data Annotation (Optional)\n\nIf you need a labeled dataset, annotate the text with additional information, such as:\nPOS Tags: Parts of Speech tagging for grammatical structure.\nNamed Entities: Tag entities like persons, locations, organizations.\nSentiment or Categories: Manually label texts for sentiment, emotions, or specific classification tasks.\nYou can use annotation tools like Brat, Prodigy, or Label Studio.\n\n\n\n8.1.6 Corpus Formatting\nAfter cleaning and processing the data, store it in a structured format:\n\nPlain Text: Simple text files (one document per file).\nXML/JSON: Structured formats with metadata tags and hierarchy.\nCSV/TSV: Useful if you need to store data with multiple fields (e.g., sentences, labels, tags).\nSpecialized Formats: If the corpus is for NLP tasks, use formats like CoNLL or CSV files with token annotations.\n\n\n\n8.1.7 Corpus Size & Diversity\n\nDetermine the size of your corpus: Depending on the task, your corpus may need to be large (for language models) or small but highly focused (for linguistic studies).\nEnsure diversity: Balance the corpus with data from various genres, demographics, or domains, as needed.\n\n\n\n8.1.8 Corpus Tools (Optional)\n\nIf you need tools to manage or explore your corpus:\nAntConc: A free tool for analyzing text corpora (concordance, collocation, etc.).\nNLTK, spaCy: Python libraries for processing and analyzing corpora, which also provide tools for tokenization, tagging, and more.\n\n\n\n8.1.9 Export/Share the Corpus\n\nOnce built, you can export your corpus for analysis or use in machine learning models.\nIf sharing, consider adding documentation and metadata about its sources, structure, and purpose.\n\n\n\n8.1.10 Example: Steps for Building a Custom Sentiment Analysis Corpus\n\nDefine Purpose: Create a sentiment analysis corpus.\nCollect Data: Scrape reviews from a website (e.g., Amazon reviews).\nPreprocess: Remove HTML tags, stopwords, punctuation, and tokenize sentences.\nAnnotate: Manually label each review with positive, negative, or neutral sentiment.\nStore: Save the corpus in a CSV format with columns: “Review Text”, “Sentiment”.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Building a Custom Corpus</span>"
    ]
  },
  {
    "objectID": "08.Custom_Corpus.html#web-scraping",
    "href": "08.Custom_Corpus.html#web-scraping",
    "title": "8  Building a Custom Corpus",
    "section": "8.2 Web Scraping",
    "text": "8.2 Web Scraping\nWeb scraping is the process of extracting data from websites. In Python, there are several popular libraries used for this purpose, including BeautifulSoup, requests, and Scrapy. Below is an overview of the steps involved in web scraping, followed by a Python code example using BeautifulSoup and requests.\n\n8.2.1 Steps for Web Scraping\n\nInstall Required Libraries: You will need to install the following libraries:\n\nrequests: To send HTTP requests to a web page and get the HTML content.\nBeautifulSoup (from the bs4 package): To parse the HTML and extract the data.\n(Optional) pandas: To store the extracted data in a structured format like a DataFrame.\n\nInstall them via pip:\npip install requests beautifulsoup4 pandas\nUnderstand the Website Structure:\n\nOpen the website you want to scrape.\nUse your browser’s developer tools (F12 or right-click → Inspect) to inspect the HTML structure and identify the elements containing the data you want to scrape (e.g., headings, paragraphs, tables, etc.).\n\nWrite the Web Scraping Script:\n\nUse requests to fetch the HTML content of the page.\nParse the HTML using BeautifulSoup and extract the required data.\nOptionally, save the data in a CSV file or a DataFrame.\n\n\n\n\n8.2.2 Example: Scraping Job Listings from a Webpage\nTo scrape data from all pages of the Books to Scrape website, including subsequent pages, you’ll need to set up a loop that continues until it reaches the last page. Each page’s URL follows the pattern https://books.toscrape.com/catalogue/page-X.html, where X is the page number.\nThe key is to keep requesting new pages and checking if a next page exists by inspecting the “Next” button. If it exists, you continue to the next page; otherwise, the loop stops.\nHere’s the modified Python code that automatically handles pagination until the end of the list:\n\n\n8.2.3 Python Code to Scrape All Pages with Pagination:\nimport requests\nfrom bs4 import BeautifulSoup\nimport pandas as pd\n\n# Create lists to store all extracted data\nall_titles = []\nall_prices = []\nall_availability = []\nall_links = []\nall_descriptions = []\n\n# Base URL for books to scrape\nbase_url = \"https://books.toscrape.com/catalogue/\"\npage_url = \"https://books.toscrape.com/catalogue/page-1.html\"\n\n# Loop through each page until there is no next page\nwhile True:\n    # Send a GET request to the current page\n    response = requests.get(page_url, timeout=10)\n\n    # Check if the request was successful\n    if response.status_code == 200:\n        soup = BeautifulSoup(response.content, 'html.parser')\n\n        # Find all book entries on the page\n        books = soup.find_all('article', class_='product_pod')\n\n        for book in books:\n            title = book.h3.a['title']\n            all_titles.append(title)\n\n            price = book.find('p', class_='price_color').text.strip()\n            all_prices.append(price)\n\n            availability = book.find('p', class_='instock availability').text.strip()\n            all_availability.append(availability)\n\n            relative_link = book.h3.a['href']\n            full_link = base_url + relative_link.replace('../../', '')\n            all_links.append(full_link)\n\n            # Now scrape the product description from the book's page\n            try:\n                book_response = requests.get(full_link, timeout=10)\n                if book_response.status_code == 200:\n                    book_soup = BeautifulSoup(book_response.content, 'html.parser')\n                    # Scrape the product description\n                    description_element = book_soup.select_one('#content_inner &gt; article &gt; p')\n                    if description_element:\n                        description = description_element.text.strip()\n                    else:\n                        description = \"No description available.\"\n                else:\n                    description = \"Failed to retrieve description.\"\n            except Exception as e:\n                description = f\"Error: {str(e)}\"\n\n            all_descriptions.append(description)\n\n        # Check if there's a next page by looking for the \"next\" button\n        next_button = soup.select_one('li.next &gt; a')\n        if next_button:\n            next_page_relative_url = next_button['href']\n            page_url = base_url + next_page_relative_url  # Construct the URL for the next page\n        else:\n            break  # No more pages, exit the loop\n    else:\n        print(f\"Failed to retrieve page. Status code: {response.status_code}\")\n        break\n\n# Save all the data to a DataFrame and CSV file\nbooks_df = pd.DataFrame({\n    'Title': all_titles,\n    'Price': all_prices,\n    'Availability': all_availability,\n    'Link': all_links,\n    'Product Description': all_descriptions\n})\n\nbooks_df.to_csv('books_scraped_with_descriptions_all_pages.csv', index=False)\n\n# Print the extracted data\nprint(books_df)\n\n\n8.2.4 Explanation:\n\nPagination Handling:\n\nThe loop starts from the first page (page-1.html) and checks for the presence of a “Next” button (li.next &gt; a) at the bottom of the page.\nIf the “Next” button exists, the script extracts the URL for the next page and continues the loop. If not, the loop ends.\n\nExtracting Book Information:\n\nFor each book on a page, the script extracts the title, price, availability, and the link to the book’s detail page.\nIt then sends a separate request for each book’s detail page and scrapes the product description from the section (e.g., #content_inner &gt; article &gt; p).\n\n\n\n\n\n\n\n\nFig. 1. How to copy “selector” address (F12 key + Selector)\n\nExiting the Loop:\n\nThe loop breaks when there is no “Next” button on the page, meaning it has reached the last page of the pagination.\n\nSaving Data:\n\nThe extracted data (titles, prices, availability, links, and product descriptions) is stored in a Pandas DataFrame and saved as a CSV file (books_scraped_with_descriptions_all_pages.csv).\n\n\n\n\n8.2.5 Expected Output:\nThe script will scrape all available pages of books and store the data in a CSV file with columns for the book title, price, availability, link, and product description.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Building a Custom Corpus</span>"
    ]
  },
  {
    "objectID": "08.Custom_Corpus.html#youtube-subtitle-extraction",
    "href": "08.Custom_Corpus.html#youtube-subtitle-extraction",
    "title": "8  Building a Custom Corpus",
    "section": "8.3 YouTube Subtitle Extraction",
    "text": "8.3 YouTube Subtitle Extraction\nYou can extract subtitles (also called closed captions) from YouTube videos using the YouTube Data API or by directly downloading the subtitles if they are available. Here’s a guide on how to do both using Python.\n\n8.3.1 Method 1: Using YouTube Transcript API (Simpler Approach)\nA Python package called youtube-transcript-api provides an easy way to extract subtitles from YouTube videos without needing an API key.\n\n8.3.1.1 Step 1: Install youtube-transcript-api\nYou can install this package using pip:\npip install youtube-transcript-api\n\n\n8.3.1.2 Step 2: Code Example to Extract Subtitles\nfrom youtube_transcript_api import YouTubeTranscriptApi\nimport csv\n\n# Replace with your YouTube video ID\nvideo_id = 'YOUR_VIDEO_ID'\n\n# Fetch the transcript\ntry:\n    transcript = YouTubeTranscriptApi.get_transcript(video_id)\n    \n    # Prepare to save the transcript to a .txt file\n    with open('transcript.txt', 'w', encoding='utf-8') as txt_file:\n        for entry in transcript:\n            start_time = entry['start']\n            duration = entry['duration']\n            text = entry['text']\n            # Writing to txt file\n            txt_file.write(f\"Start: {start_time} seconds, Duration: {duration} seconds\\nText: {text}\\n\\n\")\n    \n    # Prepare to save the transcript to a .csv file\n    with open('transcript.csv', 'w', newline='', encoding='utf-8') as csv_file:\n        writer = csv.writer(csv_file)\n        # Writing the header\n        writer.writerow(['Start Time (seconds)', 'Duration (seconds)', 'Text'])\n        \n        # Writing each transcript entry\n        for entry in transcript:\n            writer.writerow([entry['start'], entry['duration'], entry['text']])\n    \n    print(\"Transcript saved as 'transcript.txt' and 'transcript.csv'\")\n    \nexcept Exception as e:\n    print(f\"Error: {e}\")\n\n\n8.3.1.3 Example:\nIf the video URL is https://www.youtube.com/watch?v=abcd1234, then the video_id would be abcd1234.\nThis will return a list of subtitles with their start times and duration for the video. The subtitles must be available for the video (either auto-generated or uploaded by the video creator).\n\n\n8.3.1.4 Explanation:\n\nText File (transcript.txt):\n\nWe open a text file (transcript.txt) in write mode.\nFor each subtitle entry, we format and write the start time, duration, and the text of the subtitle into the file.\n\nCSV File (transcript.csv):\n\nWe open a CSV file in write mode.\nWe write the header with three columns: Start Time, Duration, and Text.\nWe then write each subtitle entry (start time, duration, and text) as a row in the CSV file.\n\n\n\n\n8.3.1.5 Output:\n\ntranscript.txt: Contains a formatted transcript with start times, durations, and text for each subtitle entry.\ntranscript.csv: Contains the transcript data organized in a table with three columns: Start Time, Duration, and Text.\n\n\n\n\n8.3.2 Method 2: Using YouTube Data API v3\nThe YouTube Data API v3 does not directly provide subtitles but can help you identify videos that have captions, and from there, you can manually fetch the captions if they are available.\n\n8.3.2.1 Step 1: Set Up YouTube Data API\n\nGo to the Google Developers Console.\nCreate a new project and enable the YouTube Data API v3.\nGenerate an API key from the credentials tab.\n\n\n\n\n\n8.3.2.2 Step 2: Install the Required Libraries\npip install google-api-python-client\n\n\n8.3.2.3 Step 3: Code Example to Check for Subtitles Availability\nIf you only want to save the video title and the entire subtitle (combined) as a single row in the CSV file, we can modify the script to:\n\nFetch the title of the video using the YouTube Data API.\nFetch the transcript (subtitles) using the youtube-transcript-api and combine the entire transcript text into one row.\nSave the video title and entire subtitle as a single row in the CSV file.\n\nHere’s the modified code:\n\n\n8.3.2.4 Modified Code:\nfrom googleapiclient.discovery import build\nfrom youtube_transcript_api import YouTubeTranscriptApi\nimport csv\n\n# Replace with your API key\napi_key = 'YOUR_API_KEY'\n\nyoutube = build('youtube', 'v3', developerKey=api_key)\n\n# Replace with the video ID you want to check\nvideo_id = 'YOUR_VIDEO_ID'\n\n# Fetch video details\nresponse = youtube.videos().list(\n    part='snippet,contentDetails',\n    id=video_id\n).execute()\n\n# Check if video exists and fetch the title\nif 'items' in response and response['items']:\n    video_title = response['items'][0]['snippet']['title']  # Get the video title\n    print(f\"Video Title: {video_title}\")\n    \n    # Check if captions are available and fetch the transcript\n    if 'contentDetails' in response['items'][0] and 'caption' in response['items'][0]['contentDetails']:\n        if response['items'][0]['contentDetails']['caption'] == 'true':\n            print(f\"Captions are available for video ID: {video_id}\")\n            \n            # Fetch transcript using youtube-transcript-api\n            try:\n                transcript = YouTubeTranscriptApi.get_transcript(video_id)\n                \n                # Combine all the subtitles into a single string\n                combined_subtitle = \" \".join([entry['text'] for entry in transcript])\n                \n                # Save the video title and combined subtitles to a .csv file\n                with open('transcript.csv', 'w', newline='', encoding='utf-8') as csv_file:\n                    writer = csv.writer(csv_file)\n                    writer.writerow(['Video Title', 'Subtitle'])  # Header row\n                    writer.writerow([video_title, combined_subtitle])  # Data row\n                \n                print(\"Transcript saved as 'transcript.csv'\")\n            \n            except Exception as e:\n                print(f\"Error fetching transcript: {e}\")\n        else:\n            print(f\"No captions available for video ID: {video_id}\")\nelse:\n    print(f\"No video found for video ID: {video_id}\")\n\n\n8.3.2.5 Explanation of Changes:\n\nVideo Title:\n\nWe extract the video title using the YouTube Data API from response['items'][0]['snippet']['title'].\n\nCombining Subtitles:\n\nInstead of saving the start time, duration, and individual subtitle entries, the script now combines all the subtitle texts into a single string using \" \".join([entry['text'] for entry in transcript]).\n\nSaving to CSV:\n\nThe script saves only two fields in the CSV: the video title and the combined subtitles (as one row).\n\n\n\n\n8.3.2.6 Output:\n\ntranscript.csv: Contains the video title and the entire subtitle in one row.\n\n\n\n8.3.2.7 Notes:\n\nThis code assumes that the subtitles are available for the video. If not, it will print an appropriate message.\nThe entire subtitle text is combined into a single string and saved in the CSV file.\n\n\n\n\n8.3.3 Method 3: Retrieve the subtitles of multiple videos from a YouTube playlist\nTo retrieve the subtitles of multiple videos from a YouTube playlist and save the results in a CSV file with each row containing the video title and the subtitles, we can follow these steps:\n\n8.3.3.1 Steps:\n\nUse the YouTube Data API to fetch all the video IDs from the playlist.\nFor each video, fetch the video title and subtitles using youtube-transcript-api.\nSave the video title and combined subtitles for each video in a CSV file.\n\n\n\n8.3.3.2 Notes:\n\nPlaylist ID: You need to replace 'YOUR_PLAYLIST_ID' with the actual YouTube Playlist ID.\nAPI Key: Replace 'YOUR_API_KEY' with your valid YouTube Data API key.\nNo Subtitles: If a video has no subtitles available, it will display \"No subtitles available\" in the CSV file.\n\n\n\n8.3.3.3 Code to Achieve This:\nfrom googleapiclient.discovery import build\nfrom youtube_transcript_api import YouTubeTranscriptApi, NoTranscriptFound, TranscriptsDisabled\nimport csv\n\n# Replace with your YouTube API key\napi_key = 'YOUR_API_KEY'\n\n# Initialize the YouTube API client\nyoutube = build('youtube', 'v3', developerKey=api_key)\n\n# Replace with your Playlist ID\nplaylist_id = 'YOUR_PLAYLIST_ID'\n\n# Function to get all video IDs from a playlist\ndef get_video_ids_from_playlist(playlist_id):\n    video_ids = []\n    next_page_token = None\n\n    while True:\n        # Fetch the playlist items\n        request = youtube.playlistItems().list(\n            part='contentDetails',\n            playlistId=playlist_id,\n            maxResults=50,\n            pageToken=next_page_token\n        )\n        response = request.execute()\n\n        # Extract video IDs\n        for item in response['items']:\n            video_ids.append(item['contentDetails']['videoId'])\n\n        # Check if there's another page of results\n        next_page_token = response.get('nextPageToken')\n        if not next_page_token:\n            break\n\n    return video_ids\n\n# Function to fetch video title and transcript\ndef fetch_video_details_and_transcript(video_id):\n    try:\n        # Fetch video details (title)\n        video_response = youtube.videos().list(\n            part='snippet',\n            id=video_id\n        ).execute()\n\n        if 'items' in video_response and video_response['items']:\n            video_title = video_response['items'][0]['snippet']['title']\n\n            # Fetch transcript using youtube-transcript-api\n            try:\n                transcript = YouTubeTranscriptApi.get_transcript(video_id)\n                combined_subtitle = \" \".join([entry['text'] for entry in transcript])\n\n                return video_title, combined_subtitle\n            except (NoTranscriptFound, TranscriptsDisabled):\n                # Handle cases where subtitles are not available\n                return video_title, \"No subtitles available\"\n        else:\n            return None, None\n    except Exception as e:\n        print(f\"Error fetching details for video ID {video_id}: {e}\")\n        return None, None\n\n# Fetch all video IDs from the playlist\nvideo_ids = get_video_ids_from_playlist(playlist_id)\n\n# Prepare to save the video title and subtitles into a CSV file\nwith open('playlist_transcripts.csv', 'w', newline='', encoding='utf-8') as csv_file:\n    writer = csv.writer(csv_file)\n    writer.writerow(['Video Title', 'Subtitle'])  # Header row\n\n    # Process each video in the playlist\n    for video_id in video_ids:\n        video_title, combined_subtitle = fetch_video_details_and_transcript(video_id)\n        if video_title:\n            # Ensure both title and subtitles are on a single row\n            writer.writerow([video_title, combined_subtitle])  # Write video title and combined subtitle\n\nprint(\"Transcripts for the playlist videos saved to 'playlist_transcripts.csv'\")\n\n\n\n8.3.4 Explanation of the Code:\n\nYouTube Data API:\n\nWe use the YouTube Data API to retrieve all the video IDs in the playlist using youtube.playlistItems().list().\nEach API call can retrieve a maximum of 50 videos per request, and we loop through the playlist if there are more videos (using pagination).\n\nYouTube Transcript API:\n\nFor each video, we fetch its transcript using the youtube-transcript-api package.\nIf subtitles are available, they are concatenated into one string.\nIf no subtitles are found or they are disabled, the message \"No subtitles available\" is saved.\n\nCSV File Output:\n\nThe result is saved in a CSV file with two columns: Video Title and Subtitle.\nEach row corresponds to one video from the playlist, containing the video’s title and its entire subtitle (or a message if subtitles are unavailable).\n\nErrors to be Fixed:\n\n\n\n\n8.3.5 Updated Code:\nTo address the issue, each video’s subtitles are saved as a separate .txt file in a folder called playlist_txt. These .txt files are then imported into a pandas DataFrame and finally exported as an Excel (.xlsx) file. Below is the complete solution:\nimport os\nimport pandas as pd\nfrom googleapiclient.discovery import build\nfrom youtube_transcript_api import YouTubeTranscriptApi, NoTranscriptFound, TranscriptsDisabled\nimport csv\nimport openpyxl\n\n# Replace with your YouTube API key\napi_key = 'YOUR_API_KEY'\n\n# Initialize the YouTube API client\nyoutube = build('youtube', 'v3', developerKey=api_key)\n\n# Replace with your Playlist ID\nplaylist_id = 'YOUR_PLAYLIST_ID'\n\n# Create directory for storing text files\nif not os.path.exists('playlist_txt'):\n    os.makedirs('playlist_txt')\n\n# Function to get all video IDs from a playlist\ndef get_video_ids_from_playlist(playlist_id):\n    video_ids = []\n    next_page_token = None\n\n    while True:\n        # Fetch the playlist items\n        request = youtube.playlistItems().list(\n            part='contentDetails',\n            playlistId=playlist_id,\n            maxResults=50,\n            pageToken=next_page_token\n        )\n        response = request.execute()\n\n        # Extract video IDs\n        for item in response['items']:\n            video_ids.append(item['contentDetails']['videoId'])\n\n        # Check if there's another page of results\n        next_page_token = response.get('nextPageToken')\n        if not next_page_token:\n            break\n\n    return video_ids\n\n# Function to fetch video title and transcript\ndef fetch_video_details_and_transcript(video_id):\n    try:\n        # Fetch video details (title)\n        video_response = youtube.videos().list(\n            part='snippet',\n            id=video_id\n        ).execute()\n\n        if 'items' in video_response and video_response['items']:\n            video_title = video_response['items'][0]['snippet']['title']\n\n            # Fetch transcript using youtube-transcript-api\n            try:\n                transcript = YouTubeTranscriptApi.get_transcript(video_id)\n                # Combine all subtitle text into a single string, ensuring it's properly joined\n                combined_subtitle = \" \".join([entry['text'].strip() for entry in transcript])\n\n                return video_title, combined_subtitle\n            except (NoTranscriptFound, TranscriptsDisabled):\n                # Handle cases where subtitles are not available\n                return video_title, \"No subtitles available\"\n        else:\n            return None, None\n    except Exception as e:\n        print(f\"Error fetching details for video ID {video_id}: {e}\")\n        return None, None\n\n# Function to save transcript as a .txt file\ndef save_transcript_as_txt(video_title, combined_subtitle):\n    sanitized_title = \"\".join([c if c.isalnum() else \"_\" for c in video_title])  # Sanitize the title to be filesystem safe\n    txt_filename = f'playlist_txt/{sanitized_title}.txt'\n    with open(txt_filename, 'w', encoding='utf-8') as txt_file:\n        txt_file.write(f\"Video Title: {video_title}\\n\\n\")\n        txt_file.write(combined_subtitle)\n    return txt_filename\n\n# Fetch all video IDs from the playlist\nvideo_ids = get_video_ids_from_playlist(playlist_id)\n\n# List to store data for importing into pandas\ntranscript_data = []\n\n# Process each video in the playlist and save transcripts as text files\nfor video_id in video_ids:\n    video_title, combined_subtitle = fetch_video_details_and_transcript(video_id)\n    if video_title:\n        # Save each video transcript as a .txt file\n        txt_filename = save_transcript_as_txt(video_title, combined_subtitle)\n        \n        # Append video title and file path for pandas import\n        transcript_data.append({'Video Title': video_title, 'Text File': txt_filename})\n    else:\n        print(f\"Skipping video ID {video_id} due to missing title.\")\n\n# Now import the .txt files into pandas and create an Excel sheet\n\n# List to hold the contents of each txt file\ndf_list = []\n\n# Read each txt file and store its content in the DataFrame\nfor transcript in transcript_data:\n    with open(transcript['Text File'], 'r', encoding='utf-8') as file:\n        text_content = file.read()\n    df_list.append({'Video Title': transcript['Video Title'], 'Transcript': text_content})\n\n# Create a pandas DataFrame\ndf = pd.DataFrame(df_list)\n\n# Export the DataFrame to an Excel file\noutput_excel = 'playlist_transcripts.xlsx'\ndf.to_excel(output_excel, index=False)\n\nprint(f\"Transcripts for the playlist videos saved to 'playlist_transcripts.xlsx'\")\n\n\n8.3.6 Explanation of Changes:\n\nDirectory Creation:\n\nThe folder playlist_txt is created using os.makedirs() if it doesn’t already exist. All .txt files are saved in this directory.\n\nText File Saving:\n\nThe function save_transcript_as_txt saves each video’s transcript as a .txt file with a sanitized file name (to avoid illegal characters).\nThe title of the video and the transcript content are written into the .txt file.\n\nPandas DataFrame Import:\n\nEach .txt file is read into a pandas DataFrame where the video title and transcript content are stored.\n\nExporting to Excel:\n\nThe DataFrame is exported as an Excel file (playlist_transcripts.xlsx) using df.to_excel().\n\n\n\n\n8.3.7 Output:\n\nText Files: A directory called playlist_txt will be created, and each video’s subtitles will be saved as a .txt file within this directory.\nExcel File: The script generates an Excel file (playlist_transcripts.xlsx) containing the video title and the complete transcript for each video.\n\n\n8.3.7.1 Example Excel Output:",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Building a Custom Corpus</span>"
    ]
  },
  {
    "objectID": "09.Preprocessing.html",
    "href": "09.Preprocessing.html",
    "title": "9  Text Preprocessing",
    "section": "",
    "text": "9.1 Key Steps in Text Preprocessing",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Text Preprocessing</span>"
    ]
  },
  {
    "objectID": "09.Preprocessing.html#key-steps-in-text-preprocessing",
    "href": "09.Preprocessing.html#key-steps-in-text-preprocessing",
    "title": "9  Text Preprocessing",
    "section": "",
    "text": "Lowercasing: Converting all text to lowercase ensures uniformity. Without this, words like “Book” and “book” would be treated as different entities, leading to unnecessary complexity in the analysis.\nTokenization: Tokenization is the process of splitting text into individual words or tokens. These tokens are the basic units on which further processing steps are applied.\nRemoving Punctuation: Punctuation marks often do not contribute to the meaning of text for analytical purposes. Stripping punctuation ensures a cleaner input for the model.\nStopword Removal: Stopwords are common words like “the,” “is,” “in,” “of,” etc., that appear frequently but contribute little to the overall meaning of the text. Removing stopwords helps reduce noise and improve the focus on the core content.\nLemmatization (Optional): Lemmatization is the process of reducing words to their base or dictionary form. While this is an optional step, it is useful for ensuring that words like “running” and “run” are treated as the same entity. In this section, however, we omit lemmatization for simplicity.\n\n\n9.1.1 Python Code Example 1\nBelow is a Python code that demonstrates how to preprocess a dataset containing raw text, specifically in the “Product Description” column. This dataset includes information about books, and we aim to clean the text data to prepare it for further analysis.\nThe code performs the following operations: - Loads the CSV file that contains the product descriptions. - Converts the text to lowercase. - Removes punctuation marks. - Tokenizes the text into individual words. - Removes common stopwords. - Saves the processed text back into a new column named “preprocessed_text” and exports the updated CSV file.\nimport pandas as pd\nimport re\n\n# Load the CSV file\nfile_path = '/mnt/data/books_scraped_with_descriptions_all_pages.csv'\ndata = pd.read_csv(file_path)\n\n# Preprocess function (without external libraries)\ndef preprocess_text_no_external(text):\n    # Lowercase the text\n    text = text.lower()\n    \n    # Remove punctuation using regex\n    text = re.sub(r'[^\\w\\s]', '', text)\n    \n    # Tokenize by splitting text\n    tokens = text.split()\n    \n    # Simple list of stopwords\n    stop_words = set(['a', 'the', 'and', 'is', 'in', 'it', 'to', 'this', 'of', 'that'])\n    \n    # Remove stopwords\n    tokens = [word for word in tokens if word not in stop_words]\n    \n    # Join tokens back to string\n    return ' '.join(tokens)\n\n# Apply preprocessing to the \"Product Description\" column\ndata['cleaned_description'] = data['Product Description'].dropna().apply(preprocess_text_no_external)\n\n# Add the \"preprocessed text\" column to the original data\ndata['preprocessed_text'] = data['cleaned_description']\n\n# Save the updated data back to the input CSV file, including the new column\nupdated_file_path = '/mnt/data/books_scraped_with_preprocessed_descriptions.csv'\ndata.to_csv(updated_file_path, index=False)\n\nprint(\"Preprocessing complete! The updated file has been saved with preprocessed text.\")\n\n\n9.1.2 Explanation of the Code 1\n\nData Loading: The CSV file containing book data is loaded into a pandas DataFrame, and the column “Product Description” is selected for preprocessing.\nText Cleaning: The preprocess_text_no_external function handles all preprocessing tasks, including:\n\nLowercasing the text.\nRemoving punctuation using regular expressions.\nTokenization by splitting the text on whitespace.\nStopword Removal by filtering out common English stopwords.\n\nProcessed Output: The cleaned text is stored in a new column called “preprocessed_text” in the DataFrame.\nFile Export: Finally, the updated DataFrame is saved back to a CSV file, which includes both the original product description and the newly preprocessed text.\n\n\n\n9.1.3 Python Code Example 2\nimport pandas as pd\nimport nltk\nimport string\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import WordNetLemmatizer\n\n# Ensure required NLTK resources are downloaded\nnltk.download('punkt')\nnltk.download('stopwords')\nnltk.download('wordnet')\nnltk.download('punkt_tab')\n# Load the CSV file\nfile_path = 'books_scraped_with_descriptions_all_pages.csv'\ndata = pd.read_csv(file_path)\n\n# Select the \"Product Description\" column\ndescriptions = data['Product Description'].dropna()\n\n# Initialize the lemmatizer\nlemmatizer = WordNetLemmatizer()\n\n# Preprocess function\ndef preprocess_text(text):\n    # Lowercase the text\n    text = text.lower()\n    \n    # Tokenize the text\n    tokens = word_tokenize(text)\n    \n    # Remove punctuation\n    tokens = [word for word in tokens if word.isalnum()]\n    \n    # Remove stopwords\n    stop_words = set(stopwords.words('english'))\n    tokens = [word for word in tokens if word not in stop_words]\n    \n    # Lemmatize tokens\n    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n    \n    # Join tokens back to string\n    return ' '.join(tokens)\n\n# Apply preprocessing to the \"Product Description\" column\ndata['cleaned_description'] = descriptions.apply(preprocess_text)\n# Add the \"preprocessed text\" column to the original data\ndata['preprocessed_text'] = data['cleaned_description']\n\n# Save the updated data back to the input CSV file, including the new column\nupdated_file_path = 'books_scraped_with_preprocessed_descriptions.csv'\ndata.to_csv(updated_file_path, index=False)\ndata\n\n\n9.1.4 Explanation of the Code 2\nThe provided code performs text preprocessing on a CSV file that contains book descriptions and saves the cleaned text back to a new CSV file. Let’s break it down step by step:\n\n9.1.4.1 Loading the CSV File\n# Load the CSV file\nfile_path = 'books_scraped_with_descriptions_all_pages.csv'\ndata = pd.read_csv(file_path)\n\nThe pd.read_csv() function loads a CSV file located at file_path into a pandas DataFrame called data. This DataFrame will hold all the data from the file, including the book descriptions.\n\n\n\n9.1.4.2 Selecting the ‘Product Description’ Column\n# Select the \"Product Description\" column\ndescriptions = data['Product Description'].dropna()\n\nThe code extracts the ‘Product Description’ column from the DataFrame using data['Product Description'].\nThe dropna() method removes any rows with missing values (i.e., NaN values) in this column to ensure only valid descriptions are processed.\n\n\n\n9.1.4.3 Initializing the Lemmatizer\n# Initialize the lemmatizer\nlemmatizer = WordNetLemmatizer()\n\nThe WordNetLemmatizer() is initialized here. Lemmatization is the process of reducing words to their base or root form (e.g., “running” → “run”). This helps standardize different forms of the same word.\n\n\n\n9.1.4.4 Defining the Preprocessing Function\n# Preprocess function\ndef preprocess_text(text):\n    # Lowercase the text\n    text = text.lower()\n    \n    # Tokenize the text\n    tokens = word_tokenize(text)\n    \n    # Remove punctuation\n    tokens = [word for word in tokens if word.isalnum()]\n    \n    # Remove stopwords\n    stop_words = set(stopwords.words('english'))\n    tokens = [word for word in tokens if word not in stop_words]\n    \n    # Lemmatize tokens\n    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n    \n    # Join tokens back to string\n    return ' '.join(tokens)\nThis function processes the raw text through the following steps:\n\nLowercasing:\n\nThe text is converted to lowercase using text.lower() to ensure uniformity (e.g., “Book” and “book” are treated the same).\n\nTokenization:\n\nword_tokenize(text) splits the text into individual tokens (words). For example, “This is a book” becomes ['This', 'is', 'a', 'book'].\n\nRemoving Punctuation:\n\nword.isalnum() checks if each word is alphanumeric. Words that contain punctuation marks or symbols are excluded.\n\nRemoving Stopwords:\n\nA set of common stopwords (e.g., “the,” “is,” “and”) is defined using stopwords.words('english'). These words are removed from the tokenized text since they usually carry little meaning in most natural language tasks.\n\nLemmatization:\n\nEach word is lemmatized using the lemmatizer.lemmatize(word) method to ensure that different forms of the same word are reduced to their root (e.g., “books” → “book”, “running” → “run”).\n\nRejoining the Tokens:\n\nAfter processing, the tokens are rejoined back into a single string using ' '.join(tokens).\n\n\n\n\n9.1.4.5 Applying the Preprocessing Function\n# Apply preprocessing to the \"Product Description\" column\ndata['cleaned_description'] = descriptions.apply(preprocess_text)\n\nThe preprocess_text() function is applied to each entry in the ‘Product Description’ column using descriptions.apply(preprocess_text).\nThe cleaned and preprocessed text is then stored in a new column in the DataFrame called 'cleaned_description'.\n\n\n\n9.1.4.6 Adding the ‘Preprocessed Text’ Column\n# Add the \"preprocessed text\" column to the original data\ndata['preprocessed_text'] = data['cleaned_description']\n\nThis line adds a new column called 'preprocessed_text' to the data DataFrame, which contains the preprocessed (cleaned) descriptions.\n\n\n\n9.1.4.7 Saving the Updated Data to a CSV File\n# Save the updated data back to the input CSV file, including the new column\nupdated_file_path = 'books_scraped_with_preprocessed_descriptions.csv'\ndata.to_csv(updated_file_path, index=False)\n\nThe updated data DataFrame, now containing both the original and preprocessed descriptions, is saved back to a new CSV file named 'books_scraped_with_preprocessed_descriptions.csv' using to_csv().\nThe index=False parameter ensures that the DataFrame index is not written to the file.\n\n\n\n9.1.4.8 Final Output:\n\nThe output CSV file contains all the original data, with the cleaned descriptions stored in the preprocessed_text column.\nThis file can be used for further analysis, such as topic modeling, sentiment analysis, or other natural language processing tasks.x\n\n\n\n\n9.1.5 Importance of Preprocessing\nEffective text preprocessing is crucial for generating meaningful insights from unstructured data. By reducing noise and standardizing the text, preprocessing enhances the performance of topic models and other natural language processing tasks.\nIn the context of topic modeling, preprocessing ensures that the algorithm focuses on meaningful words rather than irrelevant tokens, making the model’s output more interpretable and accurate.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Text Preprocessing</span>"
    ]
  },
  {
    "objectID": "10.Word_Cloud.html",
    "href": "10.Word_Cloud.html",
    "title": "10  Visualizing Word Frequencies with Graphs and Word Clouds",
    "section": "",
    "text": "10.1 Gutenberg Text Analysis\nThe code provided is meant to download and display the names of books from the Gutenberg corpus using the NLTK (Natural Language Toolkit) library.\nIn the code below, you’re selecting 5 books from the Gutenberg corpus using the books_idx list. Each index in books_idx corresponds to a specific book in books_names.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Visualizing Word Frequencies with Graphs and Word Clouds</span>"
    ]
  },
  {
    "objectID": "10.Word_Cloud.html#gutenberg-text-analysis",
    "href": "10.Word_Cloud.html#gutenberg-text-analysis",
    "title": "10  Visualizing Word Frequencies with Graphs and Word Clouds",
    "section": "",
    "text": "import numpy as np      # Import the numpy library, used for numerical operations and array handling\nimport pandas as pd     # Import the pandas library, used for data manipulation and analysis\nimport nltk             # Import the nltk library, used for natural language processing (NLP) tasks\n\nnltk.download('gutenberg')    # Download the 'gutenberg' corpus, a collection of literary texts\nbooks_names = nltk.corpus.gutenberg.fileids()  # Get a list of available book file IDs (names of the books)\nbooks_names   # Display the list of book names\n\nbooks_idx = [1, 3, 5, -5, -1]     # List of indices to select specific books from the books_names list\nselected_books = []               # Initialize an empty list to store selected books\n\nfor idx in books_idx:             # Loop through each index in books_idx\n    selected_books.append(books_names[idx])  # Append the book name at the given index to the selected_books list\n\nprint(selected_books)             # Print the list of selected book names\n\n10.1.1 (Gutenberg) Example of Selected Books:\nIf the books_names list is the following:\n['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', 'bible-kjv.txt', 'blake-poems.txt', 'bryant-stories.txt', 'burgess-busterbrown.txt', 'carroll-alice.txt', 'chesterton-ball.txt', 'chesterton-brown.txt', 'chesterton-thursday.txt', 'edgeworth-parents.txt', 'melville-moby_dick.txt', 'milton-paradise.txt', 'shakespeare-caesar.txt', 'shakespeare-hamlet.txt', 'shakespeare-macbeth.txt', 'whitman-leaves.txt']\nThe indices [1, 3, 5, -5, -1] correspond to: - 1: 'austen-persuasion.txt' - 3: 'bible-kjv.txt' - 5: 'bryant-stories.txt - -5: 'milton-paradise.txt' - -1: 'whitman-leaves.txt'\nSo, selected_books will contain:\n['austen-persuasion.txt', 'bible-kjv.txt', 'bryant-stories.txt', 'milton-paradise.txt', 'whitman-leaves.txt']\nSave the selected texts to my local drive.\nimport os\nimport nltk\n\n# Ensure the required corpus is available\nnltk.download('gutenberg')\n\n# Sample selected_books for demonstration, in case corpus access is not available\nselected_books = ['austen-persuasion.txt', 'bible-kjv.txt', 'chesterton-ball.txt', 'melville-moby_dick.txt', 'whitman-leaves.txt']\n\n# Define the folder name where the files will be saved\nfolder_name = \"/mnt/data/selected_books_texts\"\n\n# Create the folder if it doesn't exist\nif not os.path.exists(folder_name):\n    os.makedirs(folder_name)  # Create the subfolder\n\n# Iterate through each selected book and save its content to a file in the subfolder\nfor book in selected_books:\n    # Get the full text of the book using nltk.corpus.gutenberg.raw() function\n    book_text = nltk.corpus.gutenberg.raw(book)\n    \n    # Create a file name for each book by removing \".txt\" from the file id\n    file_name = book.replace('.txt', '') + '_content.txt'\n    \n    # Define the full path to save the file in the subfolder\n    full_path = os.path.join(folder_name, file_name)\n    \n    # Write the book content to a text file in the subfolder\n    with open(full_path, 'w') as file:\n        file.write(book_text)  # Write the full text of the book to the file\n\n# Displaying the directory path where the books are saved\nfolder_name\n\n\n10.1.2 (Gutenberg) Load the text of txt:\n# Load the text of \"austen-persuasion.txt\"\ndoc_austen = nltk.corpus.gutenberg.raw('austen-persuasion.txt')  # Use nltk.corpus.gutenberg.raw() to get the full text\n\n# Print the number of characters in the text\nprint('#Num of characters used:', len(doc_austen))  # Print the total number of characters in the book\n\n# Print a sample of the first 100 characters\nprint('#Text sample:')\nprint(doc_austen[:100])  # Print the first 100 characters of the book\nOutput:\n```\n\n```\n\n\n10.1.3 Tokenize\nTokenization is a fundamental step in Natural Language Processing (NLP) that involves splitting text into smaller units, known as tokens. These tokens could be words, phrases, sentences, or even characters, depending on the type of tokenization applied.\nTokenization is a crucial preprocessing step for text analysis because it breaks down raw text into manageable pieces that can be analyzed or manipulated. It allows algorithms to understand and process text data effectively. For example, tokenization helps convert a text document into a form that can be used for tasks like text classification, sentiment analysis, machine translation, etc.\n\n10.1.3.1 Types of Tokenization:\n\nWord Tokenization:\n\nSplits the text into individual words.\nExample:\n\nInput: \"The cat sat on the mat.\"\nOutput: ['The', 'cat', 'sat', 'on', 'the', 'mat']\n\n\nSentence Tokenization:\n\nSplits the text into sentences.\nExample:\n\nInput: \"The cat sat on the mat. It was happy.\"\nOutput: ['The cat sat on the mat.', 'It was happy.']\n\n\nCharacter Tokenization:\n\nSplits the text into individual characters.\nExample:\n\nInput: \"cat\"\nOutput: ['c', 'a', 't']\n\n\nSubword Tokenization:\n\nSplits words into smaller units, useful in languages where words are made up of many parts (like in morphologically rich languages). Models like BPE (Byte Pair Encoding) or WordPiece use subword tokenization.\nExample:\n\nInput: \"unhappiness\"\nOutput: ['un', 'happi', 'ness']\n\n\n\n\n\n10.1.3.2 Example of Tokenization in Python Using NLTK:\nThe NLTK (Natural Language Toolkit) provides functions to tokenize text easily.\nWord Tokenization Example:\nimport nltk\nfrom nltk.tokenize import word_tokenize\n\n# Example sentence\ntext = \"The cat sat on the mat.\"\n\n# Tokenize the sentence into words\ntokens = word_tokenize(text)\n\nprint(tokens)\nOutput:\n['The', 'cat', 'sat', 'on', 'the', 'mat', '.']\nSentence Tokenization Example:\nfrom nltk.tokenize import sent_tokenize\n\n# Example paragraph\ntext = \"The cat sat on the mat. It was happy.\"\n\n# Tokenize the text into sentences\nsentences = sent_tokenize(text)\n\nprint(sentences)\nOutput:\n['The cat sat on the mat.', 'It was happy.']\n\n\n10.1.3.3 Applications of Tokenization:\n\nText Classification: Breaking down text into words to feed into a machine learning model.\nMachine Translation: Splitting input and output sentences to translate between languages.\nInformation Retrieval: Indexing tokens (words) to help retrieve documents based on search queries.\nText Analysis: Tokenized text allows algorithms to count word frequencies, perform sentiment analysis, and more.\n\n\n\n\n10.1.4 (Gutenberg) Tokenize the selected txt:\nfrom nltk.tokenize import word_tokenize  # Import the word_tokenize function from NLTK\n\n# Tokenize the doc_alice text (assuming doc_alice contains the raw text of \"Alice in Wonderland\")\ntokens_austen = word_tokenize(doc_austen)\n\n# Print the number of tokens generated after tokenization\nprint('#Num of tokens used:', len(tokens_austen))\n\n# Print a sample of the first 20 tokens\nprint('#Token sample:')\nprint(tokens_austen[:20])\nOutput:\n\nThe following pattern captures words with 3 or more alphanumeric characters or apostrophes.\nfrom nltk.tokenize import RegexpTokenizer  # Import the RegexpTokenizer from NLTK\n\n# Initialize the tokenizer with a regular expression pattern\ntokenizer = RegexpTokenizer(r\"[\\w']{3,}\")  # This pattern captures words with 3 or more alphanumeric characters or apostrophes\n\n# Tokenize the text (converted to lowercase) using the RegexpTokenizer\nreg_tokens_austen = tokenizer.tokenize(doc_austen.lower())  # Convert text to lowercase before tokenization\n\n# Print the number of tokens after Regexp tokenization\nprint('#Num of tokens with RegexpTokenizer:', len(reg_tokens_austen))\n\n# Print a sample of the first 20 tokens\nprint('#Token sample:')\nprint(reg_tokens_austen[:20])\nOutput:",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Visualizing Word Frequencies with Graphs and Word Clouds</span>"
    ]
  },
  {
    "objectID": "10.Word_Cloud.html#what-is-stopwords",
    "href": "10.Word_Cloud.html#what-is-stopwords",
    "title": "10  Visualizing Word Frequencies with Graphs and Word Clouds",
    "section": "10.2 What is Stopwords?",
    "text": "10.2 What is Stopwords?\nStopwords are common words in a language that are often filtered out before processing natural language text. These words carry very little semantic meaning and are often omitted during tasks such as text mining, search indexing, and information retrieval. The goal of removing stopwords is to reduce the noise and focus on the more meaningful words that contribute to the context or content of the text.\n\n10.2.1 Why Remove Stopwords?\nStopwords such as “the”, “is”, “in”, “and”, “on”, etc., are very frequent in most texts but don’t carry significant meaning by themselves. Removing them can:\n\nReduce text size: Fewer words mean faster processing, which can improve the efficiency of algorithms.\nImprove focus: Removing common, non-informative words helps focus on the important terms that can provide insights.\nReduce dimensionality: When working with techniques like TF-IDF, vectorization, or word embeddings, removing stopwords helps lower the dimensionality of the data.\n\n\n\n10.2.2 Common Examples of Stopwords:\n\nEnglish stopwords: \"the\", \"is\", \"in\", \"at\", \"on\", \"and\", \"a\", \"an\", \"but\", \"or\", \"so\", \"if\", \"then\", etc.\nStopwords vary by language. Each language has its own set of stopwords based on the common, non-essential words in that language.\n\n\n\n10.2.3 Example of Stopwords in Python Using NLTK:\nThe NLTK library provides a built-in list of stopwords for various languages. Here’s how you can use them:\nimport nltk\nfrom nltk.corpus import stopwords\n\n# Download the stopwords dataset\nnltk.download('stopwords')\n\n# Get the list of English stopwords\nstop_words = set(stopwords.words('english'))\n\n# Example sentence\ntext = \"The cat sat on the mat and the dog lay beside it.\"\n\n# Tokenize the text into words\nfrom nltk.tokenize import word_tokenize\nwords = word_tokenize(text)\n\n# Filter out the stopwords\nfiltered_words = [word for word in words if word.lower() not in stop_words]\n\nprint(\"Original Words:\", words)\nprint(\"Filtered Words (without stopwords):\", filtered_words)\nOutput:\nOriginal Words: ['The', 'cat', 'sat', 'on', 'the', 'mat', 'and', 'the', 'dog', 'lay', 'beside', 'it', '.']\nFiltered Words (without stopwords): ['cat', 'sat', 'mat', 'dog', 'lay', 'beside', '.']\nAs you can see, common words like “the”, “on”, “and”, “it” have been removed, leaving only the more meaningful words.\n\n\n10.2.4 When Not to Remove Stopwords:\nWhile stopwords are typically removed for most NLP tasks, there are cases where you might want to retain them, such as:\n\nSentiment Analysis: Words like “not”, “is”, “but” can change the sentiment of a sentence.\nLanguage Modeling: For tasks like speech generation, translation, or conversational agents, removing stopwords might lose important grammatical structure.\n\n\n\n10.2.5 Stopwords in Other NLP Libraries:\nIn addition to NLTK, stopwords are also supported in other libraries like spaCy and Scikit-learn.\n\n10.2.5.1 Stopwords in spaCy:\nimport spacy\nfrom spacy.lang.en.stop_words import STOP_WORDS\n\n# Load English model\nnlp = spacy.load(\"en_core_web_sm\")\n\n# Example of using spaCy stopwords\nprint(STOP_WORDS)\nfrom nltk.corpus import stopwords  # Import the stopwords list from NLTK\n\n# Load the set of English stopwords (commonly excluded words like 'the', 'is', etc.)\nenglish_stops = set(stopwords.words('english'))  # Convert to set for faster lookup\n\n# Filter out stopwords from the list of tokens using list comprehension\nresult_austen = [word for word in reg_tokens_austen if word not in english_stops]\n\n# Print the number of tokens after stopword elimination\nprint('#Num of tokens after stopword elimination:', len(result_austen))\n\n# Print a sample of the first 20 tokens after removing stopwords\nprint('#Token sample:')\nprint(result_austen[:20])\nOutput:\n\n\n\n\n10.2.6 (Gutenberg) Word Frequency Count of the selected txt:\n# Initialize an empty dictionary to store word counts\nausten_word_count = dict()\n\n# Count the frequency of each word in result_alice\nfor word in result_austen:\n    austen_word_count[word] = austen_word_count.get(word, 0) + 1  # Increment the count for each word\n\n# Print the total number of unique words used\nprint('#Num of used words:', len(austen_word_count))\n\n# Sort the dictionary by word frequency in descending order\nsorted_word_count = sorted(austen_word_count, key=austen_word_count.get, reverse=True)\n\n# Print the top 20 most frequent words\nprint(\"#Top 20 high frequency words:\")\nfor key in sorted_word_count[:20]:  # Loop through the top 20 most frequent words\n    print(f'{repr(key)}: {austen_word_count[key]}', end=', ')\nOutput:\n\n\n\n10.2.7 (Gutenberg) Word Frequency Count of the selected txt with Part-of-Speech (POS):\n# Define the set of part-of-speech (POS) tags we are interested in\nmy_tag_set = ['NN', 'VB', 'VBD', 'JJ']  # NN: Nouns, VB: Verbs (base), VBD: Verbs (past), JJ: Adjectives\n\n# Filter words based on their POS tags\nmy_words = [word for word, tag in nltk.pos_tag(result_austen) if tag in my_tag_set]\n\n# Initialize an empty dictionary to store word counts\nausten_word_count = dict()\n\n# Count the frequency of each filtered word\nfor word in my_words:\n    austen_word_count[word] = austen_word_count.get(word, 0) + 1  # Increment the count for each word\n\n# Print the total number of unique words used after filtering by POS tags\nprint('#Num of used words:', len(austen_word_count))\n\n# Sort the dictionary by word frequency in descending order\nsorted_word_count = sorted(austen_word_count, key=austen_word_count.get, reverse=True)\n\n# Print the top 20 most frequent words\nprint(\"#Top 20 high frequency words:\")\nfor key in sorted_word_count[:20]:  # Loop through the top 20 most frequent words\n    print(f'{repr(key)}: {austen_word_count[key]}', end=', ')\nOutput:",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Visualizing Word Frequencies with Graphs and Word Clouds</span>"
    ]
  },
  {
    "objectID": "10.Word_Cloud.html#visualization",
    "href": "10.Word_Cloud.html#visualization",
    "title": "10  Visualizing Word Frequencies with Graphs and Word Clouds",
    "section": "10.3 Visualization",
    "text": "10.3 Visualization\nVisualizing word frequencies can help you understand the most common words in a text and gain insights into the text data. Here are some common methods to visualize word frequencies:\n\n10.3.1 Bar Plot\nA bar plot is a simple and effective way to visualize word frequencies. The most frequent words can be shown on the x-axis, and their frequencies on the y-axis.\n\n\n10.3.2 Word Cloud\nA word cloud represents words with varying font sizes depending on their frequencies. Words that appear more frequently are displayed larger, while less frequent words are smaller.\n\n\n\n10.3.3 Example 1: Visualize Word Frequencies Using a Bar Plot\nYou can use the matplotlib and seaborn libraries to create a bar plot showing the most frequent words.\n\n10.3.3.1 Steps:\n\nCount the word frequencies.\nSort the word frequencies in descending order.\nPlot the top N most frequent words.\n\n\n\n10.3.3.2 Code Example:\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Example word count dictionary (can be your `austen_word_count`)\nword_frequencies = {\"the\": 50, \"cat\": 10, \"sat\": 8, \"on\": 20, \"mat\": 5}\n\n# Convert to a list of tuples and sort by frequency (descending)\nsorted_words = sorted(word_frequencies.items(), key=lambda x: x[1], reverse=True)\n\n# Get top N most frequent words\ntop_words = sorted_words[:5]  # Top 5 most frequent words\n\n# Separate words and frequencies for plotting\nwords, counts = zip(*top_words)\n\n# Create a bar plot\nplt.figure(figsize=(8, 6))\nsns.barplot(x=list(words), y=list(counts), palette=\"viridis\")\nplt.title(\"Top 5 Most Frequent Words\")\nplt.ylabel(\"Frequency\")\nplt.xlabel(\"Words\")\nplt.show()\n\n\n10.3.3.3 Explanation:\nLet’s break down the line of code in detail:\nsorted_words = sorted(word_frequencies.items(), key=lambda x: x[1], reverse=True)\nword_frequencies.items()\nword_frequencies is a dictionary that stores the words as keys and their frequencies as values. For example, the dictionary might look like this:\nword_frequencies = {\n    \"the\": 50,\n    \"cat\": 10,\n    \"sat\": 8,\n    \"on\": 20,\n    \"mat\": 5\n}\nThe .items() method is used to get the items of the dictionary. When you call .items() on a dictionary, it returns a view object that contains a list of key-value pairs (tuples). Each tuple contains a word and its corresponding frequency.\nFor example:\nword_frequencies.items()\nOutput:\n[('the', 50), ('cat', 10), ('sat', 8), ('on', 20), ('mat', 5)]\nSo, word_frequencies.items() returns the following list of tuples:\n[('the', 50), ('cat', 10), ('sat', 8), ('on', 20), ('mat', 5)]\nsorted(..., key=lambda x: x[1], reverse=True)\nThis part of the code is responsible for sorting the word-frequency pairs based on the frequency values.\nsorted() Function:\nThe sorted() function takes an iterable (in this case, the list of word-frequency pairs) and returns a new sorted list. By default, sorted() sorts items in ascending order. However, it allows for customization through two important arguments:\n\nkey: This defines a function that will be applied to each item to extract a comparison key for sorting. In this case, we want to sort the tuples by their frequency (the second item in each tuple, x[1]).\nreverse: If set to True, this will sort the list in descending order instead of the default ascending order.\n\nkey=lambda x: x[1]\n\nThe key argument allows us to specify a custom sorting criterion. In this case, we use a lambda function (lambda x: x[1]) to tell Python how to sort the list.\nA lambda function is an anonymous, short function that is often used for small operations that are only needed temporarily.\n\nlambda x: x[1] means: - For each tuple in the list (e.g., ('the', 50)), treat x as the tuple. - x[1] refers to the second element of the tuple, which is the frequency in this case.\nSo, when the sorting process happens, the sorted function will compare the second element of each tuple (i.e., the frequencies) to determine the order.\nreverse=True\n\nThis tells the sorted() function to sort the list in descending order. By default, sorting is done in ascending order (smallest to largest), but reverse=True reverses that order, sorting from the highest frequency to the lowest.\n\n\nwords, counts = zip(*top_words)\nzip(*top_words):\n\n- zip() is a Python built-in function that takes multiple iterables (like lists, tuples, etc.) and returns an iterator of tuples, where each tuple contains elements from the provided iterables at corresponding positions.\n- In this case, the * operator is called the unpacking operator (also known as the splat operator). It unpacks the list of tuples (top_words) so that each tuple in the list is treated as a separate argument to the zip() function.\n\nLet’s see what happens step by step:\n\ntop_words is:\n[('the', 50), ('on', 20), ('cat', 10), ('sat', 8), ('mat', 5)]\nThe *top_words unpacks the list of tuples into separate arguments. The zip() function will receive each tuple separately, as if you passed:\nzip(('the', 50), ('on', 20), ('cat', 10), ('sat', 8), ('mat', 5))\nNow, zip() will combine the first elements of each tuple into one iterable, and the second elements into another iterable:\n\nIt takes the first element of each tuple (words) and groups them together: ('the', 'on', 'cat', 'sat', 'mat').\nIt takes the second element of each tuple (counts) and groups them together: (50, 20, 10, 8, 5).\n\n\nSo, zip(*top_words) effectively separates the words and their frequencies into two separate iterables.\nwords, counts = zip(*top_words):\n\nwords and counts are assigned the result of zip(*top_words).\n\nwords will contain the tuple with all the words: ('the', 'on', 'cat', 'sat', 'mat').\ncounts will contain the tuple with all the word frequencies: (50, 20, 10, 8, 5).\n\n\n\n\n\n10.3.4 Example in Action:\nLet’s say you have the following top_words:\ntop_words = [('the', 50), ('on', 20), ('cat', 10), ('sat', 8), ('mat', 5)]\nWhen you apply the line:\nwords, counts = zip(*top_words)\nHere’s what happens:\n\nUnpacking top_words:\n\nThe * unpacks the list into separate tuples:\n\n('the', 50)\n('on', 20)\n('cat', 10)\n('sat', 8)\n('mat', 5)\n\n\nzip() function groups elements:\n\nGroups the first elements of all tuples (the words): ('the', 'on', 'cat', 'sat', 'mat')\nGroups the second elements of all tuples (the frequencies): (50, 20, 10, 8, 5)\n\nAssigning the results:\n\nwords will contain: ('the', 'on', 'cat', 'sat', 'mat')\ncounts will contain: (50, 20, 10, 8, 5)\n\n\nNow, you have two separate sequences (words and their corresponding frequencies), which can be used for plotting or further processing.\n\n\n\n10.3.5 Example 2: Visualize Word Frequencies Using a Word Cloud\nThe wordcloud library can be used to generate word clouds. You can adjust the appearance by changing parameters such as the background color, maximum number of words, etc.\n\n10.3.5.1 Code Example:\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\n\n# Example word count dictionary (can be your `austen_word_count`)\nword_frequencies = {\"the\": 50, \"cat\": 10, \"sat\": 8, \"on\": 20, \"mat\": 5}\n\n# Create a word cloud object\nwordcloud = WordCloud(width=800, height=400, background_color=\"white\").generate_from_frequencies(word_frequencies)\n\n# Plot the word cloud\nplt.figure(figsize=(10, 5))\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis(\"off\")  # No axes, just the word cloud\nplt.show()\n\n\n10.3.5.2 Explanation:\n\nThe WordCloud() object is created using the word frequencies.\nThe generate_from_frequencies() method is used to pass the word frequencies into the word cloud.\nThe plot is displayed with matplotlib, and the axes are turned off for a cleaner look.\n\n\n\n10.3.5.3 Output:\n\n\n\n\n\n10.3.6 Choosing the Right Visualization:\n\nBar Plot: Best when you want to compare the exact counts of the most frequent words.\nWord Cloud: Provides a more artistic and immediate visual representation but doesn’t allow for easy comparison of exact counts.\n\n\n\n10.3.7 (Gutenberg) Frequency Curve:\nimport matplotlib.pyplot as plt  # Import the matplotlib library for plotting\n# %matplotlib inline  # Ensure plots appear inline in notebooks (if using Jupyter)\n\n# Extract the frequencies of the words in the sorted word list\nw = [austen_word_count[key] for key in sorted_word_count]  # Get the frequencies of sorted words\n\n# Plot the word frequencies\nplt.plot(w)\n\n# Display the plot\nplt.show()\nOutput:\n\n\n\n10.3.8 Zipf’s Law curve or Zipfian distribution.\n\n10.3.8.1 Zipf’s Law:\nZipf’s Law states that in a corpus of natural language text, the frequency of any word is inversely proportional to its rank in the frequency table. In simpler terms:\n\nThe most frequent word occurs twice as often as the second most frequent word, three times as often as the third most frequent word, and so on.\n\n\n\n10.3.8.2 Key Characteristics:\n\nThe x-axis typically represents the rank of words in terms of frequency (with the most frequent word ranked 1).\nThe y-axis represents the frequency of those words.\nThe curve shows a steep drop for the highest frequency words, followed by a long tail of lower frequency words, which is characteristic of power-law distributions.\n\n* What is power-law distribution?\nhttps://alcera.wpcomstaging.com/2016/09/14/another-powerful-idea-the-power-law/\n\n\n10.3.8.3 Why Does It Matter?\nZipf’s law highlights the natural imbalance in word usage: a few words (like “the”, “is”, “and”) are used very frequently, while the majority of words in a language are used infrequently. This insight is crucial for tasks like:\n\nStopword removal: Removing extremely common words that provide little unique information.\nVocabulary management: Determining which words to prioritize when building language models.\n\n\n\n\n10.3.9 (Gutenberg) Bar Plot:\nimport matplotlib.pyplot as plt  # Ensure matplotlib is imported\n\n# Extract the top 20 most frequent words\nn = sorted_word_count[:20]  # Select the top 20 words based on frequency\n\n# Get the frequencies of the top 20 words\nw = [austen_word_count[key] for key in n]  # Get the frequency values for the top 20 words\n\n# Plot the bar chart\nplt.bar(range(len(n)), w, tick_label=n)  # Create a bar chart; 'tick_label' adds the word labels\n\n# Rotate the x-axis labels by 45 degrees\nplt.xticks(rotation=45)\n\n# Display the plot\nplt.show()\nOutput:\n\n\n\n10.3.10 (Gutenberg) Word Cloud:\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt  # Import matplotlib for displaying the word cloud\n\n# Generate a word cloud image from the raw text\nwordcloud = WordCloud().generate(doc_austen)\n\n# Turn off the axis lines and labels\nplt.axis(\"off\")\n\n# Display the word cloud image using matplotlib\nplt.imshow(wordcloud, interpolation='bilinear')\n\n# Show the plot\nplt.show()\nOutput:\n\nwordcloud = WordCloud(max_font_size=60).generate_from_frequencies(austen_word_count)\nplt.figure()\nplt.axis(\"off\")\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.show()\nOutput:\n\nWith mask image:\nimport numpy as np  # Import numpy to handle arrays\nfrom PIL import Image  # Import PIL for image processing\nfrom wordcloud import WordCloud  # Import the WordCloud class for generating word clouds\nimport matplotlib.pyplot as plt  # Import matplotlib for displaying the word cloud\n\n# Load the mask image (used as the background shape for the word cloud)\nausten_mask = np.array(Image.open(\"cloud.png\"))  # Convert the image to a numpy array\n\n# Create a word cloud object with customization options\nwc = WordCloud(\n    background_color=\"white\",  # Set the background color of the word cloud\n    max_words=30,  # Maximum number of words to display\n    mask=austen_mask,  # Use the mask image to shape the word cloud\n    contour_width=3,  # Set the width of the outline of the mask\n    contour_color='steelblue'  # Set the color of the contour/outline\n)\n\n# Generate the word cloud using the word frequencies from alice_word_count\nwc.generate_from_frequencies(austen_word_count)\n\n# Save the generated word cloud to a file\nwc.to_file(\"cloud_output.png\")  # Save the output as \"cloud_output.png\"\n\n# Display the word cloud using matplotlib\nplt.figure()  # Create a new figure\nplt.axis(\"off\")  # Turn off axis lines and labels\nplt.imshow(wc, interpolation='bilinear')  # Display the word cloud\nplt.show()  # Show the word cloud\nOutput:\n\nEnd.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Visualizing Word Frequencies with Graphs and Word Clouds</span>"
    ]
  },
  {
    "objectID": "11.Text_Representation.html",
    "href": "11.Text_Representation.html",
    "title": "11  Text Representation Based on Counts",
    "section": "",
    "text": "11.0.1 What is a vector?\nA vector is explained in relation to scalars, matrices, and tensors. These concepts are important for understanding various mathematical and physical quantities.\nVectors and these structures are also applied in natural language processing to represent characteristics of words, phrases, and sentences.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Text Representation Based on Counts</span>"
    ]
  },
  {
    "objectID": "11.Text_Representation.html#embedding-representation-methods",
    "href": "11.Text_Representation.html#embedding-representation-methods",
    "title": "11  Text Representation Based on Counts",
    "section": "11.1 Embedding Representation Methods",
    "text": "11.1 Embedding Representation Methods\nTo understand embedding, it is necessary first to understand how vectors are represented and generated. Vector representation can broadly be classified into sparse representation and distributed representation.\n\n\n\n(Image Source: https://www.oreilly.com/content/how-neural-networks-learn-distributed-representations/)\n\n\nTo illustrate the concepts of sparse and distributed representations, we can use a simple shape example. In the above Figure, four shapes and their names are shown. The target shape’s name is represented in black, and the other shapes’ names are left blank. In this method, the number of shapes corresponds to the number of features (i.e., dimensions), and each feature is independent.\nIn sparse representation, features are encoded using 1s and 0s. The black mark is represented as 1, and the white marks as 0s. Most of the values are 0, with only the corresponding feature being marked as 1. This method is called sparse representation because only a few features are active (non-zero).\nIn the figure, the first shape is a horizontally long rectangle, and the second shape is a vertically long rectangle. These two shapes share no similarities in sparse representation. Each shape is represented independently, with no overlap in features.\n\nIn contrast, distributed representation captures the similarity between shapes. In the following Figure, both the long horizontal and long vertical rectangles share a common feature, i.e., the fact that they are rectangles, but they differ in their specific directions (horizontal or vertical). This allows for more compact and meaningful vector representations by capturing shared properties between different objects.\n\n\n\n(Image Soruce: https://www.oreilly.com/content/how-neural-networks-learn-distributed-representations/)\n\n\nFor instance, the first and second shapes share the property of being rectangles, while the third and fourth shapes share the property of being towers. In distributed representation, the shapes are described based on their common characteristics, rather than independent one-hot features as in sparse representation.\n\n\n\n\n(Image Source: https://wikidocs.net/31767)\n\n\n\n11.1.1 Local Representation:\n\nOne-hot Vector: A simple count-based method where each word is represented by a vector where only one position is marked as 1 (the word’s position in the vocabulary), while all other positions are 0.\nN-gram: A technique that considers sequences of words (or characters) of length N to capture local context in text.\n\n\n\n11.1.2 Continuous Representation:\nThis is more advanced than local representation and is generally better at capturing the semantic relationships between words.\n\nCount-Based Methods:\n\nBag of Words (DTM): A representation that counts word occurrences in documents without considering word order. This includes methods like document-term matrices (DTM).\nLSA (Latent Semantic Analysis): A method based on singular value decomposition of the document-term matrix, often used for capturing the global meaning of words across full documents.\nGloVe (Global Vectors for Word Representation): A word representation that combines the benefits of global matrix factorization and local context window methods.\n\nPrediction-Based Methods:\n\nWord2Vec (FastText): A continuous bag-of-words (CBOW) and skip-gram model that predicts either a word from its surrounding context or the context from a given word. FastText is an extension of Word2Vec that also considers subword information.\n\n\n\n\n\n11.1.3 Sparse Representation: One-Hot Encoding\nOne of the simplest forms of embedding is one-hot encoding, a type of sparse representation. In this method, each element (such as a word) in a collection (e.g., sentence, document) is represented as a vector where the target element is assigned a value of 1, and all other elements are assigned a value of 0.\nFor example, if we have the sentence “all is well” with the words “all,” “is,” and “well,” the one-hot encoding representation for these words would be:\n\n\n\nWord\nall\nis\nwell\n\n\n\n\nall\n1\n0\n0\n\n\nis\n0\n1\n0\n\n\nwell\n0\n0\n1\n\n\n\n[Figure] One-Hot Encoding Method of Embedding\nThis table represents the one-hot encoding embedding of the words “all,” “is,” and “well,” where each word is assigned a unique vector. Only the position corresponding to the word has a value of 1, and all other positions are 0.\n\n11.1.3.1 Advantages:\n\nIt is easy to understand which words are being used, as the representation is simple.\n\n\n\n11.1.3.2 Disadvantages:\n\nAs the number of words increases, the size of the vector grows, leading to high memory usage.\nOne-hot encoding does not capture any semantic similarity between words; for example, “king” and “queen” would be completely different.\n\nThe document-term matrix (DTM) is an extension of one-hot encoding at the document level. The image also mentions the term frequency-inverse document frequency (TF-IDF), which is another approach that assigns importance to words based on how often they appear across multiple documents.\n\n\n\n11.1.4 Distributed Representation\nThe second major embedding type described is distributed representation, where words are represented as vectors with real-numbered values. Unlike one-hot encoding, distributed representation captures semantic relationships between words. For example, the word “all” could be represented by a vector like [-0.0031, -0.0421, 0.0125, 0.0062].\nHere is the translation of the table from the image to English:\n\n\n\nWord\nDimension 1\nDimension 2\nDimension 3\nDimension 4\n\n\n\n\nall\n-0.0031\n-0.0421\n0.0125\n0.0062\n\n\nis\n0.0212\n0.0125\n-0.0089\n0.0376\n\n\nwell\n-0.0543\n0.0684\n-0.0023\n0.0181\n\n\n\n[Figure] Distributed Representation Method of Embedding\nThis table shows an example of distributed embeddings for the words “all,” “is,” and “well,” where each word is represented as a vector in a 4-dimensional space. Each dimension has a real number value, capturing more information about the relationships between words compared to one-hot encoding.\n\n11.1.4.1 Advantages:\n\nDistributed representations can capture the semantic meaning of words.\nThe dimensionality of vectors does not increase as more words are added.\n\nThis method is widely used in natural language processing today because it can efficiently capture word meanings and relationships. Techniques like Word2Vec calculate these word embeddings. Two popular models for Word2Vec are CBOW (Continuous Bag of Words Model) and Skip-Gram (see Chapter 12. Word Embedding and Similarity)",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Text Representation Based on Counts</span>"
    ]
  },
  {
    "objectID": "11.Text_Representation.html#examples-of-text-representation-methods",
    "href": "11.Text_Representation.html#examples-of-text-representation-methods",
    "title": "11  Text Representation Based on Counts",
    "section": "11.2 Examples of Text Representation Methods",
    "text": "11.2 Examples of Text Representation Methods\n\n11.2.1 Document-Term Matrix (DTM)\n\n\n11.2.2 Concept of Document-Term Matrix\nA Document-Term Matrix (DTM) is a matrix that represents the frequency of terms occurring in a set of documents. This matrix is widely used in natural language processing (NLP) for tasks such as text mining, information retrieval, and topic modeling.\nimport pandas as pd\n\n# Example Document-Term Matrix (DTM) data\ndata = {\n    'word1': [4, 0, 0, 3, 0, 1],\n    'word2': [0, 1, 5, 0, 2, 0],\n    'word3': [0, 7, 0, 8, 1, 4],\n    'word4': [0, 0, 3, 0, 0, 0],\n    'word5': [1, 0, 0, 1, 2, 4],\n    'word6': [2, 1, 9, 0, 12, 0],\n    'wordN': [0, 3, 1, 0, 4, 2],\n}\n\n# Creating the Document-Term Matrix DataFrame\ndtm_df = pd.DataFrame(data, index=[f'doc{i+1}' for i in range(6)])\n\ndtm_df\n\n\n\n11.2.3 Understanding the Components:\n\nRows (doc1, doc, … ,doc6): Each row represents a document in the corpus.\nColumns (word1, word2, … , wordN): Each column represents a unique word (term) in the corpus.\nValues: The numbers in the matrix represent the frequency of each term in the corresponding document. For instance, in document ( doc1 ), term ( word1 ) appears 4 times, while term ( word1 ) appears once.\n\nHere is an example of how to create a Document-Term Matrix using Python.\n# Import necessary libraries\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport pandas as pd\n\n# Define sample documents\ndocuments = [ \n    \"Sir Walter has resented it.  As the head of the house, he felt that he ought to have been consulted, especially after taking the young man so publicly by the hand\",\n    \"Of Man's first disobedience, and the fruit Of that forbidden tree whose mortal taste Brought death into the World, and all our woe,  With loss of Eden, till one greater Man  Restore us, and regain the blissful seat, \",\n    \"Come, said my soul, Such verses for my Body let us write, (for we are one,) That should I after return, Or, long, long hence, in other spheres, There to some group of mates the chants resuming, (Tallying Earth's soil, trees, winds, tumultuous waves,) Ever with pleas'd smile I may keep on, Ever and ever yet the verses owning--as, first, I here and now Signing for Soul and Body, set to them my name,\"\n]\n\n# Initialize CountVectorizer to count word frequencies\nvectorizer = CountVectorizer()\n\n# Fit and transform the documents to generate a word count vector\nX = vectorizer.fit_transform(documents)\n\n# Get the list of words\nwords = vectorizer.get_feature_names_out()\n\n# Convert the result to a DataFrame for better visualization\ndf_bow = pd.DataFrame(X.toarray(), columns=words)\n\n# Display the result\ndf_bow\nThis code creates a Document-Term Matrix from three sample documents. The matrix counts the occurrences of each word in the documents and presents it in the following format:\n\nThis structure allows for further analysis such as identifying important words in a document or across documents, performing clustering, or calculating term importance metrics like TF-IDF (Term Frequency-Inverse Document Frequency).",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Text Representation Based on Counts</span>"
    ]
  },
  {
    "objectID": "11.Text_Representation.html#tf-idf-term-frequency-inverse-document-frequency",
    "href": "11.Text_Representation.html#tf-idf-term-frequency-inverse-document-frequency",
    "title": "11  Text Representation Based on Counts",
    "section": "11.3 TF-IDF (Term Frequency-Inverse Document Frequency)",
    "text": "11.3 TF-IDF (Term Frequency-Inverse Document Frequency)\n\n11.3.1 Concept of TF-IDF\nTF-IDF is a method used to evaluate the importance of a word in a document relative to a collection of documents. TF (Term Frequency) measures how frequently a term appears in a document, while IDF (Inverse Document Frequency) quantifies how rare the term is across all documents. A higher TF-IDF score indicates that the term is important in the document but appears infrequently in other documents.\n\n\n11.3.2 Term Frequency (TF)\nThe TF of a term measures how frequently a term occurs in a document. It is calculated as:\n\\[TF(t, d) = \\frac{\\text{Number of times term } t \\text{ appears in document } d}{\\text{Total number of terms in document } d}\\]\n\n\n11.3.3 Inverse Document Frequency (IDF)\nThe IDF of a term measures how important a term is in the whole corpus. Words that are common across many documents (e.g., “the”, “is”, etc.) receive a low score. The formula is:\n\\[\nIDF(t) = \\log\\left(\\frac{N}{1 + \\text{Number of documents containing term } t}\\right)\n\\] Where:\n\nN is the total number of documents.\nThe “+1” in the denominator is added to prevent division by zero.\n\n\n\n11.3.4 TF-IDF Score\nThe TF-IDF score is computed by multiplying TF and IDF:\n\\[ TF\\text{-}IDF(t, d) = TF(t, d) \\times IDF(t) \\]\n\n\n11.3.5 Example Python Code Using sklearn for TF-IDF\nLet’s calculate the TF-IDF scores for a small example corpus using Python.\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n# Sample corpus of documents\ndocuments = [\n    \"The cat sat on the mat\",\n    \"The dog sat on the log\",\n    \"Cats and dogs are pets\"\n]\n\n# Create the TF-IDF vectorizer\nvectorizer = TfidfVectorizer()\n\n# Fit the vectorizer on the sample documents\ntfidf_matrix = vectorizer.fit_transform(documents)\n\n# Get the feature names (terms)\nterms = vectorizer.get_feature_names_out()\n\n# Convert the TF-IDF matrix into a Pandas DataFrame for better readability\nimport pandas as pd\ntfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=terms)\n\nprint(tfidf_df)\n\nThis approach helps measure the relative importance of words in the documents and can be used for various NLP tasks like text classification, document clustering, etc.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Text Representation Based on Counts</span>"
    ]
  },
  {
    "objectID": "11.Text_Representation.html#comparison-of-dtm-and-tf-idf",
    "href": "11.Text_Representation.html#comparison-of-dtm-and-tf-idf",
    "title": "11  Text Representation Based on Counts",
    "section": "11.4 Comparison of DTM and TF-IDF",
    "text": "11.4 Comparison of DTM and TF-IDF\n\nDTM simply counts how many times each word appears in the document, without considering how common the word is in the entire document collection.\nTF-IDF combines frequency information with the rarity of the word across all documents, thus highlighting words that are important in one document but uncommon in the overall corpus.\n\nBoth methods are useful for tasks like calculating document similarity, extracting keywords, and analyzing the importance of specific terms in a corpus.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Text Representation Based on Counts</span>"
    ]
  },
  {
    "objectID": "11.Text_Representation.html#conclusion",
    "href": "11.Text_Representation.html#conclusion",
    "title": "11  Text Representation Based on Counts",
    "section": "11.5 Conclusion",
    "text": "11.5 Conclusion\nCount-based word representation methods like DTM and TF-IDF are fundamental tools for converting text into numerical formats, making it easier to apply statistical approaches to natural language data. In this chapter, we explored the concepts and Python implementations of both methods, providing a foundation for more complex NLP tasks.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Text Representation Based on Counts</span>"
    ]
  },
  {
    "objectID": "12.Word_Embedding.html",
    "href": "12.Word_Embedding.html",
    "title": "12  Word Embedding and Relational Similarity",
    "section": "",
    "text": "12.1 Word Embedding",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Word Embedding and Relational Similarity</span>"
    ]
  },
  {
    "objectID": "12.Word_Embedding.html#word-embedding",
    "href": "12.Word_Embedding.html#word-embedding",
    "title": "12  Word Embedding and Relational Similarity",
    "section": "",
    "text": "12.1.1 Overview\nWord Embedding refers to a technique where words are mapped to vectors of real numbers, typically in a lower-dimensional space. These vectors capture the semantic meaning of words. Unlike TF-IDF, embeddings capture semantic similarity—words with similar meanings will have closer vectors.\nPopular models for word embeddings include:\n\nWord2Vec: Predicts context words given a target word (CBOW) or predicts the target word given the context (Skip-gram).\nGloVe: Focuses on matrix factorization, capturing global statistical information about word co-occurrences.\nFastText: Extends Word2Vec by considering subword information, which helps model morphology.\n\n\n\n12.1.2 What is Word2Vec?\nWord2Vec is a model used to learn the relationships between words based on their co-occurrence in a corpus. It creates a dense vector representation of words such that words with similar meanings are closer in the vector space.\nThere are two main architectures for Word2Vec:\n\nCBOW (Continuous Bag of Words): Predict the current word from surrounding words.\nSkip-gram: Predict surrounding words given the current word.\n\n\n\n\n12.1.3 Example in Python (Using Gensim Word2Vec)\nimport gensim\nfrom gensim.models import Word2Vec\n\n# Sample sentences\nsentences = [\n    ['natural', 'language', 'processing', 'is', 'fascinating'],\n    ['word', 'embedding', 'and', 'tf-idf', 'are', 'techniques'],\n    ['text', 'similarity', 'can', 'be', 'measured', 'using', 'vector', 'similarity']\n]\n\n# Train Word2Vec model\nmodel = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)\n\n# Check vector for a word\nword_vector = model.wv['similarity']\nprint(f\"Vector for 'similarity':\\n{word_vector}\")\n\n# Find similar words to 'text'\nsimilar_words = model.wv.most_similar('text')\nprint(f\"Words similar to 'text': {similar_words}\")\n\nIn this example, a simple Word2Vec model is trained on a small dataset. After training, we can retrieve word vectors and find similar words based on vector proximity.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Word Embedding and Relational Similarity</span>"
    ]
  },
  {
    "objectID": "12.Word_Embedding.html#step-by-step-example-using-gensims-word2vec",
    "href": "12.Word_Embedding.html#step-by-step-example-using-gensims-word2vec",
    "title": "12  Word Embedding and Relational Similarity",
    "section": "12.2 Step-by-Step Example Using Gensim’s Word2Vec",
    "text": "12.2 Step-by-Step Example Using Gensim’s Word2Vec\n\n12.2.1 Step 1: Installing Gensim\nTo use Word2Vec in Python, you will need to install Gensim. You can install it using pip:\npip install gensim\n\n\n12.2.2 Step 2: Import Required Libraries\nWe will import the necessary libraries for training the Word2Vec model.\nimport gensim\nfrom gensim.models import Word2Vec\nimport nltk\nfrom nltk.corpus import brown\nnltk.download('brown')  # We will use the Brown corpus for this example\n\n\n12.2.3 Step 3: Preparing the Data\nWe will use the Brown corpus provided by NLTK, which contains a variety of text genres. First, let’s tokenize the sentences into words.\n# Load the Brown corpus sentences\nsentences = brown.sents()\n\n# Print a sample sentence\nprint(sentences[0])\n\n\n12.2.4 Step 4: Training the Word2Vec Model\nNow that we have the tokenized sentences, we can train the Word2Vec model using Gensim. The important parameters are:\n\nsentences: The training data (list of tokenized sentences).\nvector_size: The dimensionality of the word vectors.\nwindow: The maximum distance between the current and predicted word.\nmin_count: Ignores all words with a total frequency lower than this.\n\n# Train the Word2Vec model\nmodel = Word2Vec(sentences, vector_size=100, window=5, min_count=5, workers=4)\n\n# Save the model\nmodel.save(\"word2vec_brown.model\")\n\n\n12.2.5 Step 5: Exploring the Word Embeddings\nOnce the model is trained, you can explore the word embeddings. You can find words similar to a given word, check the cosine similarity between words, and more.\n\n12.2.5.1 Finding the Most Similar Words\n# Load the trained model\nmodel = Word2Vec.load(\"word2vec_brown.model\")\n\n# Find words most similar to 'king'\nsimilar_words = model.wv.most_similar('king')\nprint(similar_words)\n\n\n12.2.5.2 Word Vector Representation\nEach word is represented by a vector. You can see the vector representation of a specific word.\n# Get the vector for a specific word\nvector = model.wv['king']\nprint(vector)\n\n\n12.2.5.3 Cosine Similarity Between Words\nCosine similarity is commonly used to measure the similarity between word vectors.\n# Calculate cosine similarity between two words\nsimilarity = model.wv.similarity('king', 'queen')\nprint(f\"Cosine similarity between 'king' and 'queen': {similarity}\")\n\n\n\n12.2.6 Step 6: Visualizing Word Embeddings (Optional)\nTo visualize word embeddings in a 2D space, we can use dimensionality reduction techniques like PCA (Principal Component Analysis) or t-SNE. Below is an example using PCA.\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\n\n# Select a few words to visualize\nwords = ['king', 'queen', 'man', 'woman', 'dog', 'cat']\nword_vectors = [model.wv[word] for word in words]\n\n# Reduce the dimensionality of word vectors using PCA\npca = PCA(n_components=2)\nword_vectors_2d = pca.fit_transform(word_vectors)\n\n# Plot the 2D word vectors\nplt.figure(figsize=(10, 5))\nfor i, word in enumerate(words):\n    plt.scatter(word_vectors_2d[i, 0], word_vectors_2d[i, 1])\n    plt.text(word_vectors_2d[i, 0] + 0.05, word_vectors_2d[i, 1] + 0.05, word)\nplt.show()\nOutput:\n\n\n\n12.2.7 Conclusion\nIn this tutorial, we explored the Word2Vec model using Gensim in Python. We trained a Word2Vec model using the Brown corpus, explored word embeddings, and visualized the vectors in a 2D space. Word embeddings are a powerful tool for capturing the semantic relationships between words and are widely used in natural language processing tasks.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Word Embedding and Relational Similarity</span>"
    ]
  },
  {
    "objectID": "12.Word_Embedding.html#vector-similarity",
    "href": "12.Word_Embedding.html#vector-similarity",
    "title": "12  Word Embedding and Relational Similarity",
    "section": "12.3 Vector Similarity",
    "text": "12.3 Vector Similarity\nOnce text data is transformed into vectors (using methods like TF-IDF or word embeddings), we can compute similarity between documents or words. A common method for measuring similarity is Cosine Similarity, which measures the cosine of the angle between two vectors.\n\\[\\text{cosine similarity}(A, B) = \\frac{A \\cdot B}{\\|A\\| \\|B\\|} \\]\nWhere:\n\n\\(A \\cdot B\\) is the dot product of the vectors.\n\\(\\|A\\|\\) and \\(\\|B\\|\\) are the magnitudes (norms) of vectors (A) and (B).\n\n\n12.3.1 Example in Python (Cosine Similarity)\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport numpy as np\n\n# Example vectors (from TF-IDF or word embedding)\nvector_a = np.array([0.1, 0.3, 0.7])\nvector_b = np.array([0.2, 0.4, 0.5])\n\n# Compute cosine similarity\nsimilarity = cosine_similarity([vector_a], [vector_b])\nprint(f\"Cosine Similarity between vector_a and vector_b: {similarity[0][0]}\")\nOutput:\nCosine Similarity between vector_a and vector_b: 0.9509634325746505",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Word Embedding and Relational Similarity</span>"
    ]
  },
  {
    "objectID": "12.Word_Embedding.html#using-pre-trained-word-vectors-using-spacy",
    "href": "12.Word_Embedding.html#using-pre-trained-word-vectors-using-spacy",
    "title": "12  Word Embedding and Relational Similarity",
    "section": "12.4 Using Pre-trained Word Vectors (using spaCy)",
    "text": "12.4 Using Pre-trained Word Vectors (using spaCy)\nBelow is an example of how to compute document similarity using spaCy, which is a powerful NLP library. This code uses spaCy’s pre-trained word vectors to compute similarity between documents.\n\n12.4.1 Install spaCy and Download Language Model\nIf you haven’t already installed spaCy and its language model, you can do so using the following commands:\npip install spacy\npython -m spacy download en_core_web_md  # Medium-sized English model with word vectors\n\n\n12.4.2 Example Code: Document Similarity Using spaCy\nHere is a Python script that demonstrates how to compute similarity between two documents using spaCy:\nimport spacy\n\n# Load the spaCy language model (with word vectors)\nnlp = spacy.load('en_core_web_md')\n\n# Define two sample documents\ndoc1 = nlp(\"Natural language processing is a fascinating field of AI that focuses on the interaction between humans and computers through language.\")\ndoc2 = nlp(\"Machine learning is a part of AI that allows computers to learn from data without being explicitly programmed.\")\n\n# Compute similarity between the two documents\nsimilarity = doc1.similarity(doc2)\n\nprint(f\"Similarity between doc1 and doc2: {similarity:.4f}\")\n\n# Compare additional documents for similarity\ndoc3 = nlp(\"Understanding how humans speak is the first step in improving human-computer interaction.\")\nsimilarity_with_doc3 = doc1.similarity(doc3)\nprint(f\"Similarity between doc1 and doc3: {similarity_with_doc3:.4f}\")\n\ndoc4 = nlp(\"I love hiking in the mountains.\")\nsimilarity_with_doc4 = doc1.similarity(doc4)\nprint(f\"Similarity between doc1 and doc4: {similarity_with_doc4:.4f}\")\n\n\n12.4.3 Explanation:\n\nLoading the Model: We load the medium-sized en_core_web_md model in spaCy, which contains word vectors necessary for computing similarity.\nCreating Documents: We create document objects (doc1, doc2, etc.) by passing strings to the nlp object, which processes them.\nComputing Similarity: The doc1.similarity(doc2) method computes the similarity between two documents using word vectors. This similarity score is a number between 0 and 1, where 1 indicates perfect similarity.\nAdditional Comparisons: We also compare doc1 with doc3 and doc4 to see how similar different texts are.\n\n\n\n12.4.4 Output:\nYou will get an output that shows similarity scores for each pair of documents. A higher similarity score means the documents are more closely related in terms of their content.\nSimilarity between doc1 and doc2: 0.9172\nSimilarity between doc1 and doc3: 0.9096\nSimilarity between doc1 and doc4: 0.7760\n\n\n12.4.5 Notes:\n\nSimilarity Score: This is computed based on the semantic content of the text using pre-trained word vectors. It considers the meaning of words, so two documents that use different words but are semantically similar should have a high similarity score.\nWord Vector Model: You could also use a larger or smaller language model depending on your needs (en_core_web_sm for a small model, or en_core_web_lg for a large model).\n\n\n\n12.4.6 Summary:\nThis code computes the similarity between different documents using spaCy’s pre-trained word vectors. You can use this method to compare documents in various NLP tasks, such as document clustering, summarization, or recommendation systems.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Word Embedding and Relational Similarity</span>"
    ]
  },
  {
    "objectID": "12.Word_Embedding.html#spacys-vs.-gensims-word2vec",
    "href": "12.Word_Embedding.html#spacys-vs.-gensims-word2vec",
    "title": "12  Word Embedding and Relational Similarity",
    "section": "12.5 spaCy’s vs. Gensim’s Word2Vec",
    "text": "12.5 spaCy’s vs. Gensim’s Word2Vec\n\n12.5.0.1 Pre-trained vs Custom Training\n\nspaCy’s Pre-trained Word Vectors:\n\nPre-trained: spaCy comes with pre-trained word vectors as part of its language models, such as en_core_web_md and en_core_web_lg. These vectors have already been trained on large datasets (like Common Crawl) and are ready to use out of the box.\nCannot Retrain Easily: spaCy is designed primarily for loading and using pre-trained models. While it is possible to integrate custom vectors into spaCy, it’s not built for training word vectors from scratch.\n\nGensim Word2Vec:\n\nCustomizable Training: Gensim’s Word2Vec is a flexible tool that allows you to train word vectors on your own corpus. You can adjust the training parameters (e.g., window, vector_size, etc.) to suit your specific needs.\nNo Pre-trained Models: Gensim provides the implementation of Word2Vec, but you need to either train the model on your data or use pre-trained vectors from external sources like Google News.\n\n\n\n\n12.5.0.2 Training Algorithm\n\nspaCy:\n\nSpaCy uses pre-trained models that are typically trained using Word2Vec-like or FastText-like algorithms, but the specifics may vary depending on the source of the vectors.\nSpaCy’s pre-trained models also include other components such as dependency parsers, named entity recognizers, etc., in addition to word vectors.\n\nGensim Word2Vec:\n\nGensim implements the original Word2Vec algorithm as described by Mikolov et al. It offers two main algorithms:\n\nSkip-gram: Predicts surrounding words given a target word.\nCBOW (Continuous Bag of Words): Predicts a target word from surrounding words.\n\nIt is a more specific tool aimed at training word vectors.\n\n\n\n\n12.5.0.3 Vector Quality and Size\n\nspaCy:\n\nSpaCy offers models like en_core_web_sm, en_core_web_md, and en_core_web_lg:\n\nsm (small model): Does not contain word vectors and only includes word embeddings based on context.\nmd (medium model): Includes word vectors, but they are smaller and less fine-grained (300 dimensions).\nlg (large model): Contains more detailed word vectors and a larger vocabulary (300 dimensions).\n\nThe quality of vectors depends on the size of the model you load.\n\nGensim Word2Vec:\n\nGensim allows you to specify the vector size, the window size, the min_count (to ignore infrequent words), and other hyperparameters during training. This means you can control the trade-off between model complexity and the quality of word embeddings.\nGensim also allows for training on domain-specific corpora, which can give higher-quality word embeddings for specific use cases (e.g., medical, legal text).\n\n\n\n\n12.5.0.4 Use Case Differences\n\nspaCy:\n\nBest for Pre-trained NLP Pipelines: SpaCy is primarily used for building NLP pipelines and includes tools for tokenization, part-of-speech tagging, dependency parsing, named entity recognition, and more. Word vectors are just one part of spaCy’s broader functionality.\nOut-of-the-Box Similarity: If you need to compute similarities between documents or words quickly without training your own model, spaCy’s pre-trained vectors are an excellent choice.\n\nGensim Word2Vec:\n\nBest for Custom Training on Specific Domains: If you have a large corpus specific to your domain (e.g., legal, medical, or technical text) and you want to train your own word embeddings, Gensim’s Word2Vec is the tool to use.\nFlexible and Customizable: Gensim is ideal when you want to experiment with different training parameters or create embeddings for specialized applications.\n\n\n\n\n12.5.0.5 Performance and Speed\n\nspaCy:\n\nSpaCy is optimized for high performance. When using pre-trained vectors, it’s designed to be fast, especially for inference tasks like similarity comparisons.\n\nGensim Word2Vec:\n\nGensim is optimized for large-scale training and can work with very large corpora efficiently. However, training from scratch on large datasets can take some time depending on the size of your data and chosen parameters.\n\n\n\n\n12.5.0.6 Additional Features\n\nspaCy:\n\nSpaCy’s models are multi-functional, including components for parsing, tagging, named entity recognition, etc. Word vectors are just one feature among many.\nSpaCy focuses on providing an end-to-end pipeline for various NLP tasks, making it a comprehensive tool for many tasks, but less specialized in word vector training.\n\nGensim Word2Vec:\n\nGensim is a specialized tool for generating word embeddings and working with them. While it does not provide the full NLP pipeline like spaCy, it excels in generating custom word embeddings and performing tasks like topic modeling, document similarity, and more.\n\n\nIn summary, spaCy’s pre-trained word vectors are great for general-purpose applications and fast deployment, while Gensim’s Word2Vec is better for training custom embeddings on specialized data.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Word Embedding and Relational Similarity</span>"
    ]
  },
  {
    "objectID": "12.Word_Embedding.html#relational-similarity",
    "href": "12.Word_Embedding.html#relational-similarity",
    "title": "12  Word Embedding and Relational Similarity",
    "section": "12.6 Relational Similarity",
    "text": "12.6 Relational Similarity\nThe famous example (see https://arxiv.org/abs/1509.01692) of “King” - “man” + “woman” = “Queen” demonstrates how word embeddings can capture semantic relationships between words. This is often referred to as vector arithmetic or analogy in word embeddings.\nYou can use both Gensim’s Word2Vec and spaCy to demonstrate this kind of analogy. Below is an example using both libraries:\n\n12.6.1 Using Gensim Word2Vec\nIf you already have a pre-trained Word2Vec model (like Google News vectors or one trained on your own corpus), you can use Gensim to perform the analogy.\nimport gensim.downloader as api\n\n# Load pre-trained Word2Vec model (Google News vectors)\n# This might take a few minutes to download (around 1.6GB)\nmodel = api.load(\"word2vec-google-news-300\")\n\n# Perform the vector arithmetic: King - Man + Woman = ?\nresult = model.most_similar(positive=['king', 'woman'], negative=['man'])\n\nprint(f\"Result: {result[0]}\")\nExplanation:\n\nPositive Words: Words that should be added (king and woman).\nNegative Words: Words that should be subtracted (man).\nResult: This will return the top word that matches the resulting vector after the arithmetic operation.\n\nExample Output:\nResult: ('queen', 0.7118192911148071)\n\nThis indicates that the word closest to the result of the vector arithmetic “King” - “man” + “woman” is “Queen” with a similarity score of 0.71.\n\n\n12.6.2 Using spaCy\nIf you are using spaCy with a pre-trained model, such as en_core_web_md or en_core_web_lg, you can perform similar vector arithmetic.\nimport spacy\n\n# Load spaCy medium or large model (that includes word vectors)\n#nlp = spacy.load(\"en_core_web_md\")\nnlp = spacy.load(\"en_core_web_lg\")\n\n# Get vectors for words\nking = nlp.vocab['king'].vector\nman = nlp.vocab['man'].vector\nwoman = nlp.vocab['woman'].vector\n\n# Perform vector arithmetic: King - Man + Woman\nresult_vector = king - man + woman\n\n# Find the word closest to the result vector\nsimilar_word = None\nmax_similarity = -1\n\nfor word in nlp.vocab:\n    if word.has_vector and word.is_lower and word.is_alpha:\n        similarity = result_vector.dot(word.vector)\n        if similarity &gt; max_similarity:\n            max_similarity = similarity\n            similar_word = word\n\nprint(f\"Result: {similar_word.text}, Similarity: {max_similarity}\")\nExplanation:\n\nVector Operations: We manually subtract and add the word vectors for king, man, and woman.\nFinding Similar Words: We compare the resulting vector to all other word vectors in the model’s vocabulary using a dot product to find the most similar word.\nPerformance: This might take a bit longer depending on your system since it loops through the entire vocabulary.\n\nExample Output:\nResult: woman, Similarity: 69.10123443603516\nBoth methods achieve the same goal of showing that “King” - “man” + “woman” results in a vector closest to “Queen”.\nEnd.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Word Embedding and Relational Similarity</span>"
    ]
  },
  {
    "objectID": "13.NLP_toolkit.html",
    "href": "13.NLP_toolkit.html",
    "title": "13  Word Embedding Activities",
    "section": "",
    "text": "13.0.1 Lexical Semantics and Word Meaning",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Word Embedding Activities</span>"
    ]
  },
  {
    "objectID": "13.NLP_toolkit.html#lexical-semantics-and-word-meaning-1",
    "href": "13.NLP_toolkit.html#lexical-semantics-and-word-meaning-1",
    "title": "13  Word Embedding Activities",
    "section": "13.1 Lexical Semantics and Word Meaning",
    "text": "13.1 Lexical Semantics and Word Meaning\nTo conduct synonym detection and word sense disambiguation using word embeddings in Python, we can use popular libraries like gensim, spaCy, or transformers that provide pre-trained word embeddings. Below are step-by-step examples for both synonym detection and word sense disambiguation.\n\n13.1.1 Synonym Detection Using Word Embeddings\nWe can detect synonyms by checking the similarity between word vectors in the embedding space. Here’s an example using the gensim library with the pre-trained Word2Vec model.\n\n13.1.1.1 Steps:\n\nInstall necessary libraries:\npip install gensim spacy\npython -m spacy download en_core_web_sm  # Download the English model for spaCy\nLoad pre-trained word embeddings and find similar words:\nimport gensim.downloader as api\n\n# Load a pre-trained Word2Vec model from Gensim\nmodel = api.load(\"word2vec-google-news-300\")  # A popular pre-trained word2vec model\n\n# Example word for synonym detection\nword = \"happy\"\n\n# Get top 5 most similar words to the target word\nsimilar_words = model.most_similar(word, topn=5)\n\nprint(f\"Top 5 synonyms for '{word}':\")\nfor similar_word, similarity_score in similar_words:\n    print(f\"{similar_word} ({similarity_score})\")\n\nThis will output the top 5 words that are most similar to “happy” based on their proximity in the embedding space.\nSample Output:\nTop 5 synonyms for 'happy':\njoyful (0.714)\ncheerful (0.701)\ncontent (0.689)\ndelighted (0.678)\nelated (0.665)\nTo filter out words that share the same part-of-speech (POS) as the target word when performing synonym detection, we need to combine the word embedding approach with POS tagging. This ensures that the similar words returned are not only semantically related but also belong to the same grammatical category (e.g., noun, verb, adjective).\nWe can achieve this by using a POS tagger from a library like spaCy, which allows us to tag words and filter out only those with the same POS as the target word.\nPython Code to Show All POS Tags in spaCy:\nimport spacy\nfrom spacy.symbols import POS\n\n# Load the spaCy English model\nnlp = spacy.load(\"en_core_web_sm\")\n\n# List all available POS tags in spaCy with their explanations\npos_tags = nlp.get_pipe(\"tagger\").labels\n\nprint(\"All available POS tags in spaCy:\")\nfor pos in pos_tags:\n    print(f\"{pos}: {spacy.explain(pos)}\")\nOutput:\nAll available POS tags in spaCy:\n$: symbol, currency\n'': closing quotation mark\n,: punctuation mark, comma\n-LRB-: left round bracket\n-RRB-: right round bracket\n.: punctuation mark, sentence closer\n:: punctuation mark, colon or ellipsis\nADD: email\nAFX: affix\nCC: conjunction, coordinating\nCD: cardinal number\nDT: determiner\nEX: existential there\nFW: foreign word\nHYPH: punctuation mark, hyphen\nIN: conjunction, subordinating or preposition\nJJ: adjective (English), other noun-modifier (Chinese)\nJJR: adjective, comparative\nJJS: adjective, superlative\nLS: list item marker\nMD: verb, modal auxiliary\nNFP: superfluous punctuation\nNN: noun, singular or mass\nNNP: noun, proper singular\nNNPS: noun, proper plural\nNNS: noun, plural\nPDT: predeterminer\nPOS: possessive ending\nPRP: pronoun, personal\nPRP$: pronoun, possessive\nRB: adverb\nRBR: adverb, comparative\nRBS: adverb, superlative\nRP: adverb, particle\nSYM: symbol\nTO: infinitival \"to\"\nUH: interjection\nVB: verb, base form\nVBD: verb, past tense\nVBG: verb, gerund or present participle\nVBN: verb, past participle\nVBP: verb, non-3rd person singular present\nVBZ: verb, 3rd person singular present\nWDT: wh-determiner\nWP: wh-pronoun, personal\nWP$: wh-pronoun, possessive\nWRB: wh-adverb\nXX: unknown\n_SP: whitespace\n``: opening quotation mark\n\nHere’s the revised version of the code, where the “word” and “pos” assignment are handled separately.\n\n\n13.1.1.2 Revised Python Code:\nimport gensim.downloader as api\nimport spacy\n\n# Load pre-trained Word2Vec model from Gensim\nmodel = api.load(\"word2vec-google-news-300\")\n\n# Load spaCy POS tagger\nnlp = spacy.load(\"en_core_web_sm\")\n# Define a function to get the POS tag of a word\ndef get_pos(word):\n    doc = nlp(word)\n    return doc[0].pos_  # Returns the POS tag of the word\n\n# Function to find synonyms with the same POS\ndef find_synonyms_with_same_pos(word, topn=10):\n    try:\n        # Get the POS of the target word\n        word_pos = get_pos(word)\n\n        # Get the most similar words from the model\n        similar_words = model.most_similar(word, topn=topn)\n\n        # Filter similar words by POS tag\n        filtered_words = [\n            (w, sim) for w, sim in similar_words if get_pos(w) == word_pos\n        ]\n\n        return filtered_words\n    except KeyError:\n        print(f\"Word '{word}' not found in the model vocabulary.\")\n        return []\nSeparate input box for word and POS tagging\nword = \"happy\"  # Define the target word\npos = get_pos(word)  # Get the POS tag for the target word\n\n# Find synonyms with the same POS\nsynonyms_with_same_pos = find_synonyms_with_same_pos(word, topn=10)\n\n# Output the result\nprint(f\"Synonyms for '{word}' with the same POS ({pos}):\")\nfor synonym, similarity in synonyms_with_same_pos:\n    print(f\"{synonym} ({similarity})\")\nExample Output:\nFor word = \"happy\", the output will be something like:\nSynonyms for 'happy' with the same POS (ADJ):\njoyful (0.714)\ncheerful (0.701)\ndelighted (0.678)\ncontent (0.689)\necstatic (0.662)\n\n\n\n13.1.2 Word Sense Disambiguation Using Contextual Embeddings\nWord sense disambiguation (WSD) can be done by using contextual word embeddings, where the meaning of a word is determined by its context. Here’s an example using the transformers library (BERT embeddings) from Hugging Face.\n\n13.1.2.1 Steps:\n\nInstall necessary libraries:\npip install transformers torch\nUse BERT to generate contextual embeddings:\nfrom transformers import BertTokenizer, BertModel\n# BertModel: This is the actual pre-trained BERT model used to generate embeddings.\n\nimport torch\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n# Load pre-trained BERT model and tokenizer\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n# BertTokenizer.from_pretrained('bert-base-uncased'): Loads a pre-trained tokenizer for BERT. The bert-base-uncased model is a smaller, lower-cased version of BERT (where all text is converted to lowercase).\nmodel = BertModel.from_pretrained('bert-base-uncased')\n# BertModel.from_pretrained('bert-base-uncased'): Loads the pre-trained BERT model. This model outputs hidden states (embeddings) that can be used for various NLP tasks.\n\n\n# Sentences with the ambiguous word \"bank\"\nsentence_1 = \"He went to the bank to deposit money.\"\nsentence_2 = \"The river bank was full of fish.\"\n\n# Tokenize and get embeddings for both sentences\ninputs_1 = tokenizer(sentence_1, return_tensors=\"pt\")\ninputs_2 = tokenizer(sentence_2, return_tensors=\"pt\")\n\n# tokenizer(sentence_1, return_tensors=\"pt\"): This tokenizes the sentences into BERT's format and returns a PyTorch tensor (pt stands for PyTorch). BERT needs input to be tokenized into a numerical form (token IDs) that it can process.\n# It converts each word into subwords (tokens) and creates corresponding token IDs.\n# The result is a tensor, which is an array containing the numerical representation of each token.\n\n\nwith torch.no_grad():\n    outputs_1 = model(**inputs_1)\n    outputs_2 = model(**inputs_2)\n\n# torch.no_grad(): This disables gradient calculations (used for training models). Here, it saves memory and speeds up computations since we only need forward passes through the model to get the embeddings.\n# outputs_1 = model(**inputs_1): This runs the tokenized input through the BERT model. The model outputs hidden states or embeddings for each token in the sentence.\n# The hidden state captures the meaning of each word in the context of the entire sentence.\n\nembedding_1 = outputs_1.last_hidden_state[0, 4, :]  # Word \"bank\" in sentence 1\nembedding_2 = outputs_2.last_hidden_state[0, 2, :]  # Word \"bank\" in sentence 2\n\n# Extract the embeddings for the word \"bank\" (assuming the word is at index 5 in both cases)\n# The output hidden states are of shape (batch_size, sequence_length, hidden_size), we take the last hidden state\n# outputs_1.last_hidden_state: The output of BERT contains hidden states for all tokens in the sentence. This has the shape (batch_size, sequence_length, hidden_size) where:\n## batch_size: The number of sentences (in this case, it's 1).\n## sequence_length: The number of tokens in the sentence.\n## hidden_size: The size of the hidden state vector (768 dimensions for BERT).\n# [0, 5, :]: We access the embedding for the token at index 5 in both sentences. BERT generates embeddings for each token in the sentence, and this line assumes that the word \"bank\" is at index 5. The : means that we're extracting all the 768 dimensions of the embedding.\n# Note: Token indexes might differ depending on tokenization, so in a real application, you should find the correct index of the word \"bank\".\n\n# Compute cosine similarity between embeddings\nsimilarity = cosine_similarity(embedding_1.unsqueeze(0), embedding_2.unsqueeze(0))\n\n# cosine_similarity(embedding_1.unsqueeze(0), embedding_2.unsqueeze(0)): This computes the cosine similarity between the two embeddings. Cosine similarity is a measure of similarity between two vectors based on their orientation (not magnitude). It ranges from -1 (completely opposite) to 1 (exactly the same), with 0 indicating no similarity.\n# unsqueeze(0): This adds an extra dimension to the embedding to make it a 2D tensor, as cosine_similarity expects the input to be 2D.\n\n\nprint(f\"Similarity between 'bank' in two different contexts: {similarity[0][0]}\")\n\nIn this example:\n\nWe take the word “bank” in two different contexts: one financial (bank to deposit money) and one geographical (river bank).\nBERT creates embeddings for the word based on its surrounding context.\nCosine similarity is computed between these embeddings to determine how similar the meanings of “bank” are in both contexts.\n\nSample Output:\nSimilarity between 'bank' in two different contexts: 0.37\nThis low similarity score suggests that the word “bank” has different meanings in these two contexts (financial institution vs. riverside).\n\nTo revise the above code and ensure that the correct index of the word “bank” is used in both sentences, we need to account for the way BERT tokenizes the input. BERT uses subword tokenization, meaning that words can sometimes be split into multiple tokens. To ensure we find the correct index of “bank”, we need to first tokenize the sentences, then search for the token ID that corresponds to “bank” within the tokenized input.\nHere’s how to revise the code:\n\n\n\n13.1.3 Revised Python Code:\nfrom transformers import BertTokenizer, BertModel\nimport torch\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n# Load pre-trained BERT model and tokenizer\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nmodel = BertModel.from_pretrained('bert-base-uncased')\n\n# Sentences with the ambiguous word \"bank\"\nsentence_1 = \"He went to the bank to deposit money.\"\nsentence_2 = \"The river bank was full of fish.\"\n\n# Tokenize and get embeddings for both sentences\ninputs_1 = tokenizer(sentence_1, return_tensors=\"pt\")\ninputs_2 = tokenizer(sentence_2, return_tensors=\"pt\")\n\n# Tokenized input with subword tokens\ntokens_1 = tokenizer.tokenize(sentence_1)\ntokens_2 = tokenizer.tokenize(sentence_2)\n\n# Find the index of the token \"bank\" in both tokenized sentences\nindex_1 = tokens_1.index(\"bank\")\nindex_2 = tokens_2.index(\"bank\")\nprint(index_1)\nprint(index_2)\n\n# Convert to tensor input for BERT\ninputs_1 = tokenizer(sentence_1, return_tensors=\"pt\")\ninputs_2 = tokenizer(sentence_2, return_tensors=\"pt\")\n\nwith torch.no_grad():\n    outputs_1 = model(**inputs_1)\n    outputs_2 = model(**inputs_2)\n\n# Extract the embeddings for the word \"bank\" using the correct index\nembedding_1 = outputs_1.last_hidden_state[0, index_1 + 1, :]  # +1 due to [CLS] token at index 0\nembedding_2 = outputs_2.last_hidden_state[0, index_2 + 1, :]  # +1 due to [CLS] token at index 0\n\n# Compute cosine similarity between embeddings\nsimilarity = cosine_similarity(embedding_1.unsqueeze(0), embedding_2.unsqueeze(0))\n\nprint(f\"Similarity between 'bank' in two different contexts: {similarity[0][0]}\")\nKey Changes:\n\nTokenization:\n\ntokenizer.tokenize(sentence_1): This tokenizes each sentence into subword tokens.\ntokens_1.index(\"bank\"): Finds the correct index of the word “bank” in the tokenized input.\n\nCorrect Index Adjustment:\n\nIn BERT’s input format, the sequence starts with a [CLS] token at index 0, so the actual index of the word “bank” is index + 1. This is why we add 1 to the token index to get the correct location in the hidden states.\n\nEmbedding Extraction:\n\nThe embedding for the word “bank” is extracted based on the calculated index in each sentence.\n\n\nExample Output:\nThis code should give you the similarity score for the word “bank” in the two different contexts. If the meanings are different (as expected here), the similarity score will be low.\nFor example:\n4\n2\nSimilarity between 'bank' in two different contexts: 0.43\nThis low similarity score indicates that the word “bank” has different meanings in these contexts (financial institution vs. riverside).",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Word Embedding Activities</span>"
    ]
  },
  {
    "objectID": "13.NLP_toolkit.html#corpus-linguistics-with-word-embeddings",
    "href": "13.NLP_toolkit.html#corpus-linguistics-with-word-embeddings",
    "title": "13  Word Embedding Activities",
    "section": "13.2 Corpus Linguistics with Word Embeddings",
    "text": "13.2 Corpus Linguistics with Word Embeddings\nIn corpus linguistics, word embeddings can be applied to two key tasks: collocation analysis and semantic similarity. Below are Python implementations for both activities, along with detailed explanations.\n\n13.2.1 Collocation Analysis Using Word Embeddings\nCollocations are word pairings that frequently occur together in a language and exhibit specific patterns. Word embeddings can help identify semantically related word pairs based on their proximity in vector space.\n\n13.2.1.1 Steps:\n\nLoad pre-trained word embeddings (such as Word2Vec).\nExtract word pairs (collocations) based on their co-occurrence and proximity in embedding space.\nSort the word pairs by their similarity score to identify common collocations.\n\n\n\n13.2.1.2 Python Code for Collocation Analysis:\nimport gensim.downloader as api\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n# Load a pre-trained Word2Vec model (Google News Vectors)\nmodel = api.load(\"word2vec-google-news-300\")\n\n# List of word pairs you want to analyze for collocation\nword_pairs = [\n    ('quick', 'fox'),\n    ('lazy', 'dog'),\n    ('king', 'queen'),\n    ('strong', 'weak'),\n    ('bank', 'money')\n]\n\n# Function to calculate cosine similarity between two words\ndef get_similarity(word1, word2):\n    try:\n        vec1 = model[word1]\n        vec2 = model[word2]\n        similarity = cosine_similarity([vec1], [vec2])[0][0]\n        return similarity\n    except KeyError:\n        return None  # If word not in vocabulary\n\n# Find collocations by calculating similarity\ncollocations = []\nfor word1, word2 in word_pairs:\n    similarity = get_similarity(word1, word2)\n    if similarity is not None:\n        collocations.append((word1, word2, similarity))\n\n# Sort by similarity score\ncollocations.sort(key=lambda x: x[2], reverse=True)\n\n# Display the collocations and their similarity scores\nprint(\"Collocations and their similarity scores:\")\nfor word1, word2, sim in collocations:\n    print(f\"{word1} - {word2}: {sim:.3f}\")\nExplanation:\n\nModel: This code uses a pre-trained Word2Vec model (word2vec-google-news-300), which contains embeddings for millions of words.\nCosine Similarity: The similarity between two word vectors is calculated using cosine similarity. This measures how closely two words are related based on their context.\nWord Pairs: The list word_pairs contains sample word pairs for which collocations are analyzed. You can modify this list to include more word pairs.\n\nSample Output:\nCollocations and their similarity scores:\nking - queen: 0.651\nquick - fox: 0.341\nlazy - dog: 0.295\nbank - money: 0.519\nstrong - weak: -0.012\n\nThe word pair “king” and “queen” shows a high similarity, indicating they are often collocates in contexts related to royalty or power.\nThe pair “strong” and “weak” has a very low (and even negative) similarity, suggesting that these are antonyms rather than collocates.\n\n\n\n\n13.2.2 2. Semantic Similarity in Corpora\nSemantic similarity is used to measure how similar two words, phrases, or sentences are in meaning. This can be used to compare texts across corpora (e.g., learner vs. native speaker corpora).\n\n13.2.2.1 Steps:\n\nUse word embeddings to compute similarity scores between words or phrases in two different corpora.\nAggregate the similarities to compare the semantic variation between the corpora.\n\n\n\n13.2.2.2 Python Code for Semantic Similarity in Corpora:\nfrom transformers import BertTokenizer, BertModel\nimport torch\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n# Load pre-trained BERT model and tokenizer for contextual embeddings\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nmodel = BertModel.from_pretrained('bert-base-uncased')\n\n# Example sentences from different corpora (Learner vs Native)\nsentence_learner = \"The cat sat on the mat.\"\nsentence_native = \"A feline rested on a carpet.\"\n\n# Tokenize the sentences and create input tensors for BERT\ninputs_learner = tokenizer(sentence_learner, return_tensors='pt')\ninputs_native = tokenizer(sentence_native, return_tensors='pt')\n\n# Pass the sentences through BERT to get hidden states\nwith torch.no_grad():\n    outputs_learner = model(**inputs_learner)\n    outputs_native = model(**inputs_native)\n\n# Extract the last hidden states for sentence embeddings\nembedding_learner = outputs_learner.last_hidden_state.mean(dim=1)  # Mean pooling for sentence embedding\nembedding_native = outputs_native.last_hidden_state.mean(dim=1)\n\n# Compute cosine similarity between the sentence embeddings\nsimilarity = cosine_similarity(embedding_learner, embedding_native)[0][0]\n\nprint(f\"Semantic similarity between the sentences: {similarity:.3f}\")\nExplanation:\n\nBERT Model: We use a pre-trained BERT model (bert-base-uncased) to compute contextual word embeddings. BERT captures the meaning of a word or sentence in the context of surrounding words.\nSentence Embedding: BERT outputs embeddings for each token in the sentence. We use mean pooling to combine these token embeddings into a single vector representing the entire sentence.\nCosine Similarity: Cosine similarity is used to compute how similar the two sentences are in meaning.\n\nSample Output:\nSemantic similarity between the sentences: 0.781\n\nThe two sentences “The cat sat on the mat.” (learner corpus) and “A feline rested on a carpet.” (native corpus) have a high similarity score (0.781), showing that despite lexical differences, their meanings are quite similar.\n\n\n\n\n13.2.3 Use Cases for Both Tasks in Corpus Linguistics:\n\nCollocation Analysis:\n\nLexicography: Identify common collocations for dictionary creation or teaching materials.\nLanguage Teaching: Help learners understand frequent word pairings and idiomatic expressions.\n\nSemantic Similarity in Corpora:\n\nLearner Corpora: Compare learner-generated texts with native speaker texts to assess the semantic proximity and linguistic variation.\nTextual Analysis: Measure how similar different versions of texts are, or compare writing from different authors or genres.\n\n\nBy applying these techniques, researchers can study patterns in natural language usage, how meanings vary across corpora, and how words co-occur in different contexts.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Word Embedding Activities</span>"
    ]
  },
  {
    "objectID": "13.NLP_toolkit.html#discourse-and-pragmatics-using-word-embeddings-python-code-and-explanations",
    "href": "13.NLP_toolkit.html#discourse-and-pragmatics-using-word-embeddings-python-code-and-explanations",
    "title": "13  Word Embedding Activities",
    "section": "13.3 Discourse and Pragmatics Using Word Embeddings: Python Code and Explanations",
    "text": "13.3 Discourse and Pragmatics Using Word Embeddings: Python Code and Explanations\nIn discourse and pragmatics, topic modeling and contextual meaning and coherence are key tasks. Below are Python implementations for each task, focusing on using word embeddings to uncover topics and assess coherence in text.\n\n13.3.1 Topic Modeling Using Embedding-Based Approaches\nTopic modeling is a technique for identifying hidden themes or topics within a collection of documents. Embedding-based models, like BERTopic, can generate clusters of semantically related words that represent underlying topics.\n\n13.3.1.1 Steps:\n\nPreprocess a collection of documents.\nUse a pre-trained embedding model to transform text into vectors.\nApply topic modeling using an embedding-based approach like BERTopic.\n\n\n\n13.3.1.2 Python Code for Topic Modeling Using BERTopic:\nYou need to install BERTopic and sentence-transformers first:\npip install bertopic sentence-transformers\nNow the Python code:\nfrom bertopic import BERTopic\nfrom sklearn.datasets import fetch_20newsgroups\n\n# Fetch sample data (20 newsgroups dataset for topic modeling)\ndata = fetch_20newsgroups(subset='all')['data']\n\n# Initialize BERTopic model (uses embedding-based topic modeling)\ntopic_model = BERTopic()\n\n# Fit the topic model on the dataset\ntopics, probabilities = topic_model.fit_transform(data)\n\n# Display the top 5 topics\ntopic_info = topic_model.get_topic_info()\nprint(topic_info.head())\n\n# Get the top words for a specific topic\ntopic_id = 0  # You can change this to explore different topics\ntop_words = topic_model.get_topic(topic_id)\nprint(f\"Top words for topic {topic_id}: {top_words}\")\nExplanation:\n\nBERTopic: BERTopic is a topic modeling library that leverages pre-trained sentence embeddings to find topics in large corpora.\nEmbedding Transformation: It uses embeddings to capture the semantic meaning of each document and then clusters these embeddings to identify topics.\nOutput:\ntopic_info: This provides a list of all topics discovered by the model, along with the size of each topic (i.e., how many documents are classified under each topic).\nget_topic(): This function returns the top words for a particular topic, providing insights into the core vocabulary related to that topic.\n\nSample Output:\nTop 5 topics:\n   Topic  Count\n0     -1   7285\n1      0   1121\n2      1    874\n3      2    797\n4      3    726\n\nTop words for topic 0: [('space', 0.03), ('nasa', 0.02), ('launch', 0.015), ('mission', 0.014), ('orbit', 0.013)]\nIn this example, Topic 0 might be related to space exploration, as evidenced by the most prominent words: “space”, “nasa”, “launch”, “mission”, “orbit”.\n\n\n\n13.3.2 Contextual Meaning and Coherence Assessment\nTo assess coherence in a text, we can analyze the semantic similarity between consecutive sentences. Cohesive and coherent texts tend to have sentences that are contextually related, whereas disjointed texts may exhibit lower similarity scores between sentences.\n\n13.3.2.1 Steps:\n\nPreprocess the text into sentences.\nUse pre-trained sentence embeddings (e.g., BERT) to compute sentence vectors.\nCalculate the similarity between consecutive sentences to assess coherence.\n\n\n\n13.3.2.2 Python Code for Contextual Meaning and Coherence Using BERT:\nYou need to install transformers for sentence embeddings:\npip install transformers torch\nNow the Python code:\nfrom transformers import BertTokenizer, BertModel\nimport torch\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport nltk\n\n# Download the NLTK sentence tokenizer\nnltk.download('punkt')\n\n# Load pre-trained BERT model and tokenizer\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nmodel = BertModel.from_pretrained('bert-base-uncased')\n\n# Sample text for coherence analysis\ntext = \"\"\"\nThe cat sat on the mat. It was a sunny day. The dog barked at the cat. The mat was clean and soft.\nThe weather changed abruptly. There was a sudden storm, and everyone rushed inside.\n\"\"\"\n\n# Tokenize the text into sentences\nsentences = nltk.sent_tokenize(text)\n\n# Function to get the sentence embedding from BERT\ndef get_sentence_embedding(sentence):\n    inputs = tokenizer(sentence, return_tensors='pt')\n    with torch.no_grad():\n        outputs = model(**inputs)\n    # Mean pooling of the hidden states to get sentence embedding\n    sentence_embedding = outputs.last_hidden_state.mean(dim=1)\n    return sentence_embedding\n\n# Compute embeddings for all sentences\nsentence_embeddings = [get_sentence_embedding(sentence) for sentence in sentences]\n\n# Compute cosine similarity between consecutive sentences\ncoherence_scores = []\nfor i in range(len(sentence_embeddings) - 1):\n    similarity = cosine_similarity(sentence_embeddings[i], sentence_embeddings[i + 1])[0][0]\n    coherence_scores.append(similarity)\n\n# Display coherence scores\nfor i, score in enumerate(coherence_scores):\n    print(f\"Coherence between sentence {i+1} and {i+2}: {score:.3f}\")\nExplanation:\n\nSentence Tokenization: The text is split into individual sentences using nltk.sent_tokenize().\nBERT Sentence Embedding: Each sentence is passed through BERT to obtain a sentence embedding, which is a dense vector representation capturing the semantic meaning of the entire sentence.\nCoherence Measurement: The coherence between consecutive sentences is measured using cosine similarity between their embeddings. A high similarity score means that the two sentences are contextually coherent, while a low score indicates a break in coherence.\n\nSample Output:\nCoherence between sentence 1 and 2: 0.721\nCoherence between sentence 2 and 3: 0.695\nCoherence between sentence 3 and 4: 0.891\nCoherence between sentence 4 and 5: 0.462\nCoherence between sentence 5 and 6: 0.853\nIn this example: - High coherence is observed between sentence pairs 3 & 4, and 5 & 6, indicating they are contextually related. - Lower coherence between sentences 4 & 5 suggests a possible topic shift or break in coherence, which could be a signal of abrupt transitions in discourse.\n\n\n\n13.3.3 Use Cases for Both Tasks in Discourse Analysis:\n\nTopic Modeling:\n\nDiscourse Studies: Identify themes in large-scale conversations (e.g., analyzing debates, interviews, or discussions).\nGenre Analysis: Discover the key topics or themes within specific genres of text (e.g., scientific articles, novels, news articles).\n\nContextual Meaning and Coherence:\n\nAutomatic Essay Scoring: Coherence scores can be used to evaluate how well a student’s essay flows from one sentence or paragraph to the next.\nDiscourse Analysis: Researchers can measure the cohesion within a text to better understand how well ideas are connected or if there are any sudden shifts in the narrative.\n\n\nThese tools offer a powerful way to apply word embeddings to uncover the structure and meaning within discourse, making them useful for both academic and practical applications in language analysis.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Word Embedding Activities</span>"
    ]
  },
  {
    "objectID": "13.NLP_toolkit.html#sociolinguistics-using-word-embeddings",
    "href": "13.NLP_toolkit.html#sociolinguistics-using-word-embeddings",
    "title": "13  Word Embedding Activities",
    "section": "13.4 Sociolinguistics Using Word Embeddings",
    "text": "13.4 Sociolinguistics Using Word Embeddings\nIn sociolinguistics, word embeddings can be used to analyze variation and change over time and across dialects or sociolects. Below are Python implementations for both tasks, along with detailed explanations.\n\n13.4.1 Variation and Change in Word Usage\nResearchers can track changes in word meanings or usage across time periods or linguistic communities by training word embeddings on different subsets of data (e.g., text from different decades or regions). By comparing embeddings of the same word across different time periods or regions, researchers can observe how word meanings shift.\n\n13.4.1.1 Steps:\n\nDivide the text corpus into different time periods (or other sociolinguistic factors like regions or social groups).\nTrain or load pre-trained word embeddings for each time period.\nCompare the embeddings of a target word across the time periods to detect changes in meaning.\n\n\n\n13.4.1.2 Python Code for Tracking Variation and Change:\n\n13.4.1.2.1 Download Pre-Trained Word2Vec Models\nIf you do not have the word2vec-google-news-300.bin file, you can use pre-trained word embeddings from gensim. You can load models like the Google News Word2Vec embeddings, which are available through gensim.\nHere’s an example of how to load a pre-trained model from Gensim’s downloader:\nimport gensim.downloader as api\n\n# Load a pre-trained Word2Vec model (e.g., Google News embeddings)\nmodel_google = api.load(\"word2vec-google-news-300\")  # Use this for the 1990s model\nmodel_wiki = api.load(\"fasttext-wiki-news-subwords-300\")  # Use the same for modern-day comparison\nimport gensim\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport gensim.downloader as api\nprint(list(api.info()['models'].keys()))\n['fasttext-wiki-news-subwords-300', 'conceptnet-numberbatch-17-06-300', 'word2vec-ruscorpora-300', 'word2vec-google-news-300', 'glove-wiki-gigaword-50', 'glove-wiki-gigaword-100', 'glove-wiki-gigaword-200', 'glove-wiki-gigaword-300', 'glove-twitter-25', 'glove-twitter-50', 'glove-twitter-100', 'glove-twitter-200', '__testing_word2vec-matrix-synopsis']\n# Load a pre-trained Word2Vec model (e.g., Google News embeddings)\nmodel_google = api.load(\"word2vec-google-news-300\")\nmodel_wiki = api.load(\"fasttext-wiki-news-subwords-300\")  \n\n# Function to compare word embeddings across context\ndef compare_word_across_time(word, model1, model2):\n    try:\n        # Get embeddings for the word from both time periods\n        vector1 = model1[word]\n        vector2 = model2[word]\n        \n        # Calculate cosine similarity to see how the word meaning has changed\n        similarity = cosine_similarity([vector1], [vector2])[0][0]\n        return similarity\n    except KeyError:\n        return f\"'{word}' not found in one of the models.\"\n\n# Example usage: Track how the meaning of the word 'cloud' is different between Google and Twitter\nword = 'cloud'\nsimilarity_score = compare_word_across_time(word, model_google, model_wiki)\n\nprint(f\"Semantic similarity for '{word}' between Google and Wiki: {similarity_score:.3f}\")\nSample Output:\nSemantic similarity for 'cloud' between Google and Wiki: -0.035\nIn this case, the similarity score suggests a moderate shift in the meaning of the word “cloud.”\n\n\n13.4.1.2.2 Load a pre-trained model\nEnsure that the file 'word2vec-google-news-300.bin' exists and the path is correctly specified. If the file is stored in a different directory, make sure to provide the absolute path to the file.\nFor example:\nmodel_google = gensim.models.KeyedVectors.load_word2vec_format('/path/to/your/word2vec-google-news-300.bin', binary=True)\n\n\n\n13.4.1.3 Train Your Own Embedding Model (Optional)\nIf you want to specifically train word embeddings on corpora from different time periods, you can use Gensim’s Word2Vec to train models on your own text data from Google and Twitter.\nHere’s a basic example of how to train a Word2Vec model:\nfrom gensim.models import Word2Vec\n\n# Assuming `Google` and `Twitter` are lists of tokenized sentences from your corpus\n# Example: sentences_Google = [[\"word1\", \"word2\", \"word3\"], [\"word4\", \"word5\"]]\n\n# Train Word2Vec models on your corpus\nmodel_google = Word2Vec(sentences_google, vector_size=300, window=5, min_count=1, workers=4)\nmodel_wiki = Word2Vec(sentences_wiki, vector_size=300, window=5, min_count=1, workers=4)\n\n# Save the models\nmodel_google.wv.save_word2vec_format('model_google.bin', binary=True)\nmodel_wiki.wv.save_word2vec_format('model_wiki.bin', binary=True)\nYou will need large corpora for Google and Wiki to train accurate embeddings. Once trained, you can load and use these models as in your original code.\n\n\n\n13.4.2 Dialectology Using Word Embeddings\nIn dialectology, researchers can compare the embeddings of words across dialects or sociolects to quantify linguistic similarities and differences. By training word embeddings on corpora representing different dialects, the embeddings for the same word can be compared to reveal how meanings or usages differ.\n\n13.4.2.1 Steps:\n\nTrain or load pre-trained word embeddings for different dialects or sociolects.\nCompare the embeddings of the same word across dialects to measure their semantic similarity.\n\n\n\n13.4.2.2 Python Code for Comparing Dialects Using Word Embeddings:\nimport gensim\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n# Load or train embeddings for different dialects (example from two dialect corpora)\nmodel_us_english = gensim.models.KeyedVectors.load_word2vec_format('word2vec_us_english.bin', binary=True)\nmodel_uk_english = gensim.models.KeyedVectors.load_word2vec_format('word2vec_uk_english.bin', binary=True)\n\n# Function to compare word embeddings across dialects\ndef compare_word_across_dialects(word, model_dialect1, model_dialect2):\n    try:\n        # Get embeddings for the word from both dialects\n        vector_dialect1 = model_dialect1[word]\n        vector_dialect2 = model_dialect2[word]\n        \n        # Calculate cosine similarity to compare the meanings across dialects\n        similarity = cosine_similarity([vector_dialect1], [vector_dialect2])[0][0]\n        return similarity\n    except KeyError:\n        return f\"'{word}' not found in one of the models.\"\n\n# Example usage: Compare the meaning of 'boot' in US English and UK English\nword = 'boot'\nsimilarity_score = compare_word_across_dialects(word, model_us_english, model_uk_english)\n\nprint(f\"Semantic similarity for '{word}' between US and UK English: {similarity_score:.3f}\")\nExplanation:\n\nDifferent Dialect Embedding Models: Two Word2Vec models (model_us_english and model_uk_english) are trained on corpora from two different English dialects: American and British.\nCosine Similarity: The similarity between the word embeddings in different dialects indicates how similarly the word is used or understood. A high similarity score indicates that the word is used similarly, while a low score suggests a difference in usage or meaning.\nUsage Example: The word “boot” has different meanings in US English (referring to footwear) and UK English (referring to the trunk of a car).\n\nSample Output:\nSemantic similarity for 'boot' between US and UK English: 0.421\nIn this example, the word “boot” has a lower similarity score, reflecting its different meanings in the two dialects.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Word Embedding Activities</span>"
    ]
  },
  {
    "objectID": "13.NLP_toolkit.html#sentiment-and-politeness-analysis-using-word-embeddings",
    "href": "13.NLP_toolkit.html#sentiment-and-politeness-analysis-using-word-embeddings",
    "title": "13  Word Embedding Activities",
    "section": "13.5 Sentiment and Politeness Analysis Using Word Embeddings:",
    "text": "13.5 Sentiment and Politeness Analysis Using Word Embeddings:\nWord embeddings can be used to analyze both politeness or formality levels and sentiment analysis. These tasks are important for understanding the pragmatic and affective aspects of language in different contexts, such as social interactions or cross-cultural communication.\n\n13.5.1 Politeness or Formality Levels Using Word Embeddings\nIn this task, we aim to assess the politeness or formality of text by measuring how closely words or phrases align with known politeness/formality markers in the embedding space. We can create a word embedding-based model to compare the text with words commonly associated with politeness or formality.\n\n13.5.1.1 Steps:\n\nCreate or load word embeddings (Word2Vec, BERT).\nDefine a set of words or phrases commonly associated with politeness or formality (e.g., “please”, “thank you”, “sir”).\nCompute the similarity between the words in the input text and the politeness markers.\n\n\n\n13.5.1.2 Python Code for Politeness/Formality Level Detection:\nimport gensim.downloader as api\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n# Load pre-trained Word2Vec model\nmodel = api.load(\"word2vec-google-news-300\")\n\n# Define a list of politeness or formality markers\npoliteness_markers = [\"please\", \"thank you\", \"sir\", \"madam\", \"kindly\", \"would you\", \"may I\"]\n\n# Function to assess the politeness of a given sentence\ndef assess_politeness(sentence, markers, model):\n    # Tokenize the sentence (simplified)\n    words = sentence.lower().split()\n    \n    # Check similarity to politeness markers\n    politeness_score = 0\n    for word in words:\n        try:\n            word_vector = model[word]\n            # Calculate similarity with each politeness marker\n            for marker in markers:\n                marker_vector = model[marker]\n                similarity = cosine_similarity([word_vector], [marker_vector])[0][0]\n                politeness_score += similarity\n        except KeyError:\n            # Word not in the model vocabulary\n            pass\n    \n    # Normalize politeness score by the number of words\n    return politeness_score / len(words) if len(words) &gt; 0 else 0\n\n# Example usage\nsentence = \"Would you kindly help me with this task, sir?\"\npoliteness_score = assess_politeness(sentence, politeness_markers, model)\n\nprint(f\"Politeness score for the sentence: {politeness_score:.3f}\")\nExplanation:\n\nPoliteness Markers: We define a list of words or phrases typically associated with politeness or formality.\nCosine Similarity: For each word in the input sentence, we compute its similarity to each politeness marker using cosine similarity. The higher the similarity score, the more polite or formal the sentence is likely to be.\nPoliteness Score: We aggregate the similarities across all words in the sentence to compute a “politeness score.”\n\nSample Output:\nPoliteness score for the sentence: 0.219\n\n\n\n13.5.2 Sentiment Analysis Using Word Embeddings\nSentiment analysis involves classifying the emotional tone of a text (e.g., positive, negative, neutral). By leveraging word embeddings, we can calculate the semantic similarity between words in the input text and words that are commonly associated with positive or negative sentiments.\n\n13.5.2.1 Steps:\n\nLoad pre-trained word embeddings.\nDefine a set of words associated with positive and negative sentiment.\nCompute the similarity between the words in the input text and the sentiment markers.\n\n\n\n13.5.2.2 Python Code for Sentiment Analysis:\n# Define a list of positive and negative sentiment markers\npositive_markers = [\"good\", \"happy\", \"joy\", \"love\", \"excellent\", \"amazing\"]\nnegative_markers = [\"bad\", \"sad\", \"angry\", \"hate\", \"terrible\", \"horrible\"]\n\n# Function to assess the sentiment of a given sentence\ndef assess_sentiment(sentence, pos_markers, neg_markers, model):\n    # Tokenize the sentence (simplified)\n    words = sentence.lower().split()\n\n    # Initialize sentiment scores\n    positive_score = 0\n    negative_score = 0\n\n    # Check similarity to sentiment markers\n    for word in words:\n        try:\n            word_vector = model[word]\n            # Compare with positive markers\n            for pos_word in pos_markers:\n                pos_vector = model[pos_word]\n                similarity = cosine_similarity([word_vector], [pos_vector])[0][0]\n                positive_score += similarity\n            # Compare with negative markers\n            for neg_word in neg_markers:\n                neg_vector = model[neg_word]\n                similarity = cosine_similarity([word_vector], [neg_vector])[0][0]\n                negative_score += similarity\n        except KeyError:\n            # Word not in the model vocabulary\n            pass\n    \n    # Determine overall sentiment based on which score is higher\n    sentiment = \"Positive\" if positive_score &gt; negative_score else \"Negative\"\n    return sentiment, positive_score, negative_score\n\n# Example usage\nsentence = \"I love this amazing product!\"\nsentiment, pos_score, neg_score = assess_sentiment(sentence, positive_markers, negative_markers, model)\n\nprint(f\"Sentiment: {sentiment}\")\nprint(f\"Positive score: {pos_score:.3f}, Negative score: {neg_score:.3f}\")\nSample Output:\nSentiment: Positive\nPositive score: 7.896, Negative score: 6.310\nIn this example, the sentence “I love this amazing product!” is classified as positive based on its higher similarity to positive sentiment markers like “love” and “amazing.”\nBy leveraging word embeddings, we can analyze both the politeness and sentiment of text in a nuanced way, providing insights into how language is used to convey emotions, politeness, and formality across different contexts.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Word Embedding Activities</span>"
    ]
  },
  {
    "objectID": "13.NLP_toolkit.html#translation-and-bilingual-word-embeddings-1",
    "href": "13.NLP_toolkit.html#translation-and-bilingual-word-embeddings-1",
    "title": "13  Word Embedding Activities",
    "section": "13.6 Translation and Bilingual Word Embeddings",
    "text": "13.6 Translation and Bilingual Word Embeddings\nBilingual word embeddings enable us to map words from different languages into a shared semantic space, facilitating cross-linguistic analysis and error detection in translation. These embeddings allow words in different languages that have similar meanings to be located close to each other in the shared space, which is useful for tasks like machine translation and semantic analysis across languages.\nIn cross-linguistic analysis, we use bilingual embeddings to map words from two different languages into a shared semantic space. This allows us to study how similar words in different languages are semantically related.\n\n13.6.0.1 Steps:\n\nLoad bilingual word embeddings for two languages.\nCompare the embeddings of words from different languages in the shared space.\nCalculate similarity between words to identify cross-linguistic semantic similarity.\n\nHere’s the revised Python code for both cross-linguistic analysis and translation error detection using English and Korean word embeddings:\n\n\n13.6.1 Cross-Linguistic Analysis Between English and Korean Using Bilingual Word Embeddings\n# First, install fastText if not already installed\npip install fasttext\nimport fasttext\nimport fasttext.util\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n# Load pre-trained fastText models for English and Korean\nfasttext.util.download_model('en', if_exists='ignore')  # English\nfasttext.util.download_model('ko', if_exists='ignore')  # Korean\nmodel_en = fasttext.load_model('cc.en.300.bin')\nmodel_ko = fasttext.load_model('cc.ko.300.bin')\n\n# Function to compare cross-linguistic similarity between English and Korean words\ndef cross_linguistic_similarity(word_en, word_ko, model_en, model_ko):\n    # Get word embeddings\n    vec_en = model_en.get_word_vector(word_en)\n    vec_ko = model_ko.get_word_vector(word_ko)\n    \n    # Compute cosine similarity\n    similarity = cosine_similarity([vec_en], [vec_ko])[0][0]\n    return similarity\n# Example usage: Compare 'apple' (English) and '사과' (Korean)\nword_en = 'apple'\nword_ko = '사과'  # 사과 (sagwa) means \"apple\" in Korean\nsimilarity_score = cross_linguistic_similarity(word_en, word_ko, model_en, model_ko)\n\nprint(f\"Similarity between '{word_en}' (English) and '{word_ko}' (Korean): {similarity_score:.3f}\")\nExplanation:\n\nfastText Models: We load pre-trained fastText models for English and Korean. These models map words from both languages into a shared 300-dimensional space.\nCross-Linguistic Similarity: The function calculates cosine similarity between the embeddings of an English word and its Korean translation.\n\nSample Output:\nSimilarity between 'apple' (English) and '사과' (Korean): 0.811\nIn this example, ‘apple’ (English) and ‘사과’ (Korean) have a high similarity score, indicating they are semantically related across languages.\n\n\n13.6.2 Translation Error Detection Between English and Korean\nWe can use bilingual word embeddings to detect translation errors by comparing the embeddings of English words with their Korean translations. If the similarity is below a threshold, the translation may be incorrect.\n\n13.6.2.1 Revised Python Code for Translation Error Detection Between English and Korean:\n# Function to detect errors in translation using bilingual word embeddings\ndef detect_translation_error(source_word, target_word, model_source, model_target, threshold=0.6):\n    # Get embeddings for source and target words\n    vec_source = model_source.get_word_vector(source_word)\n    vec_target = model_target.get_word_vector(target_word)\n    \n    # Compute cosine similarity\n    similarity = cosine_similarity([vec_source], [vec_target])[0][0]\n    \n    # Determine if there is a potential translation error\n    if similarity &lt; threshold:\n        return f\"Potential translation error: '{source_word}' and '{target_word}' have low similarity ({similarity:.3f}).\"\n    else:\n        return f\"Translation seems correct: '{source_word}' and '{target_word}' are semantically similar ({similarity:.3f}).\"\n\n# Example usage: Detecting a possible translation error between 'car' (English) and '자동차' (Korean)\nsource_word = 'car'\ntarget_word = '자동차'  # 자동차 (jadongcha) means \"car\" in Korean\nerror_message = detect_translation_error(source_word, target_word, model_en, model_ko)\n\nprint(error_message)\n\n# Example usage: Detecting a potential translation error between 'car' (English) and '책' (Korean)\nsource_word = 'car'\ntarget_word = '책'  # 책 (chaek) means \"book\" in Korean (incorrect translation)\nerror_message = detect_translation_error(source_word, target_word, model_en, model_ko)\n\nprint(error_message)\nExplanation:\n\nTranslation Error Detection: This function compares the similarity between an English word and its Korean translation. If the similarity is below a given threshold (e.g., 0.6), the translation is flagged as potentially incorrect.\nThreshold: The threshold helps to determine the cutoff for acceptable similarity. A higher threshold will be more strict in detecting errors.\n\nSample Output:\nTranslation seems correct: 'car' and '자동차' are semantically similar (0.874).\nPotential translation error: 'car' and '책' have low similarity (0.218).\nIn the first example, ‘car’ (English) and ‘자동차’ (Korean) are correctly translated, whereas ‘car’ and ‘책’ (book) are not semantically similar, indicating a translation error.\nBy using bilingual word embeddings, we can perform effective cross-linguistic analysis and translation error detection between English and Korean, improving translation systems and understanding the semantic relationships between languages.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Word Embedding Activities</span>"
    ]
  },
  {
    "objectID": "A.assignments.html",
    "href": "A.assignments.html",
    "title": "15  Assignment #01",
    "section": "",
    "text": "15.1 Assignment 01: Web Scraping and YouTube Subtitle Extraction",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Assignment #01</span>"
    ]
  },
  {
    "objectID": "A.assignments.html#assignment-01-web-scraping-and-youtube-subtitle-extraction",
    "href": "A.assignments.html#assignment-01-web-scraping-and-youtube-subtitle-extraction",
    "title": "15  Assignment #01",
    "section": "",
    "text": "15.1.1 Total Marks: 20\n\n\n15.1.2 Objective:\nIn this assignment, you will use Python to perform web scraping and extract subtitles from YouTube playlists. You will scrape text from a website over multiple pages, process the text, and save it in an Excel file. Additionally, you will extract and process subtitles from videos in a YouTube playlist, saving the results in Excel format.\n\n\n15.1.3 Assignment Instructions:",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Assignment #01</span>"
    ]
  },
  {
    "objectID": "A.assignments.html#task-1-web-scraping-10-marks",
    "href": "A.assignments.html#task-1-web-scraping-10-marks",
    "title": "15  Assignment #01",
    "section": "15.2 Task 1: Web Scraping (10 Marks)",
    "text": "15.2 Task 1: Web Scraping (10 Marks)\n\n15.2.1 Step 1: Select a Website for Crawling\nSelect a website from which you will scrape data. The website must have multiple pages that can be crawled.\nExplain why you selected this website (e.g., relevance, data availability, structure).\n\n\n15.2.2 Step 2: Crawl Data from More than 2 Pages (4 Marks)\nWrite a Python script using requests, BeautifulSoup, or other relevant libraries to scrape data from at least two pages of the selected website. Ensure that your script can navigate between pages (pagination).\n\n\n15.2.3 Step 3: Preprocess the Scraped Text and Save to Excel (xlsx) (4 Marks)\n\nPreprocess the scraped text data (e.g., remove unnecessary characters, clean formatting).\nSave the processed text in an Excel file (.xlsx format) using pandas or openpyxl.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Assignment #01</span>"
    ]
  },
  {
    "objectID": "A.assignments.html#task-2-youtube-playlist-subtitle-extraction-10-marks",
    "href": "A.assignments.html#task-2-youtube-playlist-subtitle-extraction-10-marks",
    "title": "15  Assignment #01",
    "section": "15.3 Task 2: YouTube Playlist Subtitle Extraction (10 Marks)",
    "text": "15.3 Task 2: YouTube Playlist Subtitle Extraction (10 Marks)\n\n15.3.1 Step 1: Select a YouTube Playlist (2 Marks)\n\nSelect a YouTube playlist that contains videos with subtitles.\nExplain why you chose this playlist (e.g., topic relevance, video variety).\n\n\n\n15.3.2 Step 2: Extract Subtitles from More than 2 Videos (4 Marks)\n\nWrite a Python script using pytube, youtube-transcript-api, or other relevant libraries to extract subtitles from at least two videos in the playlist.\nPrint or log the extracted subtitles.\n\n\n\n15.3.3 Step 3: Preprocess Subtitles and Save to Excel (xlsx) (4 Marks)\n\nPreprocess the extracted subtitles (e.g., remove unnecessary characters, clean formatting, timestamp adjustments).\nSave the cleaned subtitles in an Excel file (.xlsx format) using pandas or openpyxl.\n\n\n\n\n15.3.4 Submission Guidelines:\n\nTask 1:\n\nSubmit the Python script used for web scraping and text processing. The .ipynb file should include an explanation of why you chose the website using markdown.\nSubmit the Excel file containing the processed text data.\n\nTask 2:\n\nSubmit the Python script used for YouTube subtitle extraction and processing. The .ipynb file should include an explanation of why you chose the playlist using markdown.\nSubmit the Excel file containing the processed subtitles.\n\n\n\n\n15.3.5 Grading Criteria:\n\n\n\nTask\nMarks\n\n\n\n\nTask 1: Website Selection & Explanation\n2\n\n\nTask 1: Crawl Data from Multiple Pages\n4\n\n\nTask 1: Preprocess and Save to Excel\n4\n\n\nTask 2: YouTube Playlist Selection & Explanation\n2\n\n\nTask 2: Extract Subtitles from Multiple Videos\n4\n\n\nTask 2: Preprocess Subtitles and Save to Excel\n4\n\n\nTotal\n20\n\n\n\n\n\n15.3.6 Submission Deadline:\n\nPlease submit your assignment by Oct. 10, 2024 via the course portal.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Assignment #01</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Books\n\n파이썬 텍스트 마이닝 완벽 가이드 (개정판) https://wikibook.co.kr/textmining-rev/\n딥 러닝을 이용한 자연어 처리 입문 https://wikidocs.net/book/2155\n\n\n\nWebsites\n\nhttps://regexr.com/",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Basic AI Programming",
    "section": "",
    "text": "Preface\nThis material has been compiled using ChatGPT-4o for the AI Programming class of IGSE in Fall 2024. It is primarily targeted at linguistics students.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "14.Semantic_Network.html",
    "href": "14.Semantic_Network.html",
    "title": "14  Semantic Network Analysis",
    "section": "",
    "text": "14.1 Overview",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Semantic Network Analysis</span>"
    ]
  },
  {
    "objectID": "14.Semantic_Network.html#overview",
    "href": "14.Semantic_Network.html#overview",
    "title": "14  Semantic Network Analysis",
    "section": "",
    "text": "14.1.1 Key Concepts of Semantic Network Analysis:\n\nNodes and Edges:\n\nThe nodes in the network represent the concepts or terms you are analyzing.\nThe edges represent the connections between these terms. These connections can be based on co-occurrences, shared meanings, or even statistical relationships like cosine similarity in word vectors.\n\nCentrality:\n\nCentrality measures like degree centrality or betweenness centrality indicate which terms or concepts are most connected or important in the network. These terms can serve as key concepts or hubs that link many other terms together.\n\nCommunities or Clusters:\n\nSNA can help identify clusters or communities within the network, revealing groups of related terms or concepts that form cohesive sub-networks. These clusters can indicate topics or themes within a set of documents.\n\nApplications:\n\nLinguistics: It can be used to understand the structure of language and how words are related.\nSocial Media Analysis: Identifies key themes, discussions, and influencers.\nKnowledge Management: Visualizes how knowledge is structured in documents or research articles.\n\n\n\n\n14.1.2 Steps in Semantic Network Analysis:\n\nData Collection: You collect a dataset where relationships between concepts are apparent. This could be a corpus of text or a set of documents.\nText Preprocessing: This involves cleaning the text by removing stop words, punctuation, and irrelevant data. Stemming or lemmatization might also be performed to reduce words to their base forms.\nConcept Extraction: You identify the key concepts or terms in the dataset, typically using techniques like term frequency-inverse document frequency (TF-IDF) or keyword extraction algorithms.\nBuilding the Network: A co-occurrence matrix is built, where rows and columns represent words or concepts, and the cells represent the frequency with which those terms co-occur in the same context.\nNetwork Visualization: You visualize the network using graph tools, where terms are represented as nodes, and their relationships (e.g., co-occurrences) are represented by edges.\nAnalysis: You can calculate network metrics (e.g., centrality, clustering) and examine sub-networks to derive insights into how terms are related and what the key concepts are.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Semantic Network Analysis</span>"
    ]
  },
  {
    "objectID": "14.Semantic_Network.html#components",
    "href": "14.Semantic_Network.html#components",
    "title": "14  Semantic Network Analysis",
    "section": "14.2 Components",
    "text": "14.2 Components\n\n14.2.1 What is an Adjacency Matrix?\nAn Adjacency Matrix is a matrix representation of a graph, where the rows and columns represent the nodes (vertices) of the graph, and the matrix entries indicate whether pairs of nodes are adjacent (connected by an edge). It is a common way to represent graph data in mathematics, computer science, and network theory.\nFor a graph ( G = (V, E) ) where ( V ) is the set of vertices and ( E ) is the set of edges:\n\nIf there is an edge between vertex ( i ) and vertex ( j ), the entry ( A[i][j] ) in the adjacency matrix is 1.\nIf there is no edge between ( i ) and ( j ), ( A[i][j] ) is 0.\n\n\n14.2.1.1 Types of Adjacency Matrices:\n\nFor undirected graphs: The matrix is symmetric, i.e., if there is an edge between vertex ( i ) and vertex ( j ), both ( A[i][j] ) and ( A[j][i] ) are 1.\nFor directed graphs: The matrix is not necessarily symmetric. ( A[i][j] = 1 ) only if there is an edge from vertex ( i ) to vertex ( j ).\nWeighted graphs: Instead of just 0 and 1, the entries in the matrix can represent the weight of the edges between vertices.\n\n\n\n\n14.2.2 Example of an Adjacency Matrix\nConsider the following simple undirected graph:\n  A -- B\n  |  /\n  C\nThe graph has three nodes: A, B, and C. There are edges between A and B, A and C, and B and C. The adjacency matrix for this undirected graph would be:\n    A  B  C\nA  [0, 1, 1]\nB  [1, 0, 1]\nC  [1, 1, 0]\n\n\n14.2.3 Python Code to Create and Display an Adjacency Matrix\nBelow is a Python example to create and display an adjacency matrix using the numpy and networkx libraries.\nimport numpy as np\nimport networkx as nx\nimport matplotlib.pyplot as plt\n\n# Create a graph\nG = nx.Graph()\n\n# Add nodes\nG.add_nodes_from(['A', 'B', 'C'])\n\n# Add edges (undirected graph)\nG.add_edges_from([('A', 'B'), ('A', 'C'), ('B', 'C')])\n\n# Get the adjacency matrix\nadj_matrix = nx.adjacency_matrix(G).todense()\n\n# Display the adjacency matrix\nprint(\"Adjacency Matrix:\")\nprint(adj_matrix)\n\n# Visualize the graph\nnx.draw(G, with_labels=True, node_color='skyblue', edge_color='gray', node_size=1500, font_size=16)\nplt.show()\nOutput:\nAdjacency Matrix:\n[[0 1 1]\n [1 0 1]\n [1 1 0]]\nAnd the corresponding graph visualization will show the relationships between A, B, and C.\n\n\n\n14.2.4 Modifying for Directed or Weighted Graphs\n\nDirected Graph: You can use nx.DiGraph() instead of nx.Graph() to create a directed graph. In a directed graph, the edges will only be represented in one direction.\nG = nx.DiGraph()\nG.add_edges_from([('A', 'B'), ('C', 'A')])  # Directed edges\n\n# Get the adjacency matrix\nadj_matrix = nx.adjacency_matrix(G).todense()\n\n# Display the adjacency matrix\nprint(\"Adjacency Matrix:\")\nprint(adj_matrix)\n\n# Visualize the graph\nnx.draw(G, with_labels=True, node_color='skyblue', edge_color='gray', node_size=1500, font_size=16)\nplt.show()\n\nOutput:\nAdjacency Matrix:\n[[0 1 0]\n [0 0 0]\n [1 0 0]]\n\n\nWeighted Graph: To handle weighted graphs, you can assign weights to the edges, and the adjacency matrix will reflect these weights.\nimport networkx as nx\nimport matplotlib.pyplot as plt\n\n# Create a MultiGraph\nG = nx.MultiGraph()\n\n# Add nodes\nG.add_nodes_from(['A', 'B', 'C'])\n\n# Add multiple edges between nodes\nG.add_edge('A', 'B', weight=2)\nG.add_edge('A', 'B', weight=3)  # Multiple edge between A and B\nG.add_edge('A', 'C', weight=4)\nG.add_edge('B', 'C', weight=1)\n\n# Print the edges along with their attributes\nprint(\"Edges in MultiGraph with attributes:\")\nprint(G.edges(data=True))\n\n# Visualize the MultiGraph\npos = nx.spring_layout(G)\nnx.draw(G, pos, with_labels=True, node_color='lightblue', node_size=1500, font_size=16)\n\n# Create custom edge labels\nedge_labels = {}\nfor (u, v, data) in G.edges(data=True):\n    if (u, v) not in edge_labels:\n        edge_labels[(u, v)] = str(data['weight'])\n    else:\n        edge_labels[(u, v)] += f\",{data['weight']}\"\n\n# Draw edge labels\nnx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_size=12)\n\nplt.title(\"MultiGraph with Edge Weights\", fontsize=16)\nplt.axis('off')\nplt.tight_layout()\nplt.show()\nIn a weighted adjacency matrix, instead of 0 and 1, the matrix values will represent the weights of the edges.\n\nOutput:\nEdges in MultiGraph with attributes:\n[('A', 'B', {'weight': 2}), ('A', 'B', {'weight': 3}), ('A', 'C', {'weight': 4}), ('B', 'C', {'weight': 1})]\n\n\n\n14.2.5 Centrality Measures in Graphs\nCentrality measures are important in network analysis because they help identify the most important or influential nodes in a graph. Different centrality measures capture different notions of importance based on various graph properties. Below, we’ll explore some of the most common centrality measures, including Python code to calculate each one using the networkx library.\n\nDegree Centrality: How many direct connections a node has.\nBetweenness Centrality: How often a node lies on the shortest path between other nodes.\nCloseness Centrality: How close a node is to all other nodes in the network.\nEigenvector Centrality: The influence of a node in the network, considering the centrality of its neighbors.\nPageRank: A variant of eigenvector centrality that ranks nodes based on incoming links from other important nodes.\n\n\n14.2.5.1 Degree Centrality\nDegree Centrality is the simplest form of centrality. It counts the number of edges connected to a node. In an undirected graph, this is simply the number of neighbors.\n\nIn a directed graph, it can be divided into:\n\nIn-degree centrality: The number of incoming edges.\nOut-degree centrality: The number of outgoing edges.\n\n\\[\\text{Degree Centrality of node } i = \\frac{\\text{Number of edges connected to node } i}{\\text{Total number of possible edges}}\\]\nPython Code for Degree Centrality:\n### Python Code for Degree Centrality:\n\nimport networkx as nx\nimport matplotlib.pyplot as plt\n\n# Create a sample graph (based on your data or a custom graph)\nG = nx.Graph()\n\n# Add edges between nodes (example graph)\nedges = [\n    ('A', 'B'), \n    ('B', 'C'), \n    ('C', 'D'), ('C', 'E'),\n    ('D', 'E')\n]\nG.add_edges_from(edges)\n# Calculate degree centrality\ndegree_centrality = nx.degree_centrality(G)\nprint(\"Degree Centrality:\", degree_centrality)\n\n# Visualize the graph with node size proportional to degree centrality\nnode_size = [v * 3000 for v in degree_centrality.values()]\npos = nx.spring_layout(G)\nnx.draw(G, pos, with_labels=True, node_size=node_size, node_color=\"lightblue\")\nplt.show()\nOutput:\nDegree Centrality: {'A': 0.25, 'B': 0.5, 'C': 0.75, 'D': 0.5, 'E': 0.5}\n\n\n\n14.2.5.2 Betweenness Centrality\nBetweenness Centrality measures the number of times a node lies on the shortest path between other nodes. Nodes with high betweenness centrality can control the flow of information in a network.\n\\[\\text{Betweenness Centrality of node } i = \\sum{\\substack{s \\neq i \\neq t}} \\frac{\\sigma_{st}(i)}{\\sigma_{st}}\\]\nWhere:\n\n\\(\\sigma_{st}\\) is the total number of shortest paths from node \\(s\\) to node \\(t\\).\n\\(\\sigma_{st}(i)\\)is the number of shortest paths that pass through node \\(i\\).\n\n\nPython Code for Betweenness Centrality:\n# Calculate betweenness centrality\nbetweenness_centrality = nx.betweenness_centrality(G)\nprint(\"Betweenness Centrality:\", betweenness_centrality)\n\n# Visualize the graph with node size proportional to betweenness centrality\nnode_size = [v * 3000 for v in betweenness_centrality.values()]\nnx.draw(G, pos, with_labels=True, node_size=node_size, node_color=\"lightgreen\")\nplt.show()\nOutput:\nBetweenness Centrality: {'A': 0.0, 'B': 0.5, 'C': 0.6666666666666666, 'D': 0.0, 'E': 0.0}\n\n\n14.2.5.3 \n\n\n14.2.5.4 Closeness Centrality\nCloseness Centrality measures how close a node is to all other nodes in the graph. Nodes with high closeness centrality can quickly interact with all other nodes in the network. Closeness Centrality is a centrality measure used to determine how efficiently information spreads from a given node to all other nodes in the network.\n\\[\\text{Closeness Centrality of node } i = \\frac{\\text{number of nodes - 1}}{\\sum{\\text{distance from node $i$ to all other nodes}}}\\]\n\nPython Code for Closeness Centrality:\nmport networkx as nx\n\n# Create a sample graph (based on your data or a custom graph)\nG = nx.Graph()\n\n# Add edges between nodes (example graph)\nedges = [\n    ('A', 'B'), \n    ('B', 'C'), \n    ('C', 'D'), ('C', 'E'),\n    ('D', 'E')\n]\nG.add_edges_from(edges)\n\n# Compute closeness centrality for each node\ncloseness_centrality = nx.closeness_centrality(G)\n\n# Display the closeness centrality values\nfor node, centrality in closeness_centrality.items():\n    print(f\"Closeness Centrality of node {node}: {centrality:.4f}\")\n    \n# Visualize the graph with node size proportional to closeness centrality\nnode_size = [v * 3000 for v in closeness_centrality.values()]\npos = nx.spring_layout(G)\nnx.draw(G, pos, with_labels=True, node_size=node_size, node_color=\"lightgreen\")\nplt.show()\nOutput:\nCloseness Centrality of node A: 0.4444\nCloseness Centrality of node B: 0.6667\nCloseness Centrality of node C: 0.8000\nCloseness Centrality of node D: 0.5714\nCloseness Centrality of node E: 0.5714\n\n\n\n14.2.5.5 Eigenvector Centrality\nEigenvector Centrality measures the influence of a node in a network. Unlike degree centrality, which simply counts the number of connections, eigenvector centrality assigns more value to nodes that are connected to other highly connected nodes.\n\nWhy Use Eigenvector Centrality?\nEigenvector centrality is useful when you need to consider not just how many connections a node has, but also how influential its neighbors are. It is commonly used in applications like:\n\nSocial Networks: Identifying influential individuals who are connected to other influential individuals.\nWeb Search Algorithms: PageRank, a variant of eigenvector centrality, is used to rank web pages based on their importance.\nBiological Networks: Identifying key genes or proteins based on their interactions with other important molecules.\n\nFor a node \\(v\\), the eigenvector centrality \\(x_v\\) is given by the following equation:\n\\[x_v = \\frac{1}{\\lambda} \\sum_{u* \\in N(v)}A_{vu}x_u\\]\nWhere:\n\n\\(x_v\\) is the eigenvector centrality of node \\(v\\).\n\\(\\lambda\\) is a constant (the largest eigenvalue of the adjacency matrix).\n\\(N(v)\\) is the set of neighbors of node ( v ).\n\\(A_{vu}\\) is the adjacency matrix where \\(A_{vu} = 1\\) if there is an edge between \\(v\\) and \\(u\\), and 0 otherwise.\n\nEigenvector centrality is a more nuanced measure of influence compared to simpler metrics like degree centrality, making it ideal for applications where connections to other influential nodes are particularly important.\nPython Code for Eigenvector Centrality:\nimport networkx as nx\nimport matplotlib.pyplot as plt\n\n# Create a sample graph\nG = nx.Graph()\n\n# Add edges between nodes (example graph)\nedges = [\n    ('A', 'C'), \n    ('B', 'C'), \n    ('C', 'D'), \n    ('D', 'E'), \n    ('D', 'F'), \n    ('D', 'G')\n]\nG.add_edges_from(edges)\n\n# Compute eigenvector centrality for each node\neigenvector_centrality = nx.eigenvector_centrality(G)\n\n# Display the eigenvector centrality values\nfor node, centrality in eigenvector_centrality.items():\n    print(f\"Eigenvector Centrality of node {node}: {centrality:.4f}\")\n\n# Visualize the graph with node size proportional to eigenvector centrality\nnode_size = [v * 3000 for v in eigenvector_centrality.values()]\npos = nx.spring_layout(G)\nnx.draw(G, pos, with_labels=True, node_size=node_size, node_color=\"lightblue\")\nplt.show()\nOutput:\nEigenvector Centrality of node A: 0.2299\nEigenvector Centrality of node C: 0.5000\nEigenvector Centrality of node B: 0.2299\nEigenvector Centrality of node D: 0.6280\nEigenvector Centrality of node E: 0.2887\nEigenvector Centrality of node F: 0.2887\nEigenvector Centrality of node G: 0.2887\n\n\n\n14.2.5.6 PageRank Centrality\nPageRank is a centrality measure originally developed by Larry Page and Sergey Brin to rank web pages in Google’s search engine. It is a variant of Eigenvector Centrality, but with the addition of a damping factor, typically set to 0.85, that models the probability that a user will stop following links at any given point. This allows PageRank to rank the importance of nodes (e.g., web pages) not just based on the number of inbound links, but also based on the importance of the linking pages.\nWhy Use PageRank?\nPageRank is a powerful measure of centrality that accounts for both the quantity and quality of connections to a node. It’s particularly useful in situations where:\n\nWeb Search: Ranking web pages based on the importance of the pages linking to them.\nCitation Networks: Determining influential papers based on how often they are cited by other important papers.\nSocial Networks: Identifying influential individuals based on connections from other influential people.\n\nDifferences from Other Centrality Measures:\n\nDegree Centrality only counts the number of links to a node, whereas PageRank considers both the number and quality (importance) of those links.\nEigenvector Centrality is similar to PageRank but lacks the damping factor, which prevents dead-end nodes from absorbing all the rank.\n\nPageRank Centrality Formula\nThe PageRank of a node \\(v\\) is given by:\n\\[PR(v) = \\frac{1-d}{N} + d \\sum_{u \\in M(v)} \\frac{PR(u)}{L(u)}\\]\nWhere:\n\n\\(PR(v)\\)is the PageRank of node \\(v\\).\n\\(d\\) is the damping factor (usually set to 0.85), representing the probability that a random walker will continue following links.\n\\(N\\) is the total number of nodes in the graph.\n\\(M(v)\\) is the set of nodes that link to node \\(v\\) (inbound links).\n\\(PR(u)\\) is the PageRank of node \\(u\\), which links to \\(v\\).\n\\(L(u)\\) is the number of outbound links from node \\(u\\).\n\nThe PageRank algorithm is an iterative process, where the initial PageRank values are distributed equally across all nodes. It then updates these values based on the formula above until the values converge (i.e., until they stop changing significantly).\nDetailed Explanation of Components:\n\nDamping Factor \\(d\\): This factor accounts for the possibility that a random surfer on the network (or web) will jump to a random page rather than following links. Typically, \\(d\\) is set to 0.85, meaning there’s an 85% chance the surfer will follow links and a 15% chance they will jump to a random page.\nTerm \\[\\frac{1-d}{N}\\]: This is the probability that the random surfer jumps to any node randomly. It distributes a small amount of rank equally to all nodes to prevent dead ends (i.e., nodes with no outbound links) from absorbing all the rank.\nSummation Term \\[\\sum_{u \\in M(v)} \\frac{PR(u)}{L(u)}\\]: This term gives the contribution of the rank from each node \\(u\\) that links to \\(v\\). The rank from node \\(u\\) is divided by the number of outbound links from \\(u\\), so the more links \\(u\\) has, the less rank it passes to each linked node.\n\nPython Code for PageRank\nUsing the networkx library, we can easily compute PageRank for a graph. Below is a Python implementation:\nimport networkx as nx\nimport matplotlib.pyplot as plt\n\n# Create a directed graph (since PageRank works on directed graphs)\nG = nx.DiGraph()\n\n# Add edges (this is a simple example, you can add any structure you want)\nedges = [\n    ('A', 'B'), \n    ('B', 'C'), \n    ('C', 'A'), \n    ('A', 'D'), \n    ('D', 'C'), \n    ('E', 'D'), \n    ('F', 'D'), \n    ('E', 'F'), \n    ('F', 'E')\n]\nG.add_edges_from(edges)\n\n# Compute PageRank\npagerank = nx.pagerank(G, alpha=0.85)  # Damping factor alpha=0.85\n\n# Display the PageRank values\nfor node, rank in pagerank.items():\n    print(f\"PageRank of node {node}: {rank:.4f}\")\n\n# Visualize the graph with node size proportional to PageRank\nnode_size = [v * 3000 for v in pagerank.values()]\npos = nx.spring_layout(G)\nnx.draw(G, pos, with_labels=True, node_size=node_size, node_color=\"lightblue\", arrows=True)\nplt.show()\nOutput:\nPageRank of node A: 0.2827\nPageRank of node B: 0.1451\nPageRank of node C: 0.3031\nPageRank of node D: 0.1821\nPageRank of node E: 0.0435\nPageRank of node F: 0.0435\n\nInterpretation of Output:\nThe PageRank algorithm has been applied to a directed graph where nodes and edges represent relationships (e.g., links between websites, connections in a network, etc.). Below is a detailed interpretation of both the graph and the PageRank values that were calculated in the provided output.\nPageRank Scores:\n\nNode A (0.2827):\n\nNode A has the second-highest PageRank score. It is an important node in the cycle (A → B → C → A) and also has an outbound connection to D. The cycle A-B-C boosts its importance since it is connected to other central nodes.\n\nNode B (0.1451):\n\nNode B has a relatively lower PageRank compared to A and C. While it is part of the cycle, it doesn’t have any additional outbound connections like node A, which might reduce its importance compared to its neighbors.\n\nNode C (0.3031):\n\nNode C has the highest PageRank score, indicating it is the most central and influential node in the graph. This is likely because it not only participates in the cycle (A → B → C → A) but also receives an additional edge from node D, giving it more inbound influence.\n\nNode D (0.1821):\n\nNode D has a moderately high PageRank score, primarily because it is linked to the important cycle (via A and C). Although it is not part of the cycle, its proximity to influential nodes boosts its score.\n\nNode E (0.0435) and Node F (0.0435):\n\nBoth E and F have the lowest PageRank scores. These nodes are peripheral in the network, with their primary connections being reciprocal links between themselves and links to node D. Since D is the only pathway for their influence, their importance is limited compared to other nodes directly involved in the central cycle.\n\n\nVisual Representation:\nIn the provided graph (as visualized), node sizes are proportional to their PageRank scores:\n\nLarger nodes (A, C, and D) are more important, with C being the largest node, reflecting its highest PageRank score.\nSmaller nodes (E and F) are less influential and are positioned on the periphery, reflecting their lower PageRank.\n\nSummary:\n\nCentrality of Cycle Nodes: Nodes A, B, and C form a strongly connected component (cycle), making them highly influential. C is the most central because it benefits from both the cycle and its connection to D.\nPeripheral Influence: Nodes E and F are peripheral and have limited influence because they rely on node D to connect to the rest of the network.\nNode D’s Role: Node D plays an intermediary role between the central cycle and the peripheral nodes E and F, making it somewhat important, though not as central as nodes in the cycle itself.\n\nThe graph and PageRank scores together show how the structure of a network determines the relative importance of each node, with highly interconnected cycles typically having the most influence.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Semantic Network Analysis</span>"
    ]
  },
  {
    "objectID": "15.SNA_Example.html",
    "href": "15.SNA_Example.html",
    "title": "15  Semantic Network Analysis (Examples)",
    "section": "",
    "text": "15.1 Preparation (R)",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Semantic Network Analysis (Examples)</span>"
    ]
  },
  {
    "objectID": "15.SNA_Example.html#preparation",
    "href": "15.SNA_Example.html#preparation",
    "title": "15  Semantic Network Analysis using R (.rmd)",
    "section": "",
    "text": "15.1.1 Step 1: Install R\n\nGo to CRAN: Visit the Comprehensive R Archive Network (CRAN) here.\nDownload R:\n\nSelect your operating system (Windows, macOS, or Linux).\nFollow the download and installation instructions specific to your OS.\n\nInstall R:\n\nRun the downloaded file and follow the on-screen instructions.\nOnce complete, R should be ready to use.\n\n\n\n\n15.1.2 Step 2: Install RStudio\n\nGo to RStudio’s Website: Visit RStudio’s download page.\nDownload RStudio:\n\nChoose the free version of RStudio Desktop.\nDownload the version appropriate for your operating system.\n\nInstall RStudio:\n\nRun the downloaded file and follow the installation instructions.\nAfter installation, open RStudio to confirm it’s working and connected to R.\n\n\n\n\n15.1.3 Step 3: Install Required Packages for Semantic Network Analysis\nOpen RStudio and install the necessary packages for semantic network analysis. Here’s how:\n\nOpen RStudio.\nInstall packages by typing the following commands in the Console and pressing Enter after each line:\n\n\n# install.packages(\"igraph\")         # Network visualization and analysis\n# install.packages(\"quanteda\")       # Text analysis\n# install.packages(\"quanteda.textstats\") # Text statistics for semantic networks\n# install.packages(\"widyr\")          # Pairwise correlations for networks\n# install.packages(\"ggraph\")         # Network graphing tool with ggplot2 syntax\n# install.packages(\"tidygraph\")      # Manipulate network data in tidy format\n# install.packages(\"ggplot2\")\n\n\nLoad packages to check if they’re correctly installed:\n\n\nlibrary(igraph)\n\nWarning: package 'igraph' was built under R version 4.3.3\n\n\n\nAttaching package: 'igraph'\n\n\nThe following objects are masked from 'package:stats':\n\n    decompose, spectrum\n\n\nThe following object is masked from 'package:base':\n\n    union\n\nlibrary(quanteda)\n\nWarning: package 'quanteda' was built under R version 4.3.3\n\n\nWarning in .recacheSubclasses(def@className, def, env): undefined subclass\n\"ndiMatrix\" of class \"replValueSp\"; definition not updated\n\n\nPackage version: 4.1.0\nUnicode version: 14.0\nICU version: 71.1\n\n\nParallel computing: disabled\n\n\nSee https://quanteda.io for tutorials and examples.\n\nlibrary(quanteda.textstats)\n\nWarning: package 'quanteda.textstats' was built under R version 4.3.3\n\nWarning: undefined subclass \"ndiMatrix\" of class \"replValueSp\"; definition not\nupdated\n\nlibrary(widyr)\nlibrary(ggraph)\n\nLoading required package: ggplot2\n\nlibrary(tidygraph)\n\n\nAttaching package: 'tidygraph'\n\n\nThe following object is masked from 'package:quanteda':\n\n    convert\n\n\nThe following object is masked from 'package:igraph':\n\n    groups\n\n\nThe following object is masked from 'package:stats':\n\n    filter",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Semantic Network Analysis using R (Example)</span>"
    ]
  },
  {
    "objectID": "15.SNA_Example.html#example-r-code",
    "href": "15.SNA_Example.html#example-r-code",
    "title": "15  Semantic Network Analysis (Examples)",
    "section": "15.2 Example R Code",
    "text": "15.2 Example R Code\nThis R code is designed to import, rename, and display the initial rows of a dataset. Here’s a breakdown of each step:\n\n15.2.1 Data Impoart\n\nLoad Required Libraries:\n\n\nlibrary(readr); library(dplyr); library(showtext)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:igraph':\n\n    as_data_frame, groups, union\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\nLoading required package: sysfonts\n\n\nLoading required package: showtextdb\n\n\n\nlibrary(readr): Loads the readr package, which includes functions for reading rectangular data (like CSV files) quickly and efficiently.\nlibrary(dplyr): Loads the dplyr package, a popular package for data manipulation.\nlibrary(showtext): Loads the showtext package, used for handling fonts when creating plots. Although it’s loaded here, it’s not directly used in the subsequent code.\n\n\nData Import:\n\n\ndata_raw &lt;- read_csv(\"data.csv\", show_col_types = FALSE)\n\n\ndata_raw &lt;- read_csv(\"data.csv\"): Reads a CSV file named “data.csv” and stores it in a variable called data_raw. The read_csv() function from readr is designed for fast reading and returns a tibble, a more user-friendly form of the standard data frame in R.\n\n\nRename a Column:\n\n\ndata_original &lt;- data_raw %&gt;% rename(Journals = j_short)\n\n\ndata_original &lt;- data_raw %&gt;% rename(Journals = j_short): Creates a new data frame data_original by renaming the j_short column to Journals.\n%&gt;%: This is the pipe operator from dplyr, which allows you to chain functions together.\nrename(Journals = j_short): The rename() function changes the name of j_short to Journals in the data frame.\n\n\nDisplay the First Rows of the Dataset:\n\n\nhead(data_original)\n\n# A tibble: 6 × 5\n  serial id_record         Journals  year title_en_lower_cleaned                \n   &lt;dbl&gt; &lt;chr&gt;             &lt;chr&gt;    &lt;dbl&gt; &lt;chr&gt;                                 \n1      1 geta_2001_6_1_5   geta      2001 a study on the teaching models of nat…\n2      2 geta_2001_6_1_33  geta      2001 a study of teaching process oriented …\n3      3 geta_2001_6_1_61  geta      2001 a study of the development and applic…\n4      4 geta_2001_6_1_83  geta      2001 an instructional model for english wr…\n5      5 geta_2001_6_1_103 geta      2001 a study on the effects of call on ele…\n6      6 geta_2001_6_1_121 geta      2001 a study on methods of enhancing focus…\n\n\n\nhead(data_original): Displays the first six rows of the data_original data frame, giving a quick view of its structure and content.\n\n\n\n15.2.2 Stopwords\nThis R code is handling custom stopwords by reading, modifying, and filtering them as follows:\n\nRead Stopwords File:\n\n\nstopwords &lt;- readLines(\"my_stopwords.txt\", encoding = \"UTF-8\")\n\n\nreadLines(\"my_stopwords.txt\", encoding = \"UTF-8\"): Reads the content of the file \"my_stopwords.txt\" line by line, storing each line as an individual element in the stopwords vector. The encoding = \"UTF-8\" ensures that any non-ASCII characters are read correctly.\nstopwords: This variable now contains a list of words that will be treated as stopwords.\n\n\nCheck Type of Stopwords:\n\ntypeof(stopwords)\n\n[1] \"character\"\n\n\n\ntypeof(stopwords): Checks the data type of stopwords. Since readLines() returns a character vector, this will likely output \"character\".\n\nAdd New Stopwords:\n\nstopwords_added &lt;- c(\"english\", \"korean\")  # add new stopwords \nstopwords &lt;- append(stopwords, stopwords_added)\nstopwords[0:10]\n\n [1] \"‘d\"  \"‘ll\" \"‘m\"  \"‘re\" \"‘s\"  \"‘ve\" \"’d\"  \"’ll\" \"’m\"  \"’re\"\n\n\n\nstopwords_added &lt;- c(\"english\", \"korean\"): Creates a vector stopwords_added with new words (“english” and “korean”) to be added as stopwords.\nappend(stopwords, stopwords_added): Appends the new words in stopwords_added to the end of the stopwords vector.\n\nRemove Specific Stopwords:\n\nstopwords_deleted &lt;- c(\"korean\", \"‘ll\", \"‘d\")\nfor (i in stopwords_deleted){\n  stopwords &lt;- gsub(i, \"\", stopwords)\n}\nstopwords_deleted\n\n[1] \"korean\" \"‘ll\"    \"‘d\"    \n\n\n\nstopwords_deleted &lt;- c(\"korean\", \"‘ll\", \"‘d\"): Specifies words to be removed from the stopwords list, including “korean”, “’ll” (contraction for “will”), and “’d” (contraction for “would”).\nLoop to Remove Words:\n\nfor (i in stopwords_deleted): Iterates over each word in stopwords_deleted.\nstopwords &lt;- gsub(i, \"\", stopwords): Uses gsub() to replace occurrences of each word i in stopwords with an empty string, effectively removing it. This works by searching for each specified word and replacing it with \"\" wherever found.\n\n\n\n\n\n15.2.3 Library for Graphs\n\nInstall ggplot2:\n\n\n# install.packages(\"ggplot2\")\n\n\ninstall.packages(\"ggplot2\"): Installs the ggplot2 package, which is one of R’s most popular packages for creating high-quality graphics. It needs to be run only once per environment (or whenever ggplot2 is missing or needs updating).\n\n\nLoad Library:\n\nlibrary(ggplot2)\n\n\nlibrary(ggplot2): Loads ggplot2, enabling you to use its functions for data visualization. ggplot2 provides a grammar of graphics, allowing for flexible creation of graphs.\n\n\n\n\n15.2.4 Custom Functions for Creating Bar and Line Graphs\nThese two custom functions, bar_freq_f and line_freq_f, are designed to create bar and line graphs using ggplot2. Each function takes an input dataset and generates a graph with specified formatting and aesthetic customizations.\n\nBar Graph Function (bar_freq_f):\n\nbar_freq_f &lt;- function(input) {\n  ggplot(input, aes(x = year, y = count)) +\n    geom_bar(stat = \"identity\", width = 0.8, fill = \"gray88\") + \n    geom_text(aes(x = year, y = count, label = count),\n              hjust = 0.4,\n              color = \"blue\", \n              size = 3) +\n    theme(panel.grid.major = element_blank(),\n          panel.grid.minor = element_blank(),\n          panel.background = element_blank(),\n          axis.line = element_line(colour = \"black\")) \n}\n\n\nggplot(input, aes(x = year, y = count)): Initializes a ggplot object with year on the x-axis and count on the y-axis.\ngeom_bar(stat = \"identity\", width = 0.8, fill = \"gray88\"): Adds a bar graph where each bar’s height represents the count value (stat = \"identity\" means the y-value is taken directly from the count variable). The bar color is set to gray88, and the width is set to 0.8.\ngeom_text(...): Adds text labels displaying the count value on each bar, with a horizontal offset (hjust = 0.4), red color, and size of 3.\ntheme(...): Customizes the graph’s appearance by:\n\nRemoving major and minor grid lines.\nSetting the background to be blank (white).\nAdding a black line for the axes.\n\n\nLine Graph Function (line_freq_f):\n\n\nline_freq_f &lt;- function(input) {\n ggplot(input, aes(x = year, y = count, group = Journals)) +\n   geom_line(aes(color = Journals)) + \n   geom_point(aes(color = Journals, shape = Journals)) +\n   theme(legend.position = 'right') + \n   theme(panel.grid.major = element_blank(), \n         panel.grid.minor = element_blank(),\n         panel.background = element_blank(), \n         axis.line = element_line(colour = \"black\"))\n   }\n\n\nggplot(input, aes(x = year, y = count, group = Journals)): Initializes a ggplot object with year on the x-axis, count on the y-axis, and each line group represented by the Journals variable.\ngeom_line(aes(color = Journals)): Adds lines to the plot, coloring each line according to the Journals variable.\ngeom_point(aes(color = Journals, shape = Journals)): Adds points to the line plot, matching the color and shape to each Journals group.\ntheme(legend.position = 'right'): Positions the legend on the right side of the graph.\ntheme(...): Customizes the plot similarly to the bar graph, removing grid lines and background while adding a black axis line.\n\n\n\n15.2.5 Custom Functions for Creating NLP Tasks\nThis code defines multiple functions to conduct natural language processing (NLP) tasks, including tokenization, stopword removal, word counting, word cloud creation, co-occurrence network analysis, and n-gram processing. Below is an explanation of each function:\n\nNLP Tagging Function (ud_tagger_f)\n\nlibrary(udpipe)\nud_model &lt;- udpipe_download_model(language = \"english\")\n\nDownloading udpipe model from https://raw.githubusercontent.com/jwijffels/udpipe.models.ud.2.5/master/inst/udpipe-ud-2.5-191206/english-ewt-ud-2.5-191206.udpipe to /Users/won/Library/CloudStorage/OneDrive-고려사이버대학교/09_Book_Writing/F2024_IGSE_Basic_AI_Programming/english-ewt-ud-2.5-191206.udpipe\n\n\n - This model has been trained on version 2.5 of data from https://universaldependencies.org\n\n\n - The model is distributed under the CC-BY-SA-NC license: https://creativecommons.org/licenses/by-nc-sa/4.0\n\n\n - Visit https://github.com/jwijffels/udpipe.models.ud.2.5 for model license details.\n\n\n - For a list of all models and their licenses (most models you can download with this package have either a CC-BY-SA or a CC-BY-SA-NC license) read the documentation at ?udpipe_download_model. For building your own models: visit the documentation by typing vignette('udpipe-train', package = 'udpipe')\n\n\nDownloading finished, model stored at '/Users/won/Library/CloudStorage/OneDrive-고려사이버대학교/09_Book_Writing/F2024_IGSE_Basic_AI_Programming/english-ewt-ud-2.5-191206.udpipe'\n\nud_model &lt;- udpipe_load_model(ud_model$file_model)\n\nud_tagger_f &lt;- function(input) {\n    udpipe_annotate(ud_model, x = input$title_en_lower_cleaned, doc_id = input$serial) %&gt;%\n    as.data.frame() %&gt;%\n    filter(!lemma %in% stopwords) %&gt;%\n    subset(upos %in% c(\"VERB\", \"NOUN\", \"ADJ\"))\n}\n\n\nThis function uses the udpipe library to tag words with parts of speech (POS) and filter out unwanted words.\nudpipe_annotate() annotates the text, creating a data frame with columns like lemma, upos, etc.\nFilters out stopwords and keeps only verbs, nouns, and adjectives (upos values).\n\nWord Count Function (word_count_f)\n\nword_count_f &lt;- function(input) {\n    input %&gt;%\n    group_by(lemma) %&gt;%\n    summarise(count = n(), n_distinct_maker = n_distinct(lemma)) %&gt;%\n    arrange(desc(count))\n}\n\n\nGroups and counts occurrences of each lemma (word root), showing frequency for each unique word.\nn_distinct_maker provides a count of unique lemmas.\n\nWord Cloud Function (wordcloud_f)\n\nlibrary(tm); library(wordcloud); library(memoise)\n\nWarning: package 'tm' was built under R version 4.3.3\n\n\nLoading required package: NLP\n\n\n\nAttaching package: 'NLP'\n\n\nThe following object is masked from 'package:ggplot2':\n\n    annotate\n\n\nThe following objects are masked from 'package:quanteda':\n\n    meta, meta&lt;-\n\n\n\nAttaching package: 'tm'\n\n\nThe following object is masked from 'package:quanteda':\n\n    stopwords\n\n\nLoading required package: RColorBrewer\n\nwordcloud_f &lt;- function(input, min_freq, max_words) {\n    text &lt;- input$lemma\n    myCorpus &lt;- Corpus(VectorSource(text))\n    myDTM &lt;- TermDocumentMatrix(myCorpus, control = list(minWordLength = 1))\n    m &lt;- as.matrix(myDTM)\n    words_all &lt;- sort(rowSums(m), decreasing = TRUE)\n    df_all &lt;- data.frame(word = names(words_all), freq = words_all)\n    wordcloud(words = df_all$word, freq = df_all$freq, min.freq = min_freq, max.words = max_words, random.order = FALSE, rot.per = 0.35, colors = brewer.pal(8, \"Dark2\"))\n}\n\n\nCreates a word cloud based on word frequency.\nUses TermDocumentMatrix to calculate word frequencies, then visualizes the most frequent words.\n\nPair Words Function (pair_words_f)\n\nlibrary(dplyr); library(widyr)\n\npair_words_f &lt;- function(input) {\n    input %&gt;%\n    pairwise_count(item = lemma, feature = doc_id, sort = T)\n}\n\n\nComputes the pairwise count of words based on document co-occurrence.\n\nDegree Centrality Graph Function (centrality_network)\n\nlibrary(ggraph)\n\ncentrality_network &lt;- function(x) {\n    ggraph(x, layout = \"fr\") +\n    geom_edge_link(color = \"gray50\", alpha = 0.5) +\n    geom_node_point(aes(size = centrality, color = group), show.legend = F) +\n    scale_size(range = c(5, 15)) +\n    geom_node_text(aes(label = name), repel = T, size = 3, family = \"Arial\") +\n    theme_graph()\n}\n\n\nVisualizes a network graph based on degree centrality of nodes.\ncentrality determines node size, and group determines color, with ggraph used for layout.\n\nPhi Coefficient Function (words_cors_f)\n\nwords_cors_f &lt;- function(input, nsize) {\n    input %&gt;%\n    add_count(lemma) %&gt;%\n    filter(n &gt;= nsize) %&gt;%\n    pairwise_cor(item = lemma, feature = doc_id, sort = T)\n}\n\n\nComputes the phi correlation (similarity) between words that frequently co-occur in documents.\n\nCorrelation Graph Function (graph_cors_f and word_network_cor_f)\n\ngraph_cors_f &lt;- function(input, cor_size) {\n    input %&gt;%\n    filter(correlation &gt;= cor_size) %&gt;%\n    as_tbl_graph(directed = F) %&gt;%\n    mutate(centrality = centrality_degree(), group = as.factor(group_infomap()))\n}\n\nword_network_cor_f &lt;- function(input, cor_size) {\n    graph_cor &lt;- graph_cors_f(input, cor_size) \n    ggraph(graph_cor, layout = \"fr\") +\n    geom_edge_link(color = \"gray50\", aes(edge_alpha = correlation, edge_width = correlation), show.legend = F) +\n    scale_edge_width(range = c(1, 4)) +\n    geom_node_point(aes(size = centrality, color = group), show.legend = F) +\n    scale_size(range = c(5, 10)) +\n    geom_node_text(aes(label = name), repel = T, size = 3, family = \"Arial\") +\n    theme_graph()\n}\n\n\ngraph_cors_f filters correlations above a specified threshold and creates a network graph based on word similarity.\nword_network_cor_f plots the graph, with nodes sized by centrality and edges weighted by correlation.\n\nN-Gram Processing and Graph Functions\n\ndata_ngram_separated_f &lt;- function(x_data_input, ngram_size) {\n    x_data_input %&gt;%\n    group_by(doc_id) %&gt;%\n    summarise(sentence = paste(lemma, collapse = \" \")) %&gt;%\n    unnest_tokens(input = sentence, output = bigram, token = \"ngrams\", n = ngram_size) %&gt;%\n    separate(bigram, c(\"word1\", \"word2\"), sep = \" \") %&gt;%\n    count(word1, word2, sort = T) %&gt;% na.omit()\n}\n\ndata_ngram_graph_f &lt;- function(ngram_separated, filter_size) {\n    ngram_separated %&gt;%\n    filter(n &gt;= filter_size) %&gt;%\n    as_tbl_graph(directed = F) %&gt;%\n    mutate(centrality = centrality_degree(), group = as.factor(group_infomap()))\n}\n\nword_network_ngram_f &lt;- function(data_ngram_graph) {\n    ggraph(data_ngram_graph, layout = \"fr\") +\n    geom_edge_link(color = \"gray50\", alpha = 0.5) +\n    geom_node_point(aes(size = centrality, color = group), show.legend = F) +\n    scale_size(range = c(4, 8)) +\n    geom_node_text(aes(label = name), repel = T, size = 3, family = \"Arial\") +\n    theme_graph()\n}\n\n\ndata_ngram_separated_f creates n-grams and calculates word-pair frequencies.\ndata_ngram_graph_f filters frequent n-grams and structures the data for graphing.\nword_network_ngram_f visualizes the n-gram network with ggraph.\n\n\n\n\n15.2.6 Articles by Year (Bar Graphs)\nThis code calculates the number of articles per year from the data_original dataset, saves the summary as a CSV file, and then creates a bar graph of article counts by year using the custom function bar_freq_f.\n\nGroup and Summarize Data by Year:\n\narticles_by_year &lt;- data_original %&gt;% \n  group_by(year) %&gt;% \n  summarise(count = n())\n\n\ngroup_by(year): Groups data_original by the year column, so each year will have its own grouping.\nsummarise(count = n()): Calculates the count (number of articles) in each year and stores it in a new column called count.\nThe resulting articles_by_year data frame has two columns: year and count.\n\nSave Summary to CSV:\n\nwrite.csv(articles_by_year, \"articles_by_year.csv\")\n\n\nSaves the articles_by_year data frame as a CSV file named \"articles_by_year.csv\" in the working directory.\n\nGenerate Bar Graph:\n\nbar_freq_f(articles_by_year)\n\n\n\n\n\n\n\n\n\nCalls the bar_freq_f function to create a bar graph of article counts by year.\nThe graph uses year on the x-axis and count on the y-axis, with additional customizations (e.g., bar color, grid settings) as defined in the bar_freq_f function.\n\n\n\n\n15.2.7 Articles by Year and Journal (Line Graphs)\nThis code calculates the number of articles per year for each journal in data_original, then creates a line graph of these counts using the line_freq_f function.\n\nGroup and Summarize Data by Journal and Year:\n\narticles_by_year_journal &lt;- data_original %&gt;%\n  group_by(Journals, year) %&gt;%\n  summarise(count = n())\n\n`summarise()` has grouped output by 'Journals'. You can override using the\n`.groups` argument.\n\n\n\ngroup_by(Journals, year): Groups data_original by both Journals and year so that each journal-year combination has its own grouping.\nsummarise(count = n()): Counts the number of articles for each journal-year combination, storing the count in a new column called count.\nThe resulting articles_by_year_journal data frame has three columns: Journals, year, and count.\n\nGenerate Line Graph:\n\nline_freq_f(articles_by_year_journal)\n\n\n\n\n\n\n\n\n\nCalls the line_freq_f function to create a line graph, displaying the count of articles by year for each Journals group.\nThe function plots each journal as a separate line with distinct colors and shapes for better differentiation, and positions the legend on the right side.\n\n\n\n\n15.2.8 POS Tagging\nThis code installs and loads the udpipe package, and then applies the custom ud_tagger_f function on data_original to perform POS tagging and filtering. Here’s a breakdown of each part:\n\nInstall and Load udpipe:\n\n\n#install.packages(\"udpipe\")\n#library(udpipe)\n\n\ninstall.packages(\"udpipe\"): Installs the udpipe package, which provides tools for natural language processing, including tokenization, POS tagging, lemmatization, and dependency parsing.\nlibrary(udpipe): Loads the udpipe library to use its functions.\n\n\nApply ud_tagger_f Function:\n\n\nx_data_english &lt;- ud_tagger_f(data_original)\n\n\nud_tagger_f(data_original): Calls the ud_tagger_f function (defined previously) on the data_original dataset. Here’s what ud_tagger_f does:\n\nIt uses udpipe_annotate() to annotate the text in data_original$title_en_lower_cleaned (assumed to be a column with cleaned English titles).\nFilters out stopwords using the stopwords list.\nKeeps only specific POS tags: verbs, nouns, and adjectives.\n\nThe resulting data frame, x_data_english, contains the annotated and filtered data, including information like lemma, upos, and doc_id.\n\n\n\n15.2.9 Word Count (Noun, Verb, Adj Only)\nThis code performs two main tasks using x_data_english, which contains POS-tagged and filtered data (nouns, verbs, and adjectives only) from a previous step. The tasks include word counting and identifying the top 20 most frequent words along with their distribution across articles.\n\nPOS Tagging Count with word_count_f\n\n\nx_data_cnt &lt;- word_count_f(x_data_english) \nwrite.csv(x_data_cnt, 'x_data_cnt.csv')\n\n\nx_data_cnt &lt;- word_count_f(x_data_english): Applies the word_count_f function to x_data_english. This function:\n\nGroups by each unique word (lemma).\nCounts the occurrences of each word.\nReturns a data frame (x_data_cnt) with columns for each word (lemma) and its count.\n\nwrite.csv(x_data_cnt, 'x_data_cnt.csv'): Exports the word count data to a CSV file named x_data_cnt.csv.\n\n\nCounting Articles by Top Words\n\n\nlibrary(dplyr); library(lattice); library(udpipe)\ndata_english_freq &lt;- txt_freq(x_data_english$lemma)\ntop20_words_english &lt;- head(data_english_freq, 20)\narticles_by_top20_words_english &lt;- subset(x_data_english, lemma %in% top20_words_english$key) %&gt;%\n group_by(lemma) %&gt;%\n summarise(count = n(), n_distinct_maker = n_distinct(doc_id)) %&gt;%\n arrange(desc(n_distinct_maker))\nwrite.csv(articles_by_top20_words_english, 'articles_by_top20_words_english.csv')\narticles_by_top20_words_english\n\n# A tibble: 20 × 3\n   lemma       count n_distinct_maker\n   &lt;chr&gt;       &lt;int&gt;            &lt;int&gt;\n 1 student       238              234\n 2 learn         200              184\n 3 effect        178              178\n 4 use           171              170\n 5 school        165              159\n 6 korean        149              149\n 7 learner       125              125\n 8 efl           117              117\n 9 language      119              113\n10 elementary     96               96\n11 write         109               96\n12 teacher       108               95\n13 vocabulary     83               72\n14 perception     69               69\n15 teaching       70               69\n16 college        69               67\n17 education      64               59\n18 university     60               59\n19 development    58               58\n20 class          58               57\n\n\n\ndata_english_freq &lt;- txt_freq(x_data_english$lemma): Uses the txt_freq function from udpipe to compute the frequency of each lemma (word) in x_data_english.\ntop20_words_english &lt;- head(data_english_freq, 20): Extracts the top 20 most frequent words based on the txt_freq output.\narticles_by_top20_words_english:\n\nsubset(x_data_english, lemma %in% top20_words_english$key): Filters x_data_english to include only the top 20 frequent words.\ngroup_by(lemma): Groups by each of the top 20 words.\nsummarise(count = n(), n_distinct_maker = n_distinct(doc_id)): Counts the total occurrences of each word and the number of distinct documents (doc_id) where each word appears.\narrange(desc(n_distinct_maker)): Orders the words by the number of unique documents in descending order.\n\nwrite.csv(articles_by_top20_words_english, 'articles_by_top20_words_english.csv'): Saves the summary data of articles containing the top 20 words to a CSV file.\n\n\n\n15.2.10 Word Cloud\nThis code creates a word cloud using the wordcloud_f function, which visualizes the most frequent words in x_data_english. Here’s a breakdown:\n\nLoad Required Libraries:\n\nlibrary(dplyr); library(stringr); library(wordcloud); library(RColorBrewer); library(wordcloud2); library(tm)\n\n\nlibrary(dplyr): For data manipulation.\nlibrary(stringr): For string manipulation functions.\nlibrary(wordcloud) and library(wordcloud2): For creating word clouds.\nlibrary(RColorBrewer): For color palettes.\nlibrary(tm): For text mining, used to create a text corpus and term-document matrix.\n\nSet Seed for Reproducibility:\n\nset.seed(1234)\n\n\nset.seed(1234): Sets a seed for random number generation, ensuring that the word cloud has the same layout each time it is generated.\n\nGenerate Word Cloud:\n\nwordcloud_f(x_data_english, 30, 100)\n\n\n\n\n\n\n\n\n\nwordcloud_f(x_data_english, 30, 100): Calls the wordcloud_f function with the following parameters:\n\nx_data_english: The input data containing the annotated and filtered lemmas.\nmin_freq = 30: Only words with a minimum frequency of 30 will be included.\nmax_words = 100: Limits the word cloud to the top 100 words.\n\n\n\n\n\n15.2.11 Co-Occurrence Network\nThis code is preparing to create a co-occurrence network by defining a function that computes pairwise word counts based on shared document occurrences. Here’s an explanation:\n\nLoad Required Libraries:\n\n# install.packages(\"widyr\")\nlibrary(dplyr); library(widyr)\n\n\nlibrary(dplyr): For data manipulation.\nlibrary(widyr): For pairwise operations, including pairwise counts and correlations, often used for co-occurrence analysis.\n\nDefine pair_words_f Function:\n\npair_words_f &lt;- function(input) {\n  input %&gt;%\n  pairwise_count(item = lemma, feature = doc_id, sort = T)\n}\n\n\npair_words_f: This function calculates the pairwise counts of words (lemmas) that appear together in the same document.\nParameters:\n\ninput: The input dataset, which should include at least two columns: lemma (word) and doc_id (document identifier).\nitem = lemma: Specifies the word or lemma for which pairwise co-occurrence counts will be calculated.\nfeature = doc_id: Groups lemmas by their document ID, allowing the function to identify pairs of words appearing together in the same document.\nsort = T: Sorts the results in descending order by count, so the most frequently co-occurring word pairs appear at the top.\n\nReturns: A data frame with columns item1, item2, and n, where n is the count of documents in which each word pair (item1 and item2) co-occurs.\n\nOutput pair_words_f: To generate a co-occurrence network, you would call this function on a dataset:\n::: {.cell}\npairwise_data &lt;- pair_words_f(x_data_english)\n\npairwise_data\n::: {.cell-output .cell-output-stdout}\n# A tibble: 36,128 × 3\n   item1      item2          n\n   &lt;chr&gt;      &lt;chr&gt;      &lt;dbl&gt;\n 1 student    school        74\n 2 school     student       74\n 3 korean     student       73\n 4 student    korean        73\n 5 effect     student       63\n 6 student    effect        63\n 7 elementary school        61\n 8 school     elementary    61\n 9 learn      student       58\n10 student    learn         58\n# ℹ 36,118 more rows\n::: :::\n\nThis output (pairwise_data) could then be used to visualize the co-occurrence network.\n\n\n15.2.12 Degree Centrality\nThis code calculates the degree centrality of a co-occurrence network and identifies communities within the network. It filters out infrequent word pairs and saves the network data with centrality metrics. Here’s an explanation:\n\nLoad Required Library:\n\nlibrary(tidygraph)\n\n\nlibrary(tidygraph): Used to work with graph structures in a tidy data format, enabling manipulation and analysis of network data.\n\nDefine degreecentrality_f Function:\n\ndegreecentrality_f &lt;- function(input, nsize) {\n  input %&gt;%\n  filter(n &gt;= nsize) %&gt;%\n  as_tbl_graph(directed = F) %&gt;%\n  mutate(centrality = centrality_degree(),        # Calculate degree centrality\n         group = as.factor(group_infomap()))      # Identify community groups\n}\n\n\ninput: The input dataset, expected to contain word pair data (from pair_words_f) with columns item1, item2, and n, where n is the co-occurrence count.\nfilter(n &gt;= nsize): Filters the data to keep only word pairs that occur in at least nsize documents.\nas_tbl_graph(directed = F): Converts the filtered data into an undirected graph object. Each unique word is a node, and edges represent co-occurrences.\nmutate(...): Adds calculated metrics to the graph:\n\ncentrality = centrality_degree(): Calculates degree centrality for each node, measuring the number of direct connections a word has.\ngroup = as.factor(group_infomap()): Uses the Infomap algorithm to detect community structures, assigning each node to a group.\n\n\nApply degreecentrality_f and Save Output:\n\ndegreecentrality_english &lt;- degreecentrality_f(pairwise_data, 50)\nwrite.csv(degreecentrality_english, 'degreecentrality_english.csv')\n\n\ndegreecentrality_english &lt;- degreecentrality_f(pairwise_data, 50): Calls the function with x_data_english_pairwords (output from pair_words_f) and nsize = 50, retaining only word pairs appearing in at least 50 documents.\nwrite.csv(degreecentrality_english, 'degreecentrality_english.csv'): Exports the resulting graph with centrality and community data to a CSV file.\n\n\nNetwork Graph\nThis code defines a function to create a network graph based on degree centrality and then applies this function to visualize the centrality network of degreecentrality_english. Here’s a breakdown of how it works:\n\nLoad Required Library:\n\nlibrary(ggraph)\n\n\nlibrary(ggraph): Provides tools for visualizing network data using ggplot2-like syntax, which is helpful for customizing network layouts and styling.\n\nDefine centrality_network Function:\n\ncentrality_network &lt;- function(x) {\n  ggraph(x, layout = \"fr\") +      # Layout set to 'fr' (Fruchterman-Reingold)\n    geom_edge_link(color = \"gray50\", alpha = 0.5) +             # Gray edges with transparency\n    geom_node_point(aes(size = centrality, color = group), show.legend = F) + # Node size by centrality, color by group\n    scale_size(range = c(5, 15)) +            # Node size range\n    geom_node_text(aes(label = name), repel = T, size = 3, family = \"Arial\") + # Labels repelled from nodes, set font\n    theme_graph()                             # Simplified theme without gridlines\n}\n\n\nggraph(x, layout = \"fr\"): Creates a graph using the fr (Fruchterman-Reingold) layout, which positions nodes based on their connections.\ngeom_edge_link(color = \"gray50\", alpha = 0.5): Adds edges (links) between nodes, styled with a gray color and slight transparency.\ngeom_node_point(aes(size = centrality, color = group), show.legend = F): Adds nodes (words), where:\n\nsize = centrality: Sizes nodes by their degree centrality (number of direct connections).\ncolor = group: Colors nodes by their community group (detected via group_infomap() in previous code).\nshow.legend = F: Hides the legend for a cleaner look.\n\nscale_size(range = c(5, 15)): Sets the size range for nodes to make central nodes larger.\ngeom_node_text(aes(label = name), repel = T, size = 3, family = \"Arial\"): Adds labels to nodes (words) with a repelling effect to keep text outside node points.\ntheme_graph(): Applies a minimalistic theme, removing gridlines and backgrounds.\n\nGenerate and Display the Graph:\n\nset.seed(1234)\ncentrality_network(degreecentrality_english)\n\n\n\n\n\n\n\n\n\nset.seed(1234): Sets a seed to ensure the network layout is reproducible.\ncentrality_network(degreecentrality_english): Calls the centrality_network function on degreecentrality_english, displaying the degree centrality graph.\n\n\nThis code generates a degree centrality network graph of co-occurring words, where:\n\nNode size represents the degree centrality (higher degree nodes are larger).\nNode color represents community grouping. This visualization provides insight into the most connected and central words and how they cluster into different communities within the co-occurrence network.\n\n\n\n15.2.13 Correlation-based Network (Phi Coefficient)\nThis code defines functions for creating a correlation-based network using the phi coefficient and then visualizes this network based on word correlations in x_data_english. Here’s an explanation of each section:\n\nLoad Required Libraries:\n\nlibrary(dplyr); library(widyr)\n\n\nlibrary(dplyr): For data manipulation.\nlibrary(widyr): For calculating pairwise correlations, useful for identifying word relationships.\n\nDefine words_cors_f Function:\n\nwords_cors_f &lt;- function(input, nsize) {\n  input %&gt;%\n  add_count(lemma) %&gt;%\n  filter(n &gt;= nsize) %&gt;%\n  pairwise_cor(item = lemma, feature = doc_id, sort = T)\n}\n\n\nwords_cors_f calculates pairwise correlations between words that frequently co-occur in documents.\nParameters:\n\ninput: The input dataset with columns lemma (words) and doc_id (document identifier).\nnsize: The minimum occurrence threshold for a word to be included.\n\nFunction Details:\n\nadd_count(lemma): Counts occurrences of each word (lemma) across documents.\nfilter(n &gt;= nsize): Retains words that appear in at least nsize documents.\npairwise_cor(item = lemma, feature = doc_id, sort = T): Calculates pairwise correlations between words based on their document co-occurrence, with higher correlations indicating stronger associations.\n\n\nDefine graph_cors_f Function:\n\ngraph_cors_f &lt;- function(input, cor_size) {\n  input %&gt;%\n  filter(correlation &gt;= cor_size) %&gt;%\n  as_tbl_graph(directed = F) %&gt;%\n  mutate(centrality = centrality_degree(),\n         group = as.factor(group_infomap()))\n}\n\n\ngraph_cors_f filters correlations and creates a network graph object with centrality and community information.\nParameters:\n\ninput: The correlation data (output of words_cors_f).\ncor_size: Minimum correlation threshold for including word pairs.\n\nFunction Details:\n\nfilter(correlation &gt;= cor_size): Keeps only word pairs with correlations above the specified threshold.\nas_tbl_graph(directed = F): Converts the data into an undirected graph.\ncentrality = centrality_degree(): Calculates degree centrality for each word node.\ngroup = as.factor(group_infomap()): Identifies community groups within the network.\n\n\nDefine word_network_cor_f Function for Visualization:\n\nword_network_cor_f &lt;- function(input, cor_size) {\n  graph_cor &lt;- graph_cors_f(input, cor_size) \n  ggraph(graph_cor, layout = \"fr\") +\n    geom_edge_link(color = \"gray50\", aes(edge_alpha = correlation, edge_width = correlation), show.legend = F) +\n    scale_edge_width(range = c(1, 4)) +\n    geom_node_point(aes(size = centrality, color = group), show.legend = F) +\n    scale_size(range = c(5, 10)) +\n    geom_node_text(aes(label = name), repel = T, size = 3, family = \"Arial\") +\n    theme_graph()\n}\n\n\nword_network_cor_f visualizes the correlation network.\nParameters:\n\ninput: Correlation data.\ncor_size: Minimum correlation threshold for displaying connections.\n\nFunction Details:\n\ngraph_cor &lt;- graph_cors_f(input, cor_size): Generates the graph with nodes and edges based on the specified correlation size.\nggraph(..., layout = \"fr\"): Sets up a Fruchterman-Reingold layout.\nEdges and Nodes:\n\ngeom_edge_link(...): Adds edges between nodes, with transparency and width proportional to correlation strength.\ngeom_node_point(...): Sizes nodes by centrality and colors them by group.\ngeom_node_text(...): Labels each node with the word, using a repelling effect to avoid overlap.\n\n\n\nGenerate Correlation Data and Visualize:\n\nset.seed(1234)\nwords_cors_english &lt;- words_cors_f(x_data_english, 50)\nwords_cors_english\n\n# A tibble: 552 × 3\n   item1      item2      correlation\n   &lt;chr&gt;      &lt;chr&gt;            &lt;dbl&gt;\n 1 elementary school           0.412\n 2 school     elementary       0.412\n 3 efl        learner          0.280\n 4 learner    efl              0.280\n 5 university student          0.269\n 6 student    university       0.269\n 7 college    student          0.252\n 8 student    college          0.252\n 9 vocabulary learn            0.233\n10 learn      vocabulary       0.233\n# ℹ 542 more rows\n\n# write.csv(words_cors_english, 'word_cors_all.csv')\n\n\nGenerate Correlation Data:\n\nwords_cors_f(x_data_english, 50): Calculates correlations between words that appear in at least 50 documents.\nwrite.csv(...): Saves the full correlation data if needed.\n\n\n\nset.seed(1234)\nword_network_cor_f(words_cors_english, 0.15)\n\n\n\n\n\n\n\n\n\nword_network_cor_f(words_cors_english, 0.15): Visualizes words with correlations of at least 0.15.\n\n\n\n\n15.2.14 n-gram Network\nThis code creates and visualizes an n-gram network, specifically focusing on bigrams (two-word sequences). It defines functions to generate and process n-grams, build a network graph based on the frequency of n-grams, and visualize the network with centrality information. Here’s a breakdown:\n\nLoad Required Libraries:\n\nlibrary(stringr); library(dplyr); library(tidyr); library(tidytext); library(igraph)\n\n\nAttaching package: 'tidyr'\n\n\nThe following object is masked from 'package:igraph':\n\n    crossing\n\n\n\nlibrary(tidytext): Provides text tokenization tools, including functions to create n-grams.\nlibrary(igraph): Offers functions for calculating graph properties like closeness centrality.\n\nDefine data_ngram_separated_f Function:\n\ndata_ngram_separated_f &lt;- function(x_data_input, ngram_size) {\n  x_data_input %&gt;%\n  group_by(doc_id) %&gt;%\n  summarise(sentence = paste(lemma, collapse = \" \")) %&gt;%\n  unnest_tokens(input = sentence, output = bigram, token = \"ngrams\", n = ngram_size) %&gt;%\n  separate(bigram, c(\"word1\", \"word2\"), sep = \" \") %&gt;% # Separate bigram into individual words\n  count(word1, word2, sort = T) %&gt;% na.omit() # Count n-gram frequencies\n}\n\n\nParameters:\n\nx_data_input: The input dataset with columns lemma (words) and doc_id (document identifier).\nngram_size: Specifies the size of the n-gram (e.g., 2 for bigrams).\n\nFunction Details:\n\ngroup_by(doc_id) and summarise(...): Combines each document’s lemmas into a single sentence.\nunnest_tokens(...): Extracts n-grams from the combined sentence, storing them in the bigram column.\nseparate(bigram, c(\"word1\", \"word2\"), sep = \" \"): Splits the bigram into two columns, word1 and word2.\ncount(word1, word2, sort = T) %&gt;% na.omit(): Counts each word pair’s frequency and removes any missing values.\n\n\nDefine data_ngram_graph_f Function:\n\ndata_ngram_graph_f &lt;- function(ngram_separated, filter_size) {\n  ngram_separated %&gt;%\n  filter(n &gt;= filter_size) %&gt;%\n  as_tbl_graph(directed = F) %&gt;%\n  mutate(centrality = centrality_degree(), group = as.factor(group_infomap()))\n}\n\n\nParameters:\n\nngram_separated: Output from data_ngram_separated_f, containing n-gram frequencies.\nfilter_size: Minimum frequency threshold to include n-grams in the network.\n\nFunction Details:\n\nfilter(n &gt;= filter_size): Filters word pairs by frequency.\nas_tbl_graph(directed = F): Converts the filtered data to an undirected graph.\nmutate(...):\n\ncentrality = centrality_degree(): Calculates the degree centrality of each word.\ngroup = as.factor(group_infomap()): Uses the Infomap algorithm to detect communities within the graph.\n\n\n\nDefine word_network_ngram_f Function:\n\nword_network_ngram_f &lt;- function(data_ngram_graph) {\n  ggraph(data_ngram_graph, layout = \"fr\") +\n    geom_edge_link(color = \"gray50\", alpha = 0.5) +\n    geom_node_point(aes(size = centrality, color = group), show.legend = F) +\n    scale_size(range = c(4, 8)) +\n    geom_node_text(aes(label = name), repel = T, size = 3, family = \"Arial\") +\n    theme_graph()\n}\n\n\nParameters:\n\ndata_ngram_graph: Graph object created by data_ngram_graph_f.\n\nFunction Details:\n\ngeom_edge_link(...): Draws edges with gray color and transparency.\ngeom_node_point(...): Sizes nodes by degree centrality and colors them by community.\ngeom_node_text(...): Labels nodes with word names.\ntheme_graph(): Simplifies the plot appearance by removing background elements.\n\n\nGenerate and Visualize N-Gram Data: Bigram\n\ndata_english_bigram &lt;- data_ngram_separated_f(x_data_english, 2)\n# write.csv(data_english_bigram, 'data_english_bigram.csv')\ndata_ngram_graph_english &lt;- data_ngram_graph_f(data_english_bigram, 5)\nword_network_ngram_f(data_ngram_graph_english)\n\n\n\n\n\n\n\n\nTrigram\n\ndata_english_trigram &lt;- data_ngram_separated_f(x_data_english, 3)\n\nWarning: Expected 2 pieces. Additional pieces discarded in 5728 rows [1, 2, 3, 4, 5, 6,\n7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, ...].\n\ndata_3gram_graph_english &lt;- data_ngram_graph_f(data_english_trigram, 4)\nword_network_ngram_f(data_3gram_graph_english)\n\n\n\n\n\n\n\n\n\nGenerate N-Gram Data:\n\ndata_ngram_separated_f(x_data_english, 2): Creates bigrams from x_data_english and counts their occurrences.\nwrite.csv(data_english_bigram, 'data_english_bigram.csv'): Exports bigram frequency data.\n\nCreate N-Gram Graph:\n\ndata_ngram_graph_f(data_english_bigram, 5): Builds a graph including bigrams with a minimum frequency of 5.\n\nVisualize N-Gram Network:\n\nword_network_ngram_f(data_ngram_graph_english): Visualizes the n-gram network, with nodes sized by centrality and colored by community.\n\n\n\n\n\n15.2.15 Modularity Analysis\nModularity is a measure used in network analysis to identify the strength of division of a network into communities or modules. A higher modularity score suggests stronger community structure, with more connections within communities and fewer between them. In R, we can use the Infomap, Louvain, or Walktrap algorithms to detect communities and then calculate modularity.\n\n15.2.15.1 Infomap Algorithm\n\nPrinciple: Infomap is based on information theory. It uses a random walk approach to detect communities by minimizing the expected description length of a random walk on the network.\nProcess:\n\nThe algorithm simulates a random walk over the network and assigns nodes to communities by tracking how information flows between them.\nIt minimizes a map equation, which represents the amount of information needed to describe the path of the random walk.\nBy assigning nodes to communities where information flow is dense, it reduces the total map description length.\n\nCharacteristics:\n\nEffective for detecting communities in networks with hierarchical or nested structures.\nSensitive to the direction of information flow, making it useful for directed networks.\n\nUsage: Often used in large, complex networks, such as social and biological networks.\n\n\n\n15.2.15.2 Louvain Algorithm\n\nPrinciple: Louvain is based on modularity optimization, where modularity is a measure of the density of links within communities compared to links between communities.\nProcess:\n\nThe algorithm begins by assigning each node to its own community.\nIt then iteratively merges nodes into communities to maximize modularity gain.\nOnce no further modularity improvements are possible, each community is collapsed into a “super-node,” and the process repeats until a maximum modularity is reached.\n\nCharacteristics:\n\nFast and scalable, making it suitable for large networks.\nTends to produce communities of varying sizes, which can sometimes split large communities into smaller ones.\n\nUsage: Commonly used for undirected networks, such as social networks, biological networks, and citation networks.\n\n\n\n15.2.15.3 Walktrap Algorithm\n\nPrinciple: Walktrap is also based on random walks, but unlike Infomap, it uses short random walks to detect communities.\nProcess:\n\nThe algorithm performs short random walks starting from each node to gather information about the structure of the network.\nThe assumption is that nodes within the same community are more likely to be reached by short random walks than nodes in different communities.\nIt merges communities iteratively by calculating the similarity between nodes based on these random walks and minimizing the distance between communities.\n\nCharacteristics:\n\nProduces more balanced community sizes compared to Louvain.\nMore computationally intensive than Louvain, so it is less suited for extremely large networks.\n\nUsage: Suitable for smaller to medium-sized networks where the structure is not overly hierarchical.\n\n\n\n\n15.2.15.4 Comparison of the Algorithms\n\n\n\n\n\n\n\n\n\nAlgorithm\nKey Concept\nBest for\nLimitations\n\n\n\n\nInfomap\nInformation theory and random walk optimization\nLarge, complex networks; directed networks\nSensitive to direction, may be less interpretable in undirected networks\n\n\nLouvain\nModularity optimization\nLarge networks; undirected networks\nTends to split large communities; hierarchical structure not well captured\n\n\nWalktrap\nShort random walks to find closely connected nodes\nSmall to medium-sized networks; balanced community sizes\nComputationally intensive for large networks\n\n\n\nEach algorithm is tailored to different types of networks, structures, and analysis goals. Generally:\n\nInfomap is well-suited for networks with hierarchical community structures or directed edges.\nLouvain is a fast, general-purpose method for large, undirected networks.\nWalktrap is useful when balanced community sizes are preferred, but it’s more computationally demanding.\n\nHere’s how to perform modularity analysis on an n-gram co-occurrence network.\n\n\n15.2.15.5 Step-by-Step Modularity Analysis with Network Data in R: Part 1\nThis code assumes you have an n-gram network graph, such as the one generated in the previous example with bigram data.\n\nLoad Required Libraries\n\nlibrary(igraph)\nlibrary(tidygraph)\nlibrary(ggraph)\n\nCommunity Detection and Modularity Calculation Function We can use different algorithms to detect communities and then calculate the modularity for each detected community structure. Here, we’ll use the Louvain and Infomap methods.\n\nmodularity_analysis &lt;- function(graph) {\n  # Louvain community detection\n  louvain_comm &lt;- cluster_louvain(graph)\n  modularity_louvain &lt;- modularity(louvain_comm)\n\n  # Infomap community detection\n  infomap_comm &lt;- cluster_infomap(graph)\n  modularity_infomap &lt;- modularity(infomap_comm)\n\n  # Summary output\n  list(\n    Louvain_Modularity = modularity_louvain,\n    Infomap_Modularity = modularity_infomap,\n    Louvain_Communities = louvain_comm,\n    Infomap_Communities = infomap_comm\n  )\n}\n\nPrepare and Analyze the Network Graph\nAssuming you have already created a graph with as_tbl_graph() from your n-gram data, you can now calculate modularity:\n\n# Prepare your graph object\n# Example: data_ngram_graph_english &lt;- data_ngram_graph_f(data_english_bigram, 5)\n\n# Convert to igraph for modularity analysis\nigraph_graph &lt;- as.igraph(data_ngram_graph_english)\n\n# Perform modularity analysis\nmodularity_results &lt;- modularity_analysis(igraph_graph)\n\n# Display modularity scores\nprint(modularity_results$Louvain_Modularity)\n\n[1] 0.6131639\n\nprint(modularity_results$Infomap_Modularity)\n\n[1] 0.587661\n\n\nVisualize Communities with Modularity\nTo see the detected communities in a network plot, we can map community labels from the Louvain or Infomap methods as node colors.\n\n# Choose Louvain communities for visualization\nV(igraph_graph)$community &lt;- membership(modularity_results$Louvain_Communities)\n\n# Plot with communities as colors\nggraph(igraph_graph, layout = \"fr\") +\n  geom_edge_link(color = \"gray80\", alpha = 0.5) +\n  geom_node_point(aes(color = as.factor(community)), size = 5) +\n  geom_node_text(aes(label = name), repel = TRUE, size = 3, family = \"Arial\") +\n  theme_graph() +\n  labs(title = \"N-Gram Network with Louvain Community Detection\",\n       subtitle = paste(\"Modularity:\", round(modularity_results$Louvain_Modularity, 3)))\n\n\n\n\n\n\n\n\n\nThis approach helps identify well-defined clusters within a network, where higher modularity values indicate strong community structures, which is essential in understanding patterns within complex networks.\n\n\n15.2.15.6 Step-by-Step Modularity Analysis with Network Data in R: Part 2\nTo enhance the modularity visualization with community grouping circles, we can modify the code to add a convex hull around each community group, creating visual boundaries for each detected community. Here’s the revised code:\n\n#install.packages(\"ggforce\")  \n#install.packages(\"concaveman\")\n\n\n# Load necessary libraries\nlibrary(igraph)\nlibrary(tidygraph)\nlibrary(ggraph)\nlibrary(ggforce)\nlibrary(concaveman)\n\n# Define function for community detection and modularity analysis\nmodularity_analysis &lt;- function(graph) {\n  # Louvain community detection\n  louvain_comm &lt;- cluster_louvain(graph)\n  modularity_louvain &lt;- modularity(louvain_comm)\n  \n  # Infomap community detection\n  infomap_comm &lt;- cluster_infomap(graph)\n  modularity_infomap &lt;- modularity(infomap_comm)\n  \n  # Return a list of results\n  list(\n    Louvain_Modularity = modularity_louvain,\n    Infomap_Modularity = modularity_infomap,\n    Louvain_Communities = louvain_comm,\n    Infomap_Communities = infomap_comm\n  )\n}\n\n# Example: Convert graph to igraph object if needed and perform modularity analysis\nigraph_graph &lt;- as.igraph(data_ngram_graph_english)\nmodularity_results &lt;- modularity_analysis(igraph_graph)\n\n# Add Louvain communities to graph for visualization\nV(igraph_graph)$community &lt;- membership(modularity_results$Louvain_Communities)\n\n# Define function to visualize the network with grouping circles\nword_network_with_community_circles &lt;- function(graph) {\n  ggraph(graph, layout = \"fr\") +\n    # Add community boundaries using convex hulls\n    geom_mark_hull(aes(x = x, y = y, group = as.factor(community), fill = as.factor(community)),\n                   concavity = 4, color = \"black\", alpha = 0.2) +\n    # Add edges\n    geom_edge_link(color = \"gray80\", alpha = 0.5) +\n    # Add nodes, coloring by community\n    geom_node_point(aes(color = as.factor(community)), size = 5) +\n    # Add node labels\n    geom_node_text(aes(label = name), repel = TRUE, size = 3, family = \"Arial\") +\n    # Apply theme and labels\n    theme_graph() +\n    labs(title = \"N-Gram Network with Community Detection\",\n         subtitle = paste(\"Louvain Modularity:\", round(modularity_results$Louvain_Modularity, 3)))\n}\n\n# Visualize the network with community grouping circles\nset.seed(1234)\nword_network_with_community_circles(igraph_graph)\n\n\n\n\n\n\n\n\n\n\n\n15.2.16 Explanation of Modifications\n\nCommunity Detection and Modularity Calculation:\n\nThe modularity_analysis function detects communities using both the Louvain and Infomap algorithms and calculates modularity scores.\nWe add the Louvain community membership to the graph as a community attribute for each node.\n\nAdding Community Grouping Circles:\n\nIn the word_network_with_community_circles function:\n\ngeom_mark_hull(...): Draws convex hulls around nodes in the same community, creating a boundary or grouping circle around each community.\n\naes(group = as.factor(community)): Groups nodes by community.\nfill = as.factor(community): Assigns a fill color to each community group for better visual separation.\nconcavity = 4: Adjusts the shape of the boundary. Smaller values make the hull tighter.\nalpha = 0.2: Adds transparency to the hulls for a softer visual effect.\n\nOther ggraph elements (edges, nodes, labels) are customized to show community structure more clearly.\n\n\nVisualize with Community Circles:\n\nCall the word_network_with_community_circles function with igraph_graph to generate a plot that includes community boundaries as circles around groups of nodes.\n\n\nThis code produces a network visualization with visible community clusters, highlighted by color-filled grouping circles, enhancing the modularity-based community structure analysis.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Semantic Network Analysis (Examples)</span>"
    ]
  },
  {
    "objectID": "15.SNA_Example.html#modularity-analysis",
    "href": "15.SNA_Example.html#modularity-analysis",
    "title": "15  Semantic Network Analysis using R (.rmd)",
    "section": "15.3 Modularity Analysis",
    "text": "15.3 Modularity Analysis\nModularity is a measure used in network analysis to identify the strength of division of a network into communities or modules. A higher modularity score suggests stronger community structure, with more connections within communities and fewer between them. In R, we can use the Infomap, Louvain, or Walktrap algorithms to detect communities and then calculate modularity.\n\n15.3.1 Infomap Algorithm\n\nPrinciple: Infomap is based on information theory. It uses a random walk approach to detect communities by minimizing the expected description length of a random walk on the network.\nProcess:\n\nThe algorithm simulates a random walk over the network and assigns nodes to communities by tracking how information flows between them.\nIt minimizes a map equation, which represents the amount of information needed to describe the path of the random walk.\nBy assigning nodes to communities where information flow is dense, it reduces the total map description length.\n\nCharacteristics:\n\nEffective for detecting communities in networks with hierarchical or nested structures.\nSensitive to the direction of information flow, making it useful for directed networks.\n\nUsage: Often used in large, complex networks, such as social and biological networks.\n\n\n\n15.3.2 Louvain Algorithm\n\nPrinciple: Louvain is based on modularity optimization, where modularity is a measure of the density of links within communities compared to links between communities.\nProcess:\n\nThe algorithm begins by assigning each node to its own community.\nIt then iteratively merges nodes into communities to maximize modularity gain.\nOnce no further modularity improvements are possible, each community is collapsed into a “super-node,” and the process repeats until a maximum modularity is reached.\n\nCharacteristics:\n\nFast and scalable, making it suitable for large networks.\nTends to produce communities of varying sizes, which can sometimes split large communities into smaller ones.\n\nUsage: Commonly used for undirected networks, such as social networks, biological networks, and citation networks.\n\n\n\n15.3.3 Walktrap Algorithm\n\nPrinciple: Walktrap is also based on random walks, but unlike Infomap, it uses short random walks to detect communities.\nProcess:\n\nThe algorithm performs short random walks starting from each node to gather information about the structure of the network.\nThe assumption is that nodes within the same community are more likely to be reached by short random walks than nodes in different communities.\nIt merges communities iteratively by calculating the similarity between nodes based on these random walks and minimizing the distance between communities.\n\nCharacteristics:\n\nProduces more balanced community sizes compared to Louvain.\nMore computationally intensive than Louvain, so it is less suited for extremely large networks.\n\nUsage: Suitable for smaller to medium-sized networks where the structure is not overly hierarchical.\n\n\n\n\n15.3.4 Comparison of the Algorithms\n\n\n\n\n\n\n\n\n\nAlgorithm\nKey Concept\nBest for\nLimitations\n\n\n\n\nInfomap\nInformation theory and random walk optimization\nLarge, complex networks; directed networks\nSensitive to direction, may be less interpretable in undirected networks\n\n\nLouvain\nModularity optimization\nLarge networks; undirected networks\nTends to split large communities; hierarchical structure not well captured\n\n\nWalktrap\nShort random walks to find closely connected nodes\nSmall to medium-sized networks; balanced community sizes\nComputationally intensive for large networks\n\n\n\nEach algorithm is tailored to different types of networks, structures, and analysis goals. Generally:\n\nInfomap is well-suited for networks with hierarchical community structures or directed edges.\nLouvain is a fast, general-purpose method for large, undirected networks.\nWalktrap is useful when balanced community sizes are preferred, but it’s more computationally demanding.\n\nHere’s how to perform modularity analysis on an n-gram co-occurrence network.\n\n\n15.3.5 Step-by-Step Modularity Analysis with Network Data in R\nThis code assumes you have an n-gram network graph, such as the one generated in the previous example with bigram data.\n\nLoad Required Libraries\n\nlibrary(igraph)\nlibrary(tidygraph)\nlibrary(ggraph)\n\nCommunity Detection and Modularity Calculation Function We can use different algorithms to detect communities and then calculate the modularity for each detected community structure. Here, we’ll use the Louvain and Infomap methods.\n\nmodularity_analysis &lt;- function(graph) {\n  # Louvain community detection\n  louvain_comm &lt;- cluster_louvain(graph)\n  modularity_louvain &lt;- modularity(louvain_comm)\n\n  # Infomap community detection\n  infomap_comm &lt;- cluster_infomap(graph)\n  modularity_infomap &lt;- modularity(infomap_comm)\n\n  # Summary output\n  list(\n    Louvain_Modularity = modularity_louvain,\n    Infomap_Modularity = modularity_infomap,\n    Louvain_Communities = louvain_comm,\n    Infomap_Communities = infomap_comm\n  )\n}\n\nPrepare and Analyze the Network Graph\nAssuming you have already created a graph with as_tbl_graph() from your n-gram data, you can now calculate modularity:\n\n# Prepare your graph object\n# Example: data_ngram_graph_english &lt;- data_ngram_graph_f(data_english_bigram, 5)\n\n# Convert to igraph for modularity analysis\nigraph_graph &lt;- as.igraph(data_ngram_graph_english)\n\n# Perform modularity analysis\nmodularity_results &lt;- modularity_analysis(igraph_graph)\n\n# Display modularity scores\nprint(modularity_results$Louvain_Modularity)\n\n[1] 0.6131639\n\nprint(modularity_results$Infomap_Modularity)\n\n[1] 0.587661\n\n\nVisualize Communities with Modularity\nTo see the detected communities in a network plot, we can map community labels from the Louvain or Infomap methods as node colors.\n\n# Choose Louvain communities for visualization\nV(igraph_graph)$community &lt;- membership(modularity_results$Louvain_Communities)\n\n# Plot with communities as colors\nggraph(igraph_graph, layout = \"fr\") +\n  geom_edge_link(color = \"gray80\", alpha = 0.5) +\n  geom_node_point(aes(color = as.factor(community)), size = 5) +\n  geom_node_text(aes(label = name), repel = TRUE, size = 3, family = \"Arial\") +\n  theme_graph() +\n  labs(title = \"N-Gram Network with Louvain Community Detection\",\n       subtitle = paste(\"Modularity:\", round(modularity_results$Louvain_Modularity, 3)))\n\n\n\n\n\n\n\n\n\nThis approach helps identify well-defined clusters within a network, where higher modularity values indicate strong community structures, which is essential in understanding patterns within complex networks.\n\n\n15.3.6 Step-by-Step Modularity Analysis with Network Data in R: Part 2\nTo enhance the modularity visualization with community grouping circles, we can modify the code to add a convex hull around each community group, creating visual boundaries for each detected community. Here’s the revised code:\n\n#install.packages(\"ggforce\")  \n#install.packages(\"concaveman\")\n\n\n# Load necessary libraries\nlibrary(igraph)\nlibrary(tidygraph)\nlibrary(ggraph)\nlibrary(ggforce)\nlibrary(concaveman)\n\n# Define function for community detection and modularity analysis\nmodularity_analysis &lt;- function(graph) {\n  # Louvain community detection\n  louvain_comm &lt;- cluster_louvain(graph)\n  modularity_louvain &lt;- modularity(louvain_comm)\n  \n  # Infomap community detection\n  infomap_comm &lt;- cluster_infomap(graph)\n  modularity_infomap &lt;- modularity(infomap_comm)\n  \n  # Return a list of results\n  list(\n    Louvain_Modularity = modularity_louvain,\n    Infomap_Modularity = modularity_infomap,\n    Louvain_Communities = louvain_comm,\n    Infomap_Communities = infomap_comm\n  )\n}\n\n# Example: Convert graph to igraph object if needed and perform modularity analysis\nigraph_graph &lt;- as.igraph(data_ngram_graph_english)\nmodularity_results &lt;- modularity_analysis(igraph_graph)\n\n# Add Louvain communities to graph for visualization\nV(igraph_graph)$community &lt;- membership(modularity_results$Louvain_Communities)\n\n# Define function to visualize the network with grouping circles\nword_network_with_community_circles &lt;- function(graph) {\n  ggraph(graph, layout = \"fr\") +\n    # Add community boundaries using convex hulls\n    geom_mark_hull(aes(x = x, y = y, group = as.factor(community), fill = as.factor(community)),\n                   concavity = 4, color = \"black\", alpha = 0.2) +\n    # Add edges\n    geom_edge_link(color = \"gray80\", alpha = 0.5) +\n    # Add nodes, coloring by community\n    geom_node_point(aes(color = as.factor(community)), size = 5) +\n    # Add node labels\n    geom_node_text(aes(label = name), repel = TRUE, size = 3, family = \"Arial\") +\n    # Apply theme and labels\n    theme_graph() +\n    labs(title = \"N-Gram Network with Community Detection\",\n         subtitle = paste(\"Louvain Modularity:\", round(modularity_results$Louvain_Modularity, 3)))\n}\n\n# Visualize the network with community grouping circles\nset.seed(1234)\nword_network_with_community_circles(igraph_graph)\n\n\n\n\n\n\n\n\n\n\n15.3.7 Explanation of Modifications\n\nCommunity Detection and Modularity Calculation:\n\nThe modularity_analysis function detects communities using both the Louvain and Infomap algorithms and calculates modularity scores.\nWe add the Louvain community membership to the graph as a community attribute for each node.\n\nAdding Community Grouping Circles:\n\nIn the word_network_with_community_circles function:\n\ngeom_mark_hull(...): Draws convex hulls around nodes in the same community, creating a boundary or grouping circle around each community.\n\naes(group = as.factor(community)): Groups nodes by community.\nfill = as.factor(community): Assigns a fill color to each community group for better visual separation.\nconcavity = 4: Adjusts the shape of the boundary. Smaller values make the hull tighter.\nalpha = 0.2: Adds transparency to the hulls for a softer visual effect.\n\nOther ggraph elements (edges, nodes, labels) are customized to show community structure more clearly.\n\n\nVisualize with Community Circles:\n\nCall the word_network_with_community_circles function with igraph_graph to generate a plot that includes community boundaries as circles around groups of nodes.\n\n\nThis code produces a network visualization with visible community clusters, highlighted by color-filled grouping circles, enhancing the modularity-based community structure analysis.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Semantic Network Analysis using R (Example)</span>"
    ]
  },
  {
    "objectID": "B.assignment2.html",
    "href": "B.assignment2.html",
    "title": "19  Assignment #02: Semantic Network Analysis",
    "section": "",
    "text": "19.1 Instructions:\nThis assignment focuses on conducting a semantic network analysis of a selected corpus. Follow the steps below to complete the assignment, and submit your work as instructed by your instructor. Ensure clarity in your analysis and interpretation.",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Assignment #02: Semantic Network Analysis</span>"
    ]
  },
  {
    "objectID": "15.SNA_Example.html#preparation-r",
    "href": "15.SNA_Example.html#preparation-r",
    "title": "15  Semantic Network Analysis (Examples)",
    "section": "",
    "text": "15.1.1 Step 1: Install R\n\nGo to CRAN: Visit the Comprehensive R Archive Network (CRAN) here.\nDownload R:\n\nSelect your operating system (Windows, macOS, or Linux).\nFollow the download and installation instructions specific to your OS.\n\nInstall R:\n\nRun the downloaded file and follow the on-screen instructions.\nOnce complete, R should be ready to use.\n\n\n\n\n15.1.2 Step 2: Install RStudio\n\nGo to RStudio’s Website: Visit RStudio’s download page.\nDownload RStudio:\n\nChoose the free version of RStudio Desktop.\nDownload the version appropriate for your operating system.\n\nInstall RStudio:\n\nRun the downloaded file and follow the installation instructions.\nAfter installation, open RStudio to confirm it’s working and connected to R.\n\n\n\n\n15.1.3 Step 3: Install Required Packages for Semantic Network Analysis\nOpen RStudio and install the necessary packages for semantic network analysis. Here’s how:\n\nOpen RStudio.\nInstall packages by typing the following commands in the Console and pressing Enter after each line:\n\n\n# install.packages(\"igraph\")         # Network visualization and analysis\n# install.packages(\"quanteda\")       # Text analysis\n# install.packages(\"quanteda.textstats\") # Text statistics for semantic networks\n# install.packages(\"widyr\")          # Pairwise correlations for networks\n# install.packages(\"ggraph\")         # Network graphing tool with ggplot2 syntax\n# install.packages(\"tidygraph\")      # Manipulate network data in tidy format\n# install.packages(\"ggplot2\")\n\n\nLoad packages to check if they’re correctly installed:\n\n\nlibrary(igraph)\n\nWarning: package 'igraph' was built under R version 4.3.3\n\n\n\nAttaching package: 'igraph'\n\n\nThe following objects are masked from 'package:stats':\n\n    decompose, spectrum\n\n\nThe following object is masked from 'package:base':\n\n    union\n\nlibrary(quanteda)\n\nWarning: package 'quanteda' was built under R version 4.3.3\n\n\nWarning in .recacheSubclasses(def@className, def, env): undefined subclass\n\"ndiMatrix\" of class \"replValueSp\"; definition not updated\n\n\nPackage version: 4.1.0\nUnicode version: 14.0\nICU version: 71.1\n\n\nParallel computing: disabled\n\n\nSee https://quanteda.io for tutorials and examples.\n\nlibrary(quanteda.textstats)\n\nWarning: package 'quanteda.textstats' was built under R version 4.3.3\n\nWarning: undefined subclass \"ndiMatrix\" of class \"replValueSp\"; definition not\nupdated\n\nlibrary(widyr)\nlibrary(ggraph)\n\nLoading required package: ggplot2\n\nlibrary(tidygraph)\n\n\nAttaching package: 'tidygraph'\n\n\nThe following object is masked from 'package:quanteda':\n\n    convert\n\n\nThe following object is masked from 'package:igraph':\n\n    groups\n\n\nThe following object is masked from 'package:stats':\n\n    filter",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Semantic Network Analysis (Examples)</span>"
    ]
  },
  {
    "objectID": "A.assignment1.html",
    "href": "A.assignment1.html",
    "title": "18  Assignment #01",
    "section": "",
    "text": "18.1 Assignment 01: Web Scraping and YouTube Subtitle Extraction",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Assignment #01</span>"
    ]
  },
  {
    "objectID": "A.assignment1.html#assignment-01-web-scraping-and-youtube-subtitle-extraction",
    "href": "A.assignment1.html#assignment-01-web-scraping-and-youtube-subtitle-extraction",
    "title": "18  Assignment #01",
    "section": "",
    "text": "18.1.1 Total Marks: 20\n\n\n18.1.2 Objective:\nIn this assignment, you will use Python to perform web scraping and extract subtitles from YouTube playlists. You will scrape text from a website over multiple pages, process the text, and save it in an Excel file. Additionally, you will extract and process subtitles from videos in a YouTube playlist, saving the results in Excel format.\n\n\n18.1.3 Assignment Instructions:",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Assignment #01</span>"
    ]
  },
  {
    "objectID": "A.assignment1.html#task-1-web-scraping-10-marks",
    "href": "A.assignment1.html#task-1-web-scraping-10-marks",
    "title": "18  Assignment #01",
    "section": "18.2 Task 1: Web Scraping (10 Marks)",
    "text": "18.2 Task 1: Web Scraping (10 Marks)\n\n18.2.1 Step 1: Select a Website for Crawling\nSelect a website from which you will scrape data. The website must have multiple pages that can be crawled.\nExplain why you selected this website (e.g., relevance, data availability, structure).\n\n\n18.2.2 Step 2: Crawl Data from More than 2 Pages (4 Marks)\nWrite a Python script using requests, BeautifulSoup, or other relevant libraries to scrape data from at least two pages of the selected website. Ensure that your script can navigate between pages (pagination).\n\n\n18.2.3 Step 3: Preprocess the Scraped Text and Save to Excel (xlsx) (4 Marks)\n\nPreprocess the scraped text data (e.g., remove unnecessary characters, clean formatting).\nSave the processed text in an Excel file (.xlsx format) using pandas or openpyxl.",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Assignment #01</span>"
    ]
  },
  {
    "objectID": "A.assignment1.html#task-2-youtube-playlist-subtitle-extraction-10-marks",
    "href": "A.assignment1.html#task-2-youtube-playlist-subtitle-extraction-10-marks",
    "title": "18  Assignment #01",
    "section": "18.3 Task 2: YouTube Playlist Subtitle Extraction (10 Marks)",
    "text": "18.3 Task 2: YouTube Playlist Subtitle Extraction (10 Marks)\n\n18.3.1 Step 1: Select a YouTube Playlist (2 Marks)\n\nSelect a YouTube playlist that contains videos with subtitles.\nExplain why you chose this playlist (e.g., topic relevance, video variety).\n\n\n\n18.3.2 Step 2: Extract Subtitles from More than 2 Videos (4 Marks)\n\nWrite a Python script using pytube, youtube-transcript-api, or other relevant libraries to extract subtitles from at least two videos in the playlist.\nPrint or log the extracted subtitles.\n\n\n\n18.3.3 Step 3: Preprocess Subtitles and Save to Excel (xlsx) (4 Marks)\n\nPreprocess the extracted subtitles (e.g., remove unnecessary characters, clean formatting, timestamp adjustments).\nSave the cleaned subtitles in an Excel file (.xlsx format) using pandas or openpyxl.\n\n\n\n\n18.3.4 Submission Guidelines:\n\nTask 1:\n\nSubmit the Python script used for web scraping and text processing. The .ipynb file should include an explanation of why you chose the website using markdown.\nSubmit the Excel file containing the processed text data.\n\nTask 2:\n\nSubmit the Python script used for YouTube subtitle extraction and processing. The .ipynb file should include an explanation of why you chose the playlist using markdown.\nSubmit the Excel file containing the processed subtitles.\n\n\n\n\n18.3.5 Grading Criteria:\n\n\n\nTask\nMarks\n\n\n\n\nTask 1: Website Selection & Explanation\n2\n\n\nTask 1: Crawl Data from Multiple Pages\n4\n\n\nTask 1: Preprocess and Save to Excel\n4\n\n\nTask 2: YouTube Playlist Selection & Explanation\n2\n\n\nTask 2: Extract Subtitles from Multiple Videos\n4\n\n\nTask 2: Preprocess Subtitles and Save to Excel\n4\n\n\nTotal\n20\n\n\n\n\n\n18.3.6 Submission Deadline:\n\nPlease submit your assignment by Oct. 10, 2024 via the course portal.",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Assignment #01</span>"
    ]
  },
  {
    "objectID": "B.assignment2.html#steps-for-the-assignment",
    "href": "B.assignment2.html#steps-for-the-assignment",
    "title": "19  Assignment #02: Semantic Network Analysis",
    "section": "19.2 Steps for the Assignment",
    "text": "19.2 Steps for the Assignment\n\nStep 1. Select Your Corpus (5 points)\n\nCorpus Selection:\n\nChoose a text corpus that interests you. Potential sources include:\n\nProject Gutenberg - a wide range of public domain books.\nKaggle Datasets - various datasets that may include textual data.\n\nAlternatively, you may use any other text-based dataset.\n\nCorpus Details:\n\nProvide a brief description of your selected corpus (1-2 sentences).\nMention the source and why you selected this corpus (1-2 sentences).\n\n\n\nStep 2. Conduct Semantic Network Analysis (15 points)\n\nPreprocessing (4 points):\n\nPreprocess the text to prepare it for analysis. Typical steps may include:\n\nTokenization (splitting text into words or phrases).\nStop word removal.\nLemmatization or stemming.\n\nDescribe the preprocessing steps in your final submission.\n\nNetwork Creation (6 points):\n\nConstruct a semantic network in which nodes represent words or phrases, and edges represent co-occurrence or semantic similarity.\nUse network analysis libraries such as NetworkX or igraph in Python.\nProvide a clear, labeled visualization of your semantic network.\n\nModularity Analysis with Louvain Algorithm (5 points):\n\nApply the Louvain algorithm to identify communities or clusters within the network.\nInterpret the results, explaining any major clusters or modules detected. Describe what these clusters might indicate about the thematic or structural aspects of your corpus.\nInclude a visualization that highlights the modularity structure, if possible.",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Assignment #02: Semantic Network Analysis</span>"
    ]
  },
  {
    "objectID": "B.assignment2.html#submission-guidelines",
    "href": "B.assignment2.html#submission-guidelines",
    "title": "19  Assignment #02: Semantic Network Analysis",
    "section": "19.3 Submission Guidelines",
    "text": "19.3 Submission Guidelines\n\nReport Format: Submit PDF (or MS-Word) report (2 pages, approximately 500 words), including screenshots of any visualizations.\nPython Code: Attach your annotated Python code as a separate Jupyter Notebook (.ipynb) file.\nDue Date: Submit your assignment by November 14, 2024 via the course portal.\n\n\nTotal: 20 points",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Assignment #02: Semantic Network Analysis</span>"
    ]
  },
  {
    "objectID": "16.Topic_Modeling.html",
    "href": "16.Topic_Modeling.html",
    "title": "16  Topic Modeling in Python",
    "section": "",
    "text": "16.1 Topic Modeling\nTopic modeling is a type of statistical modeling used in natural language processing (NLP) and text mining to discover abstract topics that occur within a collection of documents. It helps in organizing, understanding, and summarizing large sets of textual data by identifying patterns of word usage and grouping related words into topics.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Topic Modeling in Python</span>"
    ]
  },
  {
    "objectID": "16.Topic_Modeling.html#background",
    "href": "16.Topic_Modeling.html#background",
    "title": "16  Topic Modeling in Python",
    "section": "16.2 Background",
    "text": "16.2 Background\n\n16.2.1 Key Concepts of Topic Modeling\n\nDocuments and Corpus:\n\nDocument: A single piece of text (e.g., an article, a tweet, a book chapter).\nCorpus: A collection of documents.\n\nTopics:\n\nA topic is characterized by a distribution of words that frequently occur together. For example, a “sports” topic might include words like “game,” “team,” “score,” and “player.”\n\nWord Distribution:\n\nEach topic is represented by a probability distribution over a fixed vocabulary of words.\nEach document is represented by a mixture of topics, indicating the presence and proportion of each topic within the document.\n\n\n\n\n16.2.2 How Topic Modeling Works\n\nPreprocessing:\n\nTokenization: Breaking text into words or tokens.\nStop-word Removal: Eliminating common words that carry little meaning (e.g., “the,” “and”).\nStemming/Lemmatization: Reducing words to their base or root form.\nVectorization: Converting text into numerical representations, such as term frequency-inverse document frequency (TF-IDF).\n\nAlgorithm Application:\n\nLatent Dirichlet Allocation (LDA): The most popular topic modeling technique. It assumes that documents are a mixture of topics and that each topic is a mixture of words.\nNon-negative Matrix Factorization (NMF): Decomposes the document-term matrix into two lower-dimensional matrices, representing topics and their association with words.\nLatent Semantic Analysis (LSA): Uses singular value decomposition to identify patterns in the relationships between terms and documents.\n\nParameter Estimation:\n\nDetermining the number of topics.\nEstimating the probability distributions for topics and words using statistical methods.\n\nInterpretation:\n\nAnalyzing the top words in each topic to assign meaningful labels.\nUnderstanding the distribution of topics across the corpus to gain insights.\n\n\n\n\n16.2.3 Common Algorithms\n\nLatent Dirichlet Allocation (LDA):\n\nA generative probabilistic model.\nAssumes documents are generated from a mixture of topics, each of which is a distribution over words.\nWidely used due to its effectiveness and interpretability.\n\nNon-negative Matrix Factorization (NMF):\n\nDecomposes the document-term matrix into non-negative factors.\nUseful for extracting additive, parts-based representations of data.\n\nLatent Semantic Analysis (LSA):\n\nFocuses on capturing the underlying structure in the data by reducing dimensionality.\nLess probabilistic compared to LDA and NMF.\n\n\n\n\n16.2.4 Applications of Topic Modeling\n\nInformation Retrieval: Enhancing search engines by identifying relevant topics.\nDocument Classification: Organizing and categorizing large sets of documents.\nTrend Analysis: Monitoring changes in topics over time.\nRecommendation Systems: Suggesting content based on topic similarity.\nContent Summarization: Generating summaries by highlighting key topics.\nSocial Media Analysis: Understanding prevalent themes and discussions.\n\n\n\n16.2.5 Advantages\n\nScalability: Can handle large volumes of text data.\nUnsupervised Learning: Does not require labeled data.\nDiscovering Hidden Structures: Reveals underlying themes that may not be immediately apparent.\n\n\n\n16.2.6 Challenges\n\nChoosing the Number of Topics: Selecting the optimal number of topics can be subjective and may require experimentation.\nInterpretability: Topics may sometimes include ambiguous or overlapping words, making them hard to label clearly.\nQuality of Preprocessing: Effective preprocessing is crucial for meaningful topic extraction.\nScalability with Very Large Corpora: While scalable, extremely large datasets may require significant computational resources.\n\n\n\n16.2.7 Tools and Libraries\n\nGensim: A Python library for topic modeling, particularly known for its efficient implementation of LDA.\nScikit-learn: Offers implementations of NMF and LDA for topic modeling.\nMALLET: A Java-based package that provides efficient LDA and other topic modeling tools.\nTopic Modeling Toolkits in R: Such as topicmodels and stm packages.\n\n\n\n16.2.8 Example Use Case\nSuppose a company has thousands of customer reviews about its products. Topic modeling can help identify common themes, such as “product quality,” “customer service,” “pricing,” and “usability.” By understanding these topics, the company can make informed decisions to improve its offerings and address customer concerns more effectively.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Topic Modeling in Python</span>"
    ]
  },
  {
    "objectID": "16.Topic_Modeling.html#python-code",
    "href": "16.Topic_Modeling.html#python-code",
    "title": "16  Topic Modeling in Python",
    "section": "16.3 Python Code",
    "text": "16.3 Python Code\n# Importing essential libraries for data analysis\nimport pandas as pd  # pandas is used for data manipulation and analysis\nimport numpy as np   # numpy is used for numerical operations and handling arrays\n\n# Reading data from a CSV file\ndata = pd.read_csv('data_topicmodeling.csv', encoding='utf-8')\ndata.head()\n\n\npd.read_csv(): This function reads a CSV file and loads it into a DataFrame object, which can then be manipulated using various pandas methods.\nencoding='utf-8': Specifies the character encoding to handle different types of text correctly. UTF-8 is widely used because it supports a broad range of characters.\n\n\n16.3.1 Data Preprocessing Techniques\nReplacing Empty Cells with NaN\nIn many datasets, there are empty cells that need to be identified and treated appropriately. An empty cell can be represented by an empty string (''). The code below shows how to replace these empty cells with NaN (Not a Number), which pandas recognizes as a missing value.\n# Replace empty cells in the 'abstract' column with NaN\ndata['abstract'] = data['abstract'].replace('', np.nan)\nDropping Rows with NaN Values\n# Drop rows where 'abstract' column has NaN values\ndata.dropna(subset=['abstract'], inplace=True)\nApplying Lowercase Transformation\n# Convert the text in the 'abstract' column to lowercase\ndata['lower_abstract'] = data['abstract'].apply(lambda x: x.lower())\ndata.head()\n\n\n\n16.3.2 Introduction to Latent Dirichlet Allocation (LDA)\nLatent Dirichlet Allocation (LDA) is a popular algorithm used for topic modeling, which uncovers hidden thematic structures in a corpus of text. It works by assuming that each document is a mixture of various topics and that each topic is a mixture of words. Before applying LDA, it is essential to preprocess the text data, including tokenization.\n\n16.3.2.1 Tokenization Using gensim\n\n\n16.3.2.2 What is Tokenization?\nTokenization is the process of breaking down text into smaller units, known as tokens. These tokens can be words, phrases, or even characters. For LDA, tokenization helps convert text into a format that can be analyzed by the algorithm.\n\n\n16.3.2.3 Using gensim for Tokenization\ngensim is a robust Python library for topic modeling and natural language processing. The gensim.utils.simple_preprocess function provides an effective way to tokenize text.\n# Importing the gensim library and simple_preprocess function\nimport gensim\nfrom gensim.utils import simple_preprocess\n\n# Function to tokenize a list of sentences\ndef sent_to_words(sentences):\n    for sentence in sentences:\n        yield(gensim.utils.simple_preprocess(sentence, deacc=True))  # Tokenizes and removes punctuations with deacc=True\n\n\n16.3.2.4 Inspecting the Preprocessed Text\nBefore proceeding with tokenization, we review the current state of the preprocessed text data, stored in a column called lower_abstract:\n# Extracting the 'lower_abstract' column\nlower_abstract = data['lower_abstract']\nlower_abstract.head()\n\nlower_abstract.head(): Displays the first few entries of the lower_abstract column, showing the lowercase version of the abstract text data.\n\n# Applying the tokenization function to the 'lower_abstract' column\ndata['words'] = list(sent_to_words(lower_abstract))\ndata.head()\n\ndata['words']: Contains lists of tokens for each abstract, effectively converting text data into a list of words that can be used for LDA.\n\n# Creating a list of word tokens from the 'lower_abstract' column\ndata_words = list(sent_to_words(lower_abstract))\n\n# Displaying the first two tokenized entries\ndata_words[:2]\nExpected Output:\n[['word1', 'word2', 'word3', ...], ['word1', 'word2', 'word3', ...]]\n\n\n16.3.2.5 Removing Stopwords for Effective Text Analysis\nStopwords are common words in a language (e.g., “is”, “the”, “and”) that often carry little meaningful information in the context of text analysis. Removing these words helps improve the efficiency of text processing and enhances the quality of insights drawn from tasks such as topic modeling.\n\n16.3.2.5.1 Importing and Downloading Stopwords with nltk\n# Importing the nltk library and downloading the stopwords package\nimport nltk\nnltk.download('stopwords')\n\nnltk.download('stopwords'): Downloads the stopwords dataset if it is not already present in your environment.\n\nfrom nltk.corpus import stopwords\n\n# Creating a custom set of stopwords and adding more words\nstop_words_final = set(stopwords.words(\"english\"))\nprint(stop_words_final)\nadditional_words = {\"study\", \"research\", \"analysis\", \"experiment\"}  # Add more words here\nstop_words_final.update(additional_words)\n\n# Display the final set of stopwords\nstop_words_final\nprint(stop_words_final)\n\nstopwords.words(\"english\"): Returns a list of common English stopwords.\nset(): Converts the list to a set for efficient lookup operations.\nCustomization: Adding words like \"study\" ensures that domain-specific terms that are not useful for the analysis are also removed.\n\nExample Output:\n\n\n\n16.3.2.5.2 Why Remove Stopwords?\n\nEnhances Analysis: Removing stopwords reduces noise in the text data, allowing models to focus on more meaningful words.\nImproves Model Coherence: Reduces the dimensionality of the data and enhances the interpretability of topic models and other NLP algorithms.\n\n\n\n\n\n16.3.3 Building Bigram Models for Text Analysis\nIn natural language processing (NLP), a bigram refers to a sequence of two consecutive words in a sentence. Bigrams capture context that single-word tokens (unigrams) might miss, which can be beneficial for tasks such as topic modeling, language modeling, and more.\n\n16.3.3.1 Introduction to gensim’s Phrases Model\ngensim provides a Phrases class that can be used to detect and build bigrams in text. The Phrases model identifies word pairs that appear together frequently enough to be treated as a single phrase.\n\n16.3.3.1.1 Key Parameters:\n\nmin_count: Ignores all words and bigrams with a total collected count lower than this value. It helps filter out infrequent word pairs.\nthreshold: Sets the score threshold for forming a bigram. A higher threshold results in fewer detected phrases, which ensures only the most significant pairs are identified.\n\n\n\n16.3.3.1.2 Formula for Phrasing:\nA bigram is accepted if:\n\\[{(count(a, b) - min\\_count) * N / (count(a) * count(b)) &gt; threshold}\\]\nwhere (N) is the total vocabulary size.\n\n\n\n16.3.3.2 Building and Applying the Bigram Model\nThe first step is to create a Phrases object to detect word pairs in the tokenized text data.\n# Importing the Phrases class from gensim\nfrom gensim.models.phrases import Phrases, Phraser\n\n# Building the bigram model with custom min_count and threshold values\n\n# Ensure data_words is a list of tokenized sentences\n# For example: data_words = [['the', 'purpose', 'of', 'this', 'paper'], ['is', 'to', 'explain', ...]]\n\nbigram = gensim.models.Phrases(data_words, min_count=1, threshold=10)\n\nmin_count=1: Only considers word pairs that appear at least once in the text corpus.\nthreshold=10: A higher threshold results in fewer, more significant bigrams.\n\n\n\n16.3.3.3 Creating a Faster Bigram Model Using Phraser\nOnce the Phrases model is built, it can be converted to a Phraser object, which allows for more efficient transformation of tokenized text.\n# Converting the bigram model to a Phraser for efficient use\nbigram_mod = Phraser(bigram)\n\n\n16.3.3.4 Applying the Bigram Model\nTo view the results of the bigram model, apply it to tokenized text data:\n# Printing the first tokenized sentence with bigrams\nbigram_sentences = [bigram_mod[sentence] for sentence in data_words]\n\nprint(bigram_sentences[0])\nExpected Outcome:\nThe output shows a list of tokens where frequent word pairs are combined into a single token:\n\n\n\n16.3.3.5 Benefits of Using Bigrams:\n\nImproves Context: Bigrams provide more context than individual words, making text analysis richer and more meaningful.\nEnhances Topic Modeling: Combining common word pairs helps LDA and other models identify topics more accurately.\n\n\n\n\n16.3.4 Defining Custom Functions\n\n16.3.4.1 Function for Removing Stopwords\ndef remove_stopwords(texts):\n    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words_final] for doc in texts]\n\nInput: A list of tokenized sentences (texts).\nProcess: Iterates through each word in a document and removes those present in the stop_words_final set.\nOutput: A list of tokenized sentences without stopwords.\n\n\n\n16.3.4.2 Function for Creating Bigrams\nThis function applies a pre-trained bigram model to generate bigrams from tokenized sentences.\ndef make_bigrams(texts):\n    return [bigram_mod[doc] for doc in texts]\n\nInput: A list of tokenized sentences.\nProcess: Uses bigram_mod, a Phraser object, to transform sentences and add bigrams where appropriate.\nOutput: A list of tokenized sentences where frequent word pairs are combined into bigrams.\n\n\n\n16.3.4.3 Function for Lemmatization\nLemmatization reduces words to their base or dictionary form, known as the lemma. This helps group different forms of a word (e.g., “running” and “ran”) into a single term (“run”).\ndef lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n    texts_out = []\n    for sent in texts:\n        doc = nlp(\" \".join(sent))\n        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n    return texts_out\n\nInput: A list of tokenized sentences and a list of allowed part-of-speech (POS) tags (default is ['NOUN', 'ADJ', 'VERB', 'ADV']).\nProcess:\n\nUses the spacy NLP pipeline (nlp) to process sentences.\nExtracts the lemma of each token if its POS tag matches the allowed POS tags.\n\nOutput: A list of tokenized sentences with words lemmatized and filtered by POS tags.\n\n\n\n\n16.3.5 Step-by-Step Preprocessing Workflow\n\n16.3.5.1 Removing Stopwords\nThe first step is to remove common stopwords from the tokenized text using the remove_stopwords() function:\n# Remove stopwords from the tokenized data\ndata_noStopWords = remove_stopwords(data_words)\n\ndata_words: The tokenized text data.\ndata_noStopWords: The output is a list of tokenized sentences with stopwords removed.\n\n\n\n16.3.5.2 Forming Bigrams\nNext, we apply the make_bigrams() function to create bigrams in the text data:\n# Form bigrams from the data without stopwords\ndata_bgrams = make_bigrams(data_noStopWords)\nprint(data_bgrams[0])\n\n\ndata_noStopWords: The tokenized sentences without stopwords.\ndata_bgrams: The output is a list of tokenized sentences where common word pairs have been combined into bigrams.\n\n\n\n16.3.5.3 Initializing spaCy for Lemmatization\nBefore performing lemmatization, we need to initialize the spaCy language model. For efficiency, only the tagger component is kept active:\n# Initialize spaCy English model with limited components for efficiency\nimport spacy\nnlp = spacy.load(\"en_core_web_sm\", disable=['parser', 'ner'])\n\nen_core_web_sm: A lightweight English language model suitable for basic NLP tasks.\ndisable=['parser', 'ner']: Disables the parser and named entity recognition to optimize performance during lemmatization.\n\n\n\n16.3.5.4 Lemmatization\nThe final step is to apply the lemmatization() function, which reduces words to their base forms while keeping only specified parts of speech:\n# Perform lemmatization on the bigram data, keeping only nouns and verbs\nbgram_lemmatized = lemmatization(data_bgrams, allowed_postags=['NOUN', 'VERB'])\n\ndata_bgrams: The tokenized sentences with bigrams.\nallowed_postags=['NOUN', 'VERB']: Filters the tokens to keep only nouns and verbs during lemmatization.\nbgram_lemmatized: The output is a list of sentences with lemmatized tokens.\n\nprint(bgram_lemmatized[0])\n\n\n\n16.3.6 Transforming Data into Corpus and Dictionary Formats\nIn order to train topic models such as Latent Dirichlet Allocation (LDA), text data must be converted into a numerical format that represents the words and their frequencies. gensim provides tools for transforming text data into a dictionary and a corpus format, which are essential for topic modeling.\n\n16.3.6.1 Understanding Corpus and Dictionary\n\nDictionary: Maps each unique word in the text data to a unique integer ID. This dictionary acts as a reference for word encoding.\nCorpus: Represents the text data in the form of (word_id, word_frequency) tuples, where word_id corresponds to the unique ID from the dictionary and word_frequency indicates the number of times the word appears in the document.\n\n\n\n16.3.6.2 Creating a Dictionary\n\n\n16.3.6.3 Using gensim to Create a Dictionary\nThe gensim.corpora.Dictionary class is used to create a dictionary from the preprocessed text data:\n# Importing the corpora module from gensim\nimport gensim.corpora as corpora\n\n# Creating a dictionary from the lemmatized text data\ndictionary_bgram = corpora.Dictionary(bgram_lemmatized)\n\ncorpora.Dictionary(): Takes a list of tokenized sentences and creates a mapping of words to unique integer IDs.\nbgram_lemmatized: The lemmatized and preprocessed text data.\n\nExample:\nprint(dictionary_bgram.token2id)\nThe output will display a dictionary with words as keys and their corresponding integer IDs as values.\n\n\n\n16.3.7 Creating a Corpus\n\n16.3.7.1 Transforming Text Data into a Corpus\nThe corpus is created by converting each document into a bag-of-words (BoW) format, where each document is represented as a list of (word_id, word_frequency) tuples.\n# Assigning the preprocessed data to a variable for clarity\ntexts_bgram = bgram_lemmatized\n\n# Creating the corpus using the doc2bow method\ncorpus_bgram = [dictionary_bgram.doc2bow(text) for text in texts_bgram]\n\ndoc2bow(): Converts a document (a list of words) into a BoW representation, counting the frequency of each word in the document and returning it as a list of (word_id, word_frequency) tuples.\n\nExample:\n# Displaying the first few elements of the corpus\nprint(texts_bgram[1])\nprint(corpus_bgram[1:3])  # View the second and third documents\n\ntexts_bgram[1]: Displays the list of words in the second document.\ncorpus_bgram[1:3]: Displays the BoW format for the second and third documents, e.g., [(word_id, frequency), ...].\n\n\n\n16.3.7.2 Explanation of the Corpus Format\nEach document in the corpus contains tuples where:\n\nword_id: The unique integer ID assigned to the word in the dictionary.\nword_frequency: The number of times the word appears in the document.\n\nExample interpretation:\n[(0, 2), (3, 1), (5, 1)]\nThis output indicates that:\n\nThe word with ID 0 appears twice.\nThe word with ID 3 appears once.\nThe word with ID 5 appears once.\n\n\n\n16.3.7.3 Benefits of Using Corpus and Dictionary\n\nEfficient Storage: The BoW format is a sparse representation that saves space.\nCompatibility: Most topic modeling algorithms, including LDA, use a corpus and dictionary as input.\nFlexibility: The dictionary can be further modified to remove low-frequency or high-frequency words, which can help refine the analysis.\n\n\n\n\n16.3.8 Building the Base Topic Model with LDA\n\n16.3.8.1 Important Parameters for Building an LDA Model\nWhen building an LDA model, several parameters influence the outcome and quality of the generated topics:\nNumber of Topics (num_topics)\n\nk: The number of topics the model should find in the corpus. This parameter requires careful tuning based on the data and the goals of the analysis.\nExample: k = 4, meaning the model will attempt to identify 4 distinct topics.\n\nid2word\n\nid2word: A dictionary that maps word IDs to words (strings). This is crucial for determining vocabulary size and understanding the topics the model generates.\nSource: Created from the gensim.corpora.Dictionary object.\n\nHyperparameters: alpha and eta\n\nalpha (document-topic distribution):\n\nAffects the sparsity of the topic representation within documents.\nA lower value (close to 0) indicates fewer topics per document, while a higher value results in more topics per document.\nExample: alpha = 1.0/k, making it symmetric across topics.\n\neta (topic-word distribution):\n\nAffects the sparsity of the word distribution within topics.\nA lower value results in fewer words per topic, while a higher value includes more words per topic.\nExample: eta = 1.0/k, creating a symmetric distribution.\n\n\nchunksize\n\nControls the number of documents processed at a time during training.\nImpact: A larger chunksize can speed up training but requires more memory.\nExample: chunksize=100.\n\npasses\n\nThe number of times the model iterates over the entire corpus (also known as “epochs”).\nImpact: More passes generally improve the model’s quality but increase training time.\nExample: passes=10.\n\n\n\n\n16.3.9 Building the LDA Model\nBelow is the code to build an LDA model using gensim’s LdaMulticore class:\n# Importing the necessary module\nimport gensim.models\n\n# Setting parameters for the model\nk = 4  # Number of topics\na = 1.0 / k  # Alpha value for symmetric topic distribution\nb = 1.0 / k  # Eta value for symmetric word distribution\n\n# Building the LDA model\nlda_model = gensim.models.LdaMulticore(\n    corpus=corpus_bgram,          # Corpus in BoW format\n    id2word=dictionary_bgram,     # Dictionary for word-ID mapping\n    random_state=100,             # Seed for reproducibility\n    chunksize=100,                # Number of documents to process at once\n    passes=10,                    # Number of iterations over the entire corpus\n    num_topics=k,                 # Number of topics\n    alpha=a,                      # Document-topic distribution hyperparameter\n    eta=b                         # Topic-word distribution hyperparameter\n)\n\ngensim.models.LdaMulticore:\n\nUtilizes multiple CPU cores to speed up training.\nTakes corpus and id2word as mandatory arguments.\n\nHyperparameters:\n\nalpha and eta were set to 1.0/k for a symmetric prior, evenly distributing the importance of topics and words.\n\nrandom_state:\n\nEnsures reproducibility by seeding the random number generator.\n\n\nTo inspect the topics discovered by the model, use:\n# Printing the topics found by the model\nfor idx, topic in lda_model.print_topics(-1):\n    print(f\"Topic {idx}: {topic}\")\nExample Output:\nTopic 0: '0.025*\"data\" + 0.020*\"analysis\" + 0.018*\"model\" + ...'\nTopic 1: '0.030*\"machine\" + 0.025*\"learning\" + 0.015*\"algorithm\" + ...'\n...\n\n\n16.3.10 Evaluating the LDA Model with Perplexity and Coherence\nEvaluating the quality of an LDA model is crucial to ensure that it generates meaningful topics. Two common metrics used for this purpose are Perplexity and Coherence Score.\n\nPerplexity: Measures how well a model predicts a sample. A lower perplexity score indicates a better generalization of the model to unseen data.\nCoherence Score: Assesses the degree of semantic similarity between high-scoring words in a topic. Higher coherence indicates more interpretable and relevant topics.\n\n\n16.3.10.1 Code to Compute Perplexity\n# Importing the necessary module for CoherenceModel\nfrom gensim.models.coherencemodel import CoherenceModel\n\n# Computing the Perplexity of the LDA model\nprint('\\nPerplexity: ', lda_model.log_perplexity(corpus_bgram))\n\nlda_model.log_perplexity(corpus_bgram):\n\nEvaluates the model’s perplexity on the given corpus.\nThe lower the perplexity score, the better the model is at predicting unseen data.\n\n\nPerplexity: -7.123456789 (example value)\n\n\n\n16.3.11 Computing Coherence Score\nThe Coherence Score measures the degree of semantic similarity between the high-scoring words in a topic. There are different types of coherence measures, with ‘c_v’ being one of the most commonly used as it correlates well with human interpretation.\n\n16.3.11.1 Code to Compute Coherence Score\n# Computing the Coherence Score\ncoherence_model_lda = CoherenceModel(\n    model=lda_model,\n    texts=bgram_lemmatized,\n    dictionary=dictionary_bgram,\n    coherence='c_v'\n)\ncoherence_lda = coherence_model_lda.get_coherence()\n\n# Printing the Coherence Score\nprint('\\nCoherence Score: ', coherence_lda)\n\nCoherenceModel():\nTakes the trained LDA model, the tokenized texts, the dictionary, and the type of coherence measure as inputs.\ncoherence='c_v': A coherence measure that evaluates the semantic similarity between words.\nget_coherence():\nComputes and returns the coherence score for the model.\nInterpretation: A higher coherence score indicates that the topics are more interpretable and relevant.\n\nCoherence Score: 0.45 (example value)\n\n\n16.3.11.2 Importance of Model Evaluation\n\nPerplexity:\n\nWhile perplexity is a useful metric, it does not always correlate well with human interpretation. It is best used alongside coherence to assess the model’s quality.\n\nCoherence:\n\nThe coherence score provides a direct indication of how interpretable the topics are. It helps in fine-tuning the model for producing more meaningful and human-readable topics.\n\n\n\n\n\n16.3.12 Visualizing Topics and Keywords\nTo better understand the topics generated by an LDA model, visualization tools can be invaluable. PyLDAvis is a popular library that provides an interactive way to examine topics and the associated keywords. This chapter will guide you through setting up PyLDAvis and using it to visualize LDA model results.\n\n16.3.12.1 Installing and Importing PyLDAvis\nEnsure that PyLDAvis is installed in your environment:\n!pip install pyLDAvis\nImport the necessary modules:\nimport pyLDAvis\nimport pyLDAvis.gensim_models\n\n\n16.3.12.2 Preparing and Displaying the Visualization\nTo visualize the topics and their key terms, prepare the data using PyLDAvis:\n# Enable PyLDAvis to work in a notebook environment\npyLDAvis.enable_notebook()\n\n# Prepare the visualization data\nLDAvis_prepared = pyLDAvis.gensim_models.prepare(lda_model, corpus_bgram, dictionary_bgram)\n\n# Save the visualization as an HTML file\npyLDAvis.save_html(LDAvis_prepared, 'lda_model_visualization.html')\n\n# Display the visualization\npyLDAvis.display(LDAvis_prepared)\n\nThe PyLDAvis visualization consists of the following components:\n\nCircles: Represent topics. The size of the circle indicates the proportion of the corpus that the topic covers.\nDistance Between Circles: Indicates how different the topics are from one another. Topics that are more similar are closer together.\nλ (Lambda) Parameter:\n\nControls the relevance of the words shown for each topic.\nλ close to 1: Displays more frequently appearing words within each topic.\nλ close to 0: Shows words that distinguish one topic from another.\n\n\n\n\n16.3.12.3 How to Interpret:\n\nExplore topics by clicking on them to see the most relevant keywords.\nAdjust the λ slider to view words based on their frequency and distinguishing power.\n\n\n\n\n16.3.13 Document-Topic Distribution\nUnderstanding how documents are distributed across topics is essential for interpreting LDA results. The document-topic distribution shows the proportion of each topic within a document, indicating which topics are most prevalent.\n\n16.3.13.1 Extracting Document-Topic Distribution\nThe following code snippet iterates through the documents in the corpus and prints the topic distribution for each:\n# Iterate through the document-topic distribution and print for the first few documents\nfor i, topic_list in enumerate(lda_model[corpus_bgram]):\n    if i == 5:  # Limit to the first 5 documents for demonstration\n        break\n    print(f\"{i}th document consists of the following topics (proportions):\", topic_list)\n\nlda_model[corpus_bgram]:\n\nApplies the trained LDA model to the corpus and returns a list of tuples for each document. Each tuple contains a topic ID and its proportion within the document.\n\ntopic_list:\n\nA list of tuples representing the topic distribution in a document. Each tuple follows the format (topic_id, proportion), where proportion indicates how much of the document is represented by topic_id.\n\n\nExample Output:\n0th document consists of the following topics (proportions): [(0, 0.25), (2, 0.50), (3, 0.25)]\n1th document consists of the following topics (proportions): [(1, 0.60), (2, 0.40)]\n...\n\nIn the first document, Topic 0 accounts for 25%, Topic 2 for 50%, and Topic 3 for 25% of the content.\nThese proportions help identify the dominant topic(s) in each document and how each document contributes to the overall topic model.\n\n\n\n16.3.13.2 Use Cases for Document-Topic Distributions\n\nTopic-Based Document Classification: Documents can be categorized based on the most prevalent topic.\nContent Analysis: Understanding the spread of topics across a corpus helps in content segmentation and thematic analysis.\nModel Refinement: Identifying documents with mixed topics can indicate whether further preprocessing or model tuning is needed.\n\n\n\n16.3.13.3 Saving Document-Topic Distributions to CSV\nAfter computing the document-topic distributions, it is often useful to export this data for further analysis or reporting. This chapter will guide you through the process of storing the document-topic distribution as a DataFrame and exporting it to a CSV file for easy access.\n# Initialize an empty list to store document-topic distributions\ndoc_topics = []\n\n# Iterate over each document in the corpus to extract topic proportions\nfor i, topic_list in enumerate(lda_model[corpus_bgram]):\n    doc_topics.append(topic_list)\n\n# Convert the list of document-topic distributions to a DataFrame\nlda_model_topics = pd.DataFrame(doc_topics)\n\n# Export the DataFrame to a CSV file\nlda_model_topics.to_csv('doc_topics.csv', index=False)\n\n# Display the DataFrame to verify the results\nlda_model_topics\n\ndoc_topics.append(topic_list): Appends the topic distribution of each document as a list of tuples to doc_topics.\npd.DataFrame(doc_topics): Converts the list of topic distributions into a Pandas DataFrame.\nto_csv('doc_topics.csv'): Exports the DataFrame to a CSV file named doc_topics.csv for further analysis.\n\n\n\n16.3.13.4 Structure of the CSV File:\nThe CSV file will have the following structure: - Each row represents a document. - Each cell contains a tuple (topic_id, proportion) indicating the proportion of the document assigned to that topic.\nExample CSV Output:\n\n\n\nDocument\nTopic 0\nTopic 1\nTopic 2\n…\n\n\n\n\n0\n(0, 0.25)\n(1, 0.50)\n(3, 0.25)\n…\n\n\n1\n(1, 0.60)\n(2, 0.40)\nNaN\n…\n\n\n2\n(0, 0.15)\n(1, 0.35)\n(3, 0.50)\n…\n\n\n\n\n\n16.3.13.5 Benefits of Saving Document-Topic Distributions\n\nFurther Analysis: The exported CSV can be used in various data analysis tools or software for deeper examination.\nReporting and Visualization: The CSV format makes it easy to create visual reports and dashboards.\nIntegration with Other Applications: Data stored in CSV format can be used for integrations with business intelligence tools like Tableau or Power BI.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Topic Modeling in Python</span>"
    ]
  },
  {
    "objectID": "16.Topic_Modeling.html#introduction-to-latent-dirichlet-allocation-lda",
    "href": "16.Topic_Modeling.html#introduction-to-latent-dirichlet-allocation-lda",
    "title": "16  Topic Modeling in Python",
    "section": "16.3 3.1 Introduction to Latent Dirichlet Allocation (LDA)",
    "text": "16.3 3.1 Introduction to Latent Dirichlet Allocation (LDA)\nLatent Dirichlet Allocation (LDA) is a popular algorithm used for topic modeling, which uncovers hidden thematic structures in a corpus of text. It works by assuming that each document is a mixture of various topics and that each topic is a mixture of words. Before applying LDA, it is essential to preprocess the text data, including tokenization.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Topic Modeling in Python</span>"
    ]
  },
  {
    "objectID": "16.Topic_Modeling.html#tokenization-using-gensim",
    "href": "16.Topic_Modeling.html#tokenization-using-gensim",
    "title": "16  Topic Modeling in Python",
    "section": "16.4 3.2 Tokenization Using gensim",
    "text": "16.4 3.2 Tokenization Using gensim\n\n16.4.1 3.2.1 What is Tokenization?\nTokenization is the process of breaking down text into smaller units, known as tokens. These tokens can be words, phrases, or even characters. For LDA, tokenization helps convert text into a format that can be analyzed by the algorithm.\n\n\n16.4.2 3.2.2 Using gensim for Tokenization\ngensim is a robust Python library for topic modeling and natural language processing. The gensim.utils.simple_preprocess function provides an effective way to tokenize text.\n# Importing the gensim library and simple_preprocess function\nimport gensim\nfrom gensim.utils import simple_preprocess\n\n# Function to tokenize a list of sentences\ndef sent_to_words(sentences):\n    for sentence in sentences:\n        yield(gensim.utils.simple_preprocess(sentence, deacc=True))  # Tokenizes and removes punctuations with deacc=True",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Topic Modeling in Python</span>"
    ]
  },
  {
    "objectID": "16.Topic_Modeling.html#building-the-lda-model",
    "href": "16.Topic_Modeling.html#building-the-lda-model",
    "title": "16  Topic_Modeling",
    "section": "17.3 Building the LDA Model",
    "text": "17.3 Building the LDA Model\nBelow is the code to build an LDA model using gensim’s LdaMulticore class:\n# Importing the necessary module\nimport gensim.models\n\n# Setting parameters for the model\nk = 4  # Number of topics\na = 1.0 / k  # Alpha value for symmetric topic distribution\nb = 1.0 / k  # Eta value for symmetric word distribution\n\n# Building the LDA model\nlda_model = gensim.models.LdaMulticore(\n    corpus=corpus_bgram,          # Corpus in BoW format\n    id2word=dictionary_bgram,     # Dictionary for word-ID mapping\n    random_state=100,             # Seed for reproducibility\n    chunksize=100,                # Number of documents to process at once\n    passes=10,                    # Number of iterations over the entire corpus\n    num_topics=k,                 # Number of topics\n    alpha=a,                      # Document-topic distribution hyperparameter\n    eta=b                         # Topic-word distribution hyperparameter\n)\n\ngensim.models.LdaMulticore:\n\nUtilizes multiple CPU cores to speed up training.\nTakes corpus and id2word as mandatory arguments.\n\nHyperparameters:\n\nalpha and eta were set to 1.0/k for a symmetric prior, evenly distributing the importance of topics and words.\n\nrandom_state:\n\nEnsures reproducibility by seeding the random number generator.\n\n\nTo inspect the topics discovered by the model, use:\n# Printing the topics found by the model\nfor idx, topic in lda_model.print_topics(-1):\n    print(f\"Topic {idx}: {topic}\")\nExample Output:\nTopic 0: '0.025*\"data\" + 0.020*\"analysis\" + 0.018*\"model\" + ...'\nTopic 1: '0.030*\"machine\" + 0.025*\"learning\" + 0.015*\"algorithm\" + ...'\n...\n\n17.3.1 Evaluating the LDA Model with Perplexity and Coherence\nEvaluating the quality of an LDA model is crucial to ensure that it generates meaningful topics. Two common metrics used for this purpose are Perplexity and Coherence Score.\n\nPerplexity: Measures how well a model predicts a sample. A lower perplexity score indicates a better generalization of the model to unseen data.\nCoherence Score: Assesses the degree of semantic similarity between high-scoring words in a topic. Higher coherence indicates more interpretable and relevant topics.\n\n\n17.3.1.1 Code to Compute Perplexity\n# Importing the necessary module for CoherenceModel\nfrom gensim.models.coherencemodel import CoherenceModel\n\n# Computing the Perplexity of the LDA model\nprint('\\nPerplexity: ', lda_model.log_perplexity(corpus_bgram))\n\nlda_model.log_perplexity(corpus_bgram):\n\nEvaluates the model’s perplexity on the given corpus.\nThe lower the perplexity score, the better the model is at predicting unseen data.\n\n\nPerplexity: -7.123456789 (example value)\n\n\n\n17.3.2 Computing Coherence Score\nThe Coherence Score measures the degree of semantic similarity between the high-scoring words in a topic. There are different types of coherence measures, with ‘c_v’ being one of the most commonly used as it correlates well with human interpretation.\n\n17.3.2.1 Code to Compute Coherence Score\n# Computing the Coherence Score\ncoherence_model_lda = CoherenceModel(\n    model=lda_model,\n    texts=bgram_lemmatized,\n    dictionary=dictionary_bgram,\n    coherence='c_v'\n)\ncoherence_lda = coherence_model_lda.get_coherence()\n\n# Printing the Coherence Score\nprint('\\nCoherence Score: ', coherence_lda)\n\nCoherenceModel():\nTakes the trained LDA model, the tokenized texts, the dictionary, and the type of coherence measure as inputs.\ncoherence='c_v': A coherence measure that evaluates the semantic similarity between words.\nget_coherence():\nComputes and returns the coherence score for the model.\nInterpretation: A higher coherence score indicates that the topics are more interpretable and relevant.\n\nCoherence Score: 0.45 (example value)\n\n\n17.3.2.2 Importance of Model Evaluation\n\nPerplexity:\nWhile perplexity is a useful metric, it does not always correlate well with human interpretation. It is best used alongside coherence to assess the model’s quality.\nCoherence:\nThe coherence score provides a direct indication of how interpretable the topics are. It helps in fine-tuning the model for producing more meaningful and human-readable topics.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Topic_Modeling</span>"
    ]
  },
  {
    "objectID": "16.Topic_Modeling.html#explanation-of-model-construction",
    "href": "16.Topic_Modeling.html#explanation-of-model-construction",
    "title": "16  Topic_Modeling",
    "section": "17.4 10.4 Explanation of Model Construction",
    "text": "17.4 10.4 Explanation of Model Construction\n\ngensim.models.LdaMulticore:\n\nUtilizes multiple CPU cores to speed up training.\nTakes corpus and id2word as mandatory arguments.\n\nHyperparameters:\n\nalpha and eta were set to 1.0/k for a symmetric prior, evenly distributing the importance of topics and words.\n\nrandom_state:\n\nEnsures reproducibility by seeding the random number generator.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Topic_Modeling</span>"
    ]
  },
  {
    "objectID": "16.Topic_Modeling.html#viewing-the-model-output",
    "href": "16.Topic_Modeling.html#viewing-the-model-output",
    "title": "16  Topic_Modeling",
    "section": "17.5 10.5 Viewing the Model Output",
    "text": "17.5 10.5 Viewing the Model Output\nTo inspect the topics discovered by the model, use:\n# Printing the topics found by the model\nfor idx, topic in lda_model.print_topics(-1):\n    print(f\"Topic {idx}: {topic}\")\n\n17.5.1 Example Output:\nTopic 0: '0.025*\"data\" + 0.020*\"analysis\" + 0.018*\"model\" + ...'\nTopic 1: '0.030*\"machine\" + 0.025*\"learning\" + 0.015*\"algorithm\" + ...'\n...",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Topic_Modeling</span>"
    ]
  },
  {
    "objectID": "16.Topic_Modeling.html#summary",
    "href": "16.Topic_Modeling.html#summary",
    "title": "16  Topic_Modeling",
    "section": "17.6 10.6 Summary",
    "text": "17.6 10.6 Summary\nIn this chapter, we constructed a base LDA model using gensim. We discussed the significance of key parameters like num_topics, alpha, eta, chunksize, and passes. We then built the model and provided an example for viewing the generated topics. The next chapter will focus on evaluating the quality of the model and refining it for better performance.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Topic_Modeling</span>"
    ]
  },
  {
    "objectID": "16.Topic_Modeling.html#introduction",
    "href": "16.Topic_Modeling.html#introduction",
    "title": "16  Topic_Modeling",
    "section": "17.4 11.1 Introduction",
    "text": "17.4 11.1 Introduction\nEvaluating the quality of an LDA model is crucial to ensure that it generates meaningful topics. Two common metrics used for this purpose are Perplexity and Coherence Score.\n\nPerplexity: Measures how well a model predicts a sample. A lower perplexity score indicates a better generalization of the model to unseen data.\nCoherence Score: Assesses the degree of semantic similarity between high-scoring words in a topic. Higher coherence indicates more interpretable and relevant topics.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Topic_Modeling</span>"
    ]
  },
  {
    "objectID": "16.Topic_Modeling.html#computing-perplexity",
    "href": "16.Topic_Modeling.html#computing-perplexity",
    "title": "16  Topic_Modeling",
    "section": "17.5 11.2 Computing Perplexity",
    "text": "17.5 11.2 Computing Perplexity\n\n17.5.1 11.2.1 Explanation of Perplexity\nPerplexity is a statistical measure of how well a probability distribution or probability model predicts a sample. In the context of topic modeling, lower perplexity indicates that the model is more accurate.\n\n\n17.5.2 Code to Compute Perplexity\n# Importing the necessary module for CoherenceModel\nfrom gensim.models.coherencemodel import CoherenceModel\n\n# Computing the Perplexity of the LDA model\nprint('\\nPerplexity: ', lda_model.log_perplexity(corpus_bgram))\n\n\n17.5.3 Explanation:\n\nlda_model.log_perplexity(corpus_bgram):\n\nEvaluates the model’s perplexity on the given corpus.\nThe lower the perplexity score, the better the model is at predicting unseen data.\n\n\n\n\n17.5.4 Expected Output:\nPerplexity: -7.123456789 (example value)",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Topic_Modeling</span>"
    ]
  },
  {
    "objectID": "16.Topic_Modeling.html#computing-coherence-score",
    "href": "16.Topic_Modeling.html#computing-coherence-score",
    "title": "16  Topic_Modeling",
    "section": "17.6 11.3 Computing Coherence Score",
    "text": "17.6 11.3 Computing Coherence Score\n\n17.6.1 11.3.1 Explanation of Coherence Score\nThe Coherence Score measures the degree of semantic similarity between the high-scoring words in a topic. There are different types of coherence measures, with ‘c_v’ being one of the most commonly used as it correlates well with human interpretation.\n\n\n17.6.2 Code to Compute Coherence Score\n# Computing the Coherence Score\ncoherence_model_lda = CoherenceModel(\n    model=lda_model,\n    texts=bgram_lemmatized,\n    dictionary=dictionary_bgram,\n    coherence='c_v'\n)\ncoherence_lda = coherence_model_lda.get_coherence()\n\n# Printing the Coherence Score\nprint('\\nCoherence Score: ', coherence_lda)\n\n\n17.6.3 Explanation:\n\nCoherenceModel():\n\nTakes the trained LDA model, the tokenized texts, the dictionary, and the type of coherence measure as inputs.\ncoherence='c_v': A coherence measure that evaluates the semantic similarity between words.\n\nget_coherence():\n\nComputes and returns the coherence score for the model.\n\nInterpretation: A higher coherence score indicates that the topics are more interpretable and relevant.\n\n\n\n17.6.4 Expected Output:\nCoherence Score: 0.45 (example value)",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Topic_Modeling</span>"
    ]
  },
  {
    "objectID": "16.Topic_Modeling.html#importance-of-model-evaluation",
    "href": "16.Topic_Modeling.html#importance-of-model-evaluation",
    "title": "16  Topic_Modeling",
    "section": "17.7 11.4 Importance of Model Evaluation",
    "text": "17.7 11.4 Importance of Model Evaluation\n\nPerplexity:\n\nWhile perplexity is a useful metric, it does not always correlate well with human interpretation. It is best used alongside coherence to assess the model’s quality.\n\nCoherence:\n\nThe coherence score provides a direct indication of how interpretable the topics are. It helps in fine-tuning the model for producing more meaningful and human-readable topics.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Topic_Modeling</span>"
    ]
  },
  {
    "objectID": "16.Topic_Modeling.html#python-code-that-calculates-perplexity-and-coherence-scores-for-different-topic-numbers",
    "href": "16.Topic_Modeling.html#python-code-that-calculates-perplexity-and-coherence-scores-for-different-topic-numbers",
    "title": "16  Topic Modeling in Python",
    "section": "16.4 Python code that calculates perplexity and coherence scores for different topic numbers",
    "text": "16.4 Python code that calculates perplexity and coherence scores for different topic numbers\nBelow is Python code that calculates perplexity and coherence scores for different topic numbers between specified limits a and b. This code helps identify the topic number with the best scores:\nimport gensim\nfrom gensim.models.coherencemodel import CoherenceModel\n\n# Function to compute perplexity and coherence for different numbers of topics\ndef evaluate_topic_models(corpus, dictionary, texts, start, end):\n    results = []\n\n    for k in range(start, end + 1):\n        print(f\"Training LDA model with {k} topics...\")\n        \n        # Set alpha and eta to be symmetric for simplicity\n        alpha = 1.0 / k\n        eta = 1.0 / k\n\n        # Build LDA model\n        lda_model = gensim.models.LdaMulticore(\n            corpus=corpus,\n            id2word=dictionary,\n            num_topics=k,\n            random_state=100,\n            chunksize=100,\n            passes=10,\n            alpha=alpha,\n            eta=eta\n        )\n\n        # Calculate perplexity\n        perplexity = lda_model.log_perplexity(corpus)\n        print(f\"Perplexity for {k} topics: {perplexity}\")\n\n        # Calculate coherence score\n        coherence_model_lda = CoherenceModel(\n            model=lda_model,\n            texts=texts,\n            dictionary=dictionary,\n            coherence='c_v'\n        )\n        coherence = coherence_model_lda.get_coherence()\n        print(f\"Coherence Score for {k} topics: {coherence}\\n\")\n\n        # Append results\n        results.append((k, perplexity, coherence))\n\n    return results\n\n# Set the range for the number of topics\na = 2  # Minimum number of topics\nb = 10  # Maximum number of topics\n\n# Run the evaluation\nresults = evaluate_topic_models(corpus_bgram, dictionary_bgram, bgram_lemmatized, a, b)\n\n# Find the best number of topics based on coherence and perplexity\nbest_coherence = max(results, key=lambda x: x[2])\nbest_perplexity = min(results, key=lambda x: x[1])\n\nprint(f\"\\nBest coherence score obtained with {best_coherence[0]} topics: Coherence = {best_coherence[2]}\")\nprint(f\"Best perplexity obtained with {best_perplexity[0]} topics: Perplexity = {best_perplexity[1]}\")\n\n# Print all results for comparison\nprint(\"\\nDetailed Results (Topics, Perplexity, Coherence):\")\nfor res in results:\n    print(f\"Topics: {res[0]}, Perplexity: {res[1]}, Coherence: {res[2]}\")\n\nevaluate_topic_models(): A function that trains LDA models for each number of topics between start and end and calculates both perplexity and coherence scores.\nParameters:\n\ncorpus: The bag-of-words corpus.\ndictionary: The gensim dictionary object.\ntexts: The tokenized, preprocessed texts.\nstart, end: The range for the number of topics (k).\n\nOutput:\n\nA list of tuples containing (number of topics, perplexity, coherence score).\nThe best number of topics based on maximum coherence and minimum perplexity is printed.\n\nKey Metrics:\n\nPerplexity: Lower values are better.\nCoherence: Higher values are better.\n\n\nThis code allows you to identify the topic number that optimizes both perplexity and coherence for your dataset.\nExample Output:",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Topic Modeling in Python</span>"
    ]
  },
  {
    "objectID": "17.Topic_Modeling_Example.html",
    "href": "17.Topic_Modeling_Example.html",
    "title": "17  Topic Modeling with R (Example)",
    "section": "",
    "text": "17.1 Why Use Topic Modeling in Linguistics?\nIn linguistics, topic modeling is particularly valuable because it allows researchers to:\nCommon applications include examining corpora such as:",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Topic Modeling with R (Example)</span>"
    ]
  },
  {
    "objectID": "17.Topic_Modeling_Example.html#getting-started-with-r-for-topic-modeling",
    "href": "17.Topic_Modeling_Example.html#getting-started-with-r-for-topic-modeling",
    "title": "17  Topic Modeling with R (Example)",
    "section": "17.2 Getting Started with R for Topic Modeling",
    "text": "17.2 Getting Started with R for Topic Modeling\nTo conduct topic modeling in R, we use the topicmodels package, which implements Latent Dirichlet Allocation (LDA), the most popular topic modeling algorithm. Here’s how to get started.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Topic Modeling with R (Example)</span>"
    ]
  },
  {
    "objectID": "17.Topic_Modeling_Example.html#building-the-topic-model",
    "href": "17.Topic_Modeling_Example.html#building-the-topic-model",
    "title": "17  Topic Modeling with R (Example)",
    "section": "17.3 8.4 Building the Topic Model",
    "text": "17.3 8.4 Building the Topic Model\nWith our DTM ready, we can now create the topic model. In this example, we will use LDA with a predefined number of topics.\n\n17.3.1 Step 4: Set the Number of Topics\nChoosing the number of topics (k) is crucial. We often experiment with different values to find the most interpretable results. For this example, we’ll set k = 3.\n# Set the number of topics\nk &lt;- 3\n\n# Apply the LDA model\nlda_model &lt;- LDA(dtm, k = k, control = list(seed = 1234))\n\n\n17.3.2 Step 5: Examine Topics\nOnce the model is trained, we can examine the topics to understand the most significant words associated with each topic.\n# Get the terms for each topic\nterms(lda_model, 6)\nThis code returns the six most important words for each topic. These words help us interpret the themes each topic represents.\n\n\n17.3.3 Step 6: Assign Topics to Documents\nTo determine which topics are most prevalent in each document, we use the posterior probabilities from the LDA model.\n# Get the topic distribution for each document\ndocument_topics &lt;- tidy(lda_model, matrix = \"gamma\")\ndocument_topics\nThe gamma matrix shows the probability of each topic for each document. High probabilities indicate the most likely topics associated with each document.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Topic Modeling with R (Example)</span>"
    ]
  },
  {
    "objectID": "17.Topic_Modeling_Example.html#visualizing-topics",
    "href": "17.Topic_Modeling_Example.html#visualizing-topics",
    "title": "17  Topic Modeling with R (Example)",
    "section": "17.4 8.5 Visualizing Topics",
    "text": "17.4 8.5 Visualizing Topics\nVisualizations can make it easier to understand the themes in the text data. Let’s create a simple plot to display the top terms in each topic.\n# Get top terms for each topic\ntop_terms &lt;- tidy(lda_model, matrix = \"beta\") %&gt;%\n  group_by(topic) %&gt;%\n  slice_max(beta, n = 6) %&gt;%\n  ungroup() %&gt;%\n  arrange(topic, -beta)\n\n# Plot the top terms for each topic\ntop_terms %&gt;%\n  mutate(term = reorder_within(term, beta, topic)) %&gt;%\n  ggplot(aes(term, beta, fill = factor(topic))) +\n  geom_col(show.legend = FALSE) +\n  facet_wrap(~ topic, scales = \"free_y\") +\n  coord_flip() +\n  scale_x_reordered() +\n  labs(title = \"Top Terms in Each Topic\",\n       x = \"Terms\",\n       y = \"Beta\")\nThis plot allows us to see which terms are most important in each topic, helping with interpretation.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Topic Modeling with R (Example)</span>"
    ]
  },
  {
    "objectID": "17.Topic_Modeling_Example.html#interpretation-and-limitations",
    "href": "17.Topic_Modeling_Example.html#interpretation-and-limitations",
    "title": "17  Topic Modeling with R (Example)",
    "section": "17.5 8.6 Interpretation and Limitations",
    "text": "17.5 8.6 Interpretation and Limitations\n\n17.5.1 Interpreting Topics\nInterpreting the topics requires careful consideration of the terms associated with each one. The topics are not labeled automatically, so the researcher must decide on appropriate labels based on the top terms and their understanding of the dataset.\n\n\n17.5.2 Limitations of Topic Modeling\n\nAmbiguity: Words may belong to multiple topics.\nInterpretability: It may be challenging to interpret topics if they don’t align well with human intuition.\nDependency on Preprocessing: Preprocessing significantly affects the model, especially choices like removing certain stop words or using stemming.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Topic Modeling with R (Example)</span>"
    ]
  },
  {
    "objectID": "17.Topic_Modeling_Example.html#conclusion",
    "href": "17.Topic_Modeling_Example.html#conclusion",
    "title": "17  Topic Modeling with R (Example)",
    "section": "17.6 8.7 Conclusion",
    "text": "17.6 8.7 Conclusion\nTopic modeling is a valuable tool for linguistics research, enabling the discovery of underlying themes in large text corpora. In this chapter, we explored how to preprocess text data, create an LDA model in R, and interpret the resulting topics. We also discussed the strengths and limitations of topic modeling, equipping you with the skills to apply this technique in your own research.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Topic Modeling with R (Example)</span>"
    ]
  },
  {
    "objectID": "17.Topic_Modeling_Example.html#exercises",
    "href": "17.Topic_Modeling_Example.html#exercises",
    "title": "17  Topic Modeling with R (Example)",
    "section": "17.7 8.8 Exercises",
    "text": "17.7 8.8 Exercises\n\nPreprocess a different text corpus and create a DTM.\nExperiment with different values of k (number of topics) and compare the results.\nInterpret the topics and think about how they align with linguistic themes.\n\n\nThis chapter provides a foundational understanding of topic modeling for linguistics students. The exercises encourage experimentation and interpretation, helping students apply these skills to various types of text data.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Topic Modeling with R (Example)</span>"
    ]
  },
  {
    "objectID": "17.Topic_Modeling_Example.html#why-use-topic-modeling-in-linguistics",
    "href": "17.Topic_Modeling_Example.html#why-use-topic-modeling-in-linguistics",
    "title": "17  Topic Modeling with R (Example)",
    "section": "",
    "text": "Identify patterns and themes in large collections of texts.\nAnalyze thematic structures without needing pre-labeled data.\nGain insights into the linguistic or cultural elements underlying different topics.\n\n\n\nAcademic articles to find recurring themes.\nSocial media data to analyze discourse.\nHistorical texts to trace the evolution of ideas over time.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Topic Modeling with R (Example)</span>"
    ]
  },
  {
    "objectID": "17.Topic_Modeling_Example.html#install-and-load-necessary-packages",
    "href": "17.Topic_Modeling_Example.html#install-and-load-necessary-packages",
    "title": "17  Topic Modeling with R (Example)",
    "section": "17.3 Install and Load Necessary Packages",
    "text": "17.3 Install and Load Necessary Packages\nFirst, we need to install and load several packages: - tm for text mining and preprocessing, - topicmodels for creating the LDA model, and - tidyverse and tidytext for data manipulation and visualization.\n# Install packages if not already installed\ninstall.packages(\"tm\")\ninstall.packages(\"topicmodels\")\ninstall.packages(\"tidyverse\")\ninstall.packages(\"tidytext\")\n\n# Load the libraries\nlibrary(tm)\n\nWarning: package 'tm' was built under R version 4.3.3\n\n\nLoading required package: NLP\n\nlibrary(topicmodels)\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ ggplot2::annotate() masks NLP::annotate()\n✖ dplyr::filter()     masks stats::filter()\n✖ dplyr::lag()        masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(tidytext)",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Topic Modeling with R (Example)</span>"
    ]
  },
  {
    "objectID": "17.Topic_Modeling_Example.html#data-import-and-preparation",
    "href": "17.Topic_Modeling_Example.html#data-import-and-preparation",
    "title": "17  Topic Modeling with R (Example)",
    "section": "17.4 Data Import and Preparation",
    "text": "17.4 Data Import and Preparation\nBefore we dive into topic modeling, let’s learn how to import data and prepare it for analysis. This step is essential for linguistics research since it ensures that we start with clean, manageable text data.\n\n17.4.1 Importing the Data\nFor this example, we assume we have a CSV file named data_topicmodeling.csv. This file contains a collection of academic texts, with each row representing a document. The CSV file should be in the same working directory as your R script or project, or you should specify the full path.\n\n# Import the dataset\ndata &lt;- read_csv(\"data_topicmodeling.csv\")\n\nRows: 1526 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (4): author, title_eng, journal_short, abstract\ndbl (1): year\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# Add a unique identifier for each document\ndata &lt;- tibble::rowid_to_column(data, \"doc_id\")\n\nThis code reads in the CSV file and creates a new column called doc_id, assigning a unique identifier to each document. The doc_id column will help us keep track of documents when we analyze and interpret the results.\n\n\n17.4.2 Adding Custom Stop Words\nIn academic texts, there may be specific words or phrases frequently used across documents but not meaningful for topic modeling. For example, terms like “study,” “results,” or “analysis” may appear often but do not contribute to distinct thematic content. In this case, we use a custom stop words file.\n\n# Load custom stop words\nstop_words &lt;- readLines(\"academic_stopwords.txt\", encoding = \"UTF-8\")\n\nHere, academic_stopwords.txt is a text file with one stop word per line, such as “method,” “study,” or “analysis.” Make sure this file is in your working directory or specify its full path. Loading these stop words allows us to exclude them during preprocessing, helping to focus the model on more meaningful terms.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Topic Modeling with R (Example)</span>"
    ]
  },
  {
    "objectID": "17.Topic_Modeling_Example.html#pre-processing-with-pos-tagging",
    "href": "17.Topic_Modeling_Example.html#pre-processing-with-pos-tagging",
    "title": "17  Topic Modeling with R (Example)",
    "section": "17.5 Pre-processing with POS Tagging",
    "text": "17.5 Pre-processing with POS Tagging\nPre-processing with POS tagging helps us target specific types of words relevant to topic analysis. In linguistics, POS tagging can reveal the syntactic categories of words (e.g., noun, verb, adjective), allowing us to select only the most meaningful words for topic modeling.\n\n17.5.1 Install and Load udpipe for POS Tagging\nudpipe is an R package that provides access to pre-trained language models for POS tagging and dependency parsing. First, ensure udpipe is installed, then load it and download the English language model.\n\n# Install and load the udpipe package\n# install.packages('udpipe')\nlibrary(udpipe)\n\n# Download the English language model\n# ud_model &lt;- udpipe_download_model(language = \"english\")\n\n# Load the language model\n# ud_model &lt;- udpipe_load_model(ud_model$file_model)\nud_model &lt;- udpipe_load_model(\"english-ewt-ud-2.5-191206.udpipe\")\n\n\n\n17.5.2 Define the POS Tagging Function\nThe function ud_tagger_f below uses udpipe to annotate each document in our dataset. This function:\n\nApplies POS tagging on the abstract column of our dataset.\nFilters out terms listed in the stop_words file.\nSelects only words classified as verbs, nouns, adjectives, or adverbs (since these words typically carry more thematic meaning).\n\n\n# Define the POS tagging function\nud_tagger_f &lt;- function(input) {\n    udpipe_annotate(ud_model, x = input$abstract, doc_id = input$doc_id, year = input$year) %&gt;%\n    as.data.frame() %&gt;%\n    filter(!lemma %in% stop_words) %&gt;%\n    subset(upos %in% c(\"VERB\", \"NOUN\", \"ADJ\", \"ADV\"))\n}\n\n\n\n17.5.3 Apply POS Tagging to the Dataset\nThe next step is to apply our ud_tagger_f function to the dataset and save the results. This tagged dataset will focus only on the filtered POS categories and exclude irrelevant words.\n\n# Apply POS tagging to the data\ndata_tag &lt;- ud_tagger_f(data)\n\n\n\n17.5.4 Clean the Tokenized Data\nWith the POS-tagged data, we can now proceed to additional cleaning steps: 1. Remove numeric characters. 2. Delete punctuation. 3. Filter out single-character words, as they typically add little value. 4. Delete empty lemmas (words that become empty after cleaning).\n\n# Select relevant columns and clean tokens\ntext_cleaning_tokens &lt;- data_tag %&gt;% select(doc_id, lemma)\n\n# Remove numbers and punctuation\ntext_cleaning_tokens$lemma &lt;- gsub('[[:digit:]]+', '', text_cleaning_tokens$lemma)  # Remove numbers\ntext_cleaning_tokens$lemma &lt;- gsub('[[:punct:]]+', '', text_cleaning_tokens$lemma)  # Remove punctuation\n\n# Filter out one-letter words\ntext_cleaning_tokens &lt;- text_cleaning_tokens %&gt;% filter(!(nchar(lemma) == 1))\n\n# Remove empty lemmas\ntokens &lt;- text_cleaning_tokens %&gt;% filter(!(lemma == \"\"))\n\n# Add an index column for tracking purposes\ntokens &lt;- tokens %&gt;% mutate(ind = row_number())\nhead(tokens, 3)\n\n  doc_id      lemma ind\n1      1    purpose   1\n2      1    explain   2\n3      1 difference   3\n\n\nIn the above code:\n\ngsub('[[:digit:]]+', '', text_cleaning_tokens$lemma) removes any numeric characters.\ngsub('[[:punct:]]+', '', text_cleaning_tokens$lemma) removes punctuation.\nfilter(!(nchar(lemma) == 1)) filters out any single-character words.\nFinally, we remove any empty rows created by filtering.\n\nThe result is a cleaned dataset called tokens, ready for topic modeling.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Topic Modeling with R (Example)</span>"
    ]
  },
  {
    "objectID": "17.Topic_Modeling_Example.html#model-building-creating-the-document-feature-matrix-dfm",
    "href": "17.Topic_Modeling_Example.html#model-building-creating-the-document-feature-matrix-dfm",
    "title": "17  Topic Modeling with R (Example)",
    "section": "17.6 Model Building: Creating the Document-Feature Matrix (DFM)",
    "text": "17.6 Model Building: Creating the Document-Feature Matrix (DFM)\nAfter pre-processing, the next step in topic modeling is to create a DFM, which transforms our tokenized text data into a matrix suitable for topic modeling.\n\n17.6.1 Aggregate Token Counts per Document\nFirst, we need to calculate the frequency of each term (lemma) within each document. This step ensures that the DFM accurately reflects the number of times each term appears in each document.\n\n# Aggregate token counts per document\nlibrary(dplyr)\ntokens_doc &lt;- tokens %&gt;%\n  group_by(doc_id, lemma) %&gt;%\n  summarise(count = n())\n\n`summarise()` has grouped output by 'doc_id'. You can override using the\n`.groups` argument.\n\n\nHere, we use group_by(doc_id, lemma) to group the data by each document and term, and summarise(count = n()) to calculate the count of each term in each document.\n\n\n17.6.2 Convert to Document-Feature Matrix (DFM)\nWe use the quanteda and tidytext packages to convert our data into a DFM format, which is required for topic modeling. The cast_dfm function in tidytext helps convert a tidy data frame into a DFM.\n\n# Load the necessary packages\nlibrary(quanteda)\n\nWarning: package 'quanteda' was built under R version 4.3.3\n\n\nWarning in .recacheSubclasses(def@className, def, env): undefined subclass\n\"ndiMatrix\" of class \"replValueSp\"; definition not updated\n\n\nPackage version: 4.1.0\nUnicode version: 14.0\nICU version: 71.1\n\n\nParallel computing: disabled\n\n\nSee https://quanteda.io for tutorials and examples.\n\n\n\nAttaching package: 'quanteda'\n\n\nThe following object is masked from 'package:tm':\n\n    stopwords\n\n\nThe following objects are masked from 'package:NLP':\n\n    meta, meta&lt;-\n\nlibrary(tidytext)\n\n# Convert to Document-Feature Matrix\ndfm &lt;- tokens_doc %&gt;%\n  cast_dfm(doc_id, lemma, count)\n\nThe cast_dfm function takes three arguments:\n\ndoc_id: the document identifier column,\nlemma: the term or feature column, and\ncount: the frequency count of each term in each document.\n\nThe resulting DFM will have documents as rows, terms as columns, and frequency counts as the matrix values.\nTo check the dimensions of the DFM:\n\n# Check dimensions of the DFM\ndim(dfm)\n\n[1] 1517 6773\n\n# Expected output: [1517, 6773]\n\nHere, 1517 represents the number of documents, and 6773 represents the unique terms (or features) across all documents.\n\n\n17.6.3 Trim the DFM\nTo reduce sparsity in the DFM and improve the model’s focus, we apply frequency thresholds. Removing terms that appear very infrequently helps in reducing noise.\n\n# Trim the DFM\ntrimmed_dfm &lt;- dfm_trim(dfm, min_termfreq = 5, min_docfreq = 10)\n\nIn this command: - min_termfreq = 5 removes terms that appear fewer than 5 times across all documents. - min_docfreq = 10 removes terms that appear in fewer than 10 documents.\nAfter trimming, we check the new dimensions of the DFM:\n\n# Check dimensions after trimming\ndim(trimmed_dfm)\n\n[1] 1517 1287\n\n# Expected output: [1517, 1287]\n\nIn this example, the DFM is reduced to 1287 terms after removing low-frequency terms.\n\n\n17.6.4 Save the Trimmed DFM\nSaving the trimmed DFM allows us to use this matrix as input for our topic model in the next steps. It’s often helpful to save it as a CSV file for future use or verification.\n\n# Convert the DFM to a data frame using `convert`\ndfm_dataframe &lt;- convert(trimmed_dfm, to = \"data.frame\")\n\n# Save the data frame to a CSV file\nwrite.csv(dfm_dataframe, \"trimmed_dfm.csv\")\n\nThis file, trimmed_dfm.csv, is now ready for further analysis or to serve as the input for topic modeling.\nTo display the top features (most frequent terms) in the trimmed Document-Feature Matrix (DFM), we can use the topfeatures function from the quanteda package. This function will show the 20 most frequent terms in the trimmed_dfm.\n\n# Display the top 20 features in the trimmed DFM\ntop_terms &lt;- topfeatures(trimmed_dfm, n = 20)\ntop_terms\n\n   student    teacher    English        use      learn   language     school \n      3765       2311       2226       2111       1934       1515       1381 \n   learner      class elementary       test   teaching   activity      write \n      1227       1027        985        984        931        921        916 \n    effect    purpose vocabulary  classroom  education    program \n       746        728        698        677        653        644 \n\n\n\ntopfeatures(trimmed_dfm, n = 20) retrieves the 20 most frequent terms (features) in the trimmed_dfm.\ntop_terms will be a named vector where the names are terms, and the values are their frequencies across the corpus.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Topic Modeling with R (Example)</span>"
    ]
  },
  {
    "objectID": "17.Topic_Modeling_Example.html#building-and-evaluating-the-lda-model",
    "href": "17.Topic_Modeling_Example.html#building-and-evaluating-the-lda-model",
    "title": "17  Topic Modeling with R (Example)",
    "section": "17.7 Building and Evaluating the LDA Model",
    "text": "17.7 Building and Evaluating the LDA Model\nThis section covers Latent Dirichlet Allocation (LDA) with Perplexity Calculation. In this approach, we implement an LDA model and use cross-validation to determine the optimal number of topics by minimizing perplexity, a measure of model quality. This is done through 5-fold cross-validation, testing multiple topic counts to find the most appropriate configuration for the topic model.\n\n17.7.1 Convert DFM to Document-Term Matrix (DTM)\nTo run an LDA model using the topicmodels package, we need the data in a DocumentTermMatrix format. The convert function from the quanteda package simplifies this process, converting the trimmed_dfm to a DTM format compatible with topicmodels.\n\n# Load the necessary package\nlibrary(topicmodels)\n\n# Convert the trimmed DFM to a Document-Term Matrix for topic modeling\ndtm &lt;- convert(trimmed_dfm, to = \"topicmodels\", omit_empty = TRUE)\n\n# Check the dimensions of the DTM\ndim(dtm) # Expected output: [1517, 1287]\n\n[1] 1517 1287\n\n\n\n\n17.7.2 Set Up Parallel Processing for Cross-Validation\nTo optimize the topic model, we use 5-fold cross-validation across multiple values of k (the number of topics). Parallel processing speeds up this process by allowing each fold or candidate k value to be processed on a separate CPU core.\n\n# Load libraries for parallel processing\nlibrary(doParallel)\n\nLoading required package: foreach\n\n\n\nAttaching package: 'foreach'\n\n\nThe following objects are masked from 'package:purrr':\n\n    accumulate, when\n\n\nLoading required package: iterators\n\n\nLoading required package: parallel\n\nlibrary(scales)\n\n\nAttaching package: 'scales'\n\n\nThe following object is masked from 'package:purrr':\n\n    discard\n\n\nThe following object is masked from 'package:readr':\n\n    col_factor\n\n# Set up a cluster with one fewer core than available\ncluster &lt;- makeCluster(detectCores(logical = TRUE) - 1)\nregisterDoParallel(cluster)\n\n# Load `topicmodels` on all cluster nodes\nclusterEvalQ(cluster, {\n   library(topicmodels)\n})\n\n[[1]]\n[1] \"topicmodels\" \"stats\"       \"graphics\"    \"grDevices\"   \"utils\"      \n[6] \"datasets\"    \"methods\"     \"base\"       \n\n[[2]]\n[1] \"topicmodels\" \"stats\"       \"graphics\"    \"grDevices\"   \"utils\"      \n[6] \"datasets\"    \"methods\"     \"base\"       \n\n[[3]]\n[1] \"topicmodels\" \"stats\"       \"graphics\"    \"grDevices\"   \"utils\"      \n[6] \"datasets\"    \"methods\"     \"base\"       \n\n[[4]]\n[1] \"topicmodels\" \"stats\"       \"graphics\"    \"grDevices\"   \"utils\"      \n[6] \"datasets\"    \"methods\"     \"base\"       \n\n[[5]]\n[1] \"topicmodels\" \"stats\"       \"graphics\"    \"grDevices\"   \"utils\"      \n[6] \"datasets\"    \"methods\"     \"base\"       \n\n[[6]]\n[1] \"topicmodels\" \"stats\"       \"graphics\"    \"grDevices\"   \"utils\"      \n[6] \"datasets\"    \"methods\"     \"base\"       \n\n[[7]]\n[1] \"topicmodels\" \"stats\"       \"graphics\"    \"grDevices\"   \"utils\"      \n[6] \"datasets\"    \"methods\"     \"base\"       \n\n\n\n\n17.7.3 Define LDA Model Parameters and Cross-Validation Settings\nSet the parameters for the LDA model:\n\nburnin, iter, and keep control the Gibbs sampling process.\nfolds sets the number of cross-validation folds.\ncandidate_k provides the values of k (number of topics) to test.\n\n\n# Define LDA model parameters\nburnin = 1000\niter = 1000\nkeep = 50\n\n# Define cross-validation settings\nfolds &lt;- 5\nn &lt;- nrow(dtm)\nsplitfolds &lt;- sample(1:folds, n, replace = TRUE)\ncandidate_k &lt;- c(10:30) # different candidate values for the number of topics\n\n# Export necessary objects to all cluster nodes\nclusterExport(cluster, c(\"dtm\", \"burnin\", \"iter\", \"keep\", \"splitfolds\", \"folds\", \"candidate_k\"))\n\n\n\n17.7.4 Perform 5-Fold Cross-Validation for Perplexity Calculation\nThe foreach loop runs cross-validation for each candidate value of k in parallel. For each candidate k, it: 1. Splits the data into training and validation sets. 2. Fits an LDA model on the training set. 3. Calculates the perplexity on the validation set.\n# Run cross-validation for each candidate number of topics\nsystem.time({\n  results &lt;- foreach(j = 1:length(candidate_k), .combine = rbind) %dopar% {\n     k &lt;- candidate_k[j]\n     results_1k &lt;- matrix(0, nrow = folds, ncol = 2)\n     colnames(results_1k) &lt;- c(\"k\", \"perplexity\")\n     for(i in 1:folds){\n        train_set &lt;- dtm[splitfolds != i, ]\n        valid_set &lt;- dtm[splitfolds == i, ]\n  \n        fitted &lt;- LDA(train_set, k = k, method = \"Gibbs\",\n                      control = list(burnin = burnin, iter = iter, keep = keep))\n        results_1k[i,] &lt;- c(k, perplexity(fitted, newdata = valid_set))\n        }\n     return(results_1k)\n  }\n})\n# Stop the cluster after cross-validation\nstopCluster(cluster)\n\n# Convert results to a data frame\nresults_df &lt;- as.data.frame(results)\n\n\n17.7.5 Visualize the Perplexity Scores for Different Topic Counts\nTo determine the optimal number of topics, plot the perplexity scores against the candidate values of k. The ggplot2 package provides a straightforward way to visualize this relationship.\n# Load ggplot2 for plotting\nlibrary(ggplot2)\n\n# Plot perplexity scores for each candidate number of topics\nggplot(results_df, aes(x = k, y = perplexity)) +\n   geom_point() +\n   geom_smooth(se = FALSE) +\n   ggtitle(\"5-Fold Cross-Validation of Topic Modeling\") +\n   labs(x = \"Candidate Number of Topics\", y = \"Perplexity\")\n\nIn the plot, the x-axis represents the number of topics, and the y-axis shows the perplexity scores. The optimal number of topics minimizes perplexity, indicating a balance between model complexity and generalization to unseen data.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Topic Modeling with R (Example)</span>"
    ]
  },
  {
    "objectID": "17.Topic_Modeling_Example.html#training-and-analyzing-the-lda-model",
    "href": "17.Topic_Modeling_Example.html#training-and-analyzing-the-lda-model",
    "title": "17  Topic Modeling with R (Example)",
    "section": "17.8 Training and Analyzing the LDA Model",
    "text": "17.8 Training and Analyzing the LDA Model\nAfter determining the optimal number of topics, we can train a final LDA model using the entire dataset. This section demonstrates how to build the model, interpret its results, and save relevant matrices for further analysis.\n\n17.8.1 Define the Number of Topics and Train the LDA Model\nIn this example, we set K = 10 to define the number of topics and train the model using the Gibbs sampling method, a common approach for LDA.\n\n# Load the topicmodels package\nlibrary(topicmodels)\n\n# Define the number of topics\nK &lt;- 10\n\n# Train the LDA model\nlda_model &lt;- LDA(dtm, k = K, method = \"Gibbs\", \n                 control = list(alpha = 50/K, delta = 0.1, verbose = 25L, \n                                seed = 123, burnin = 100, iter = 500))\n\nK = 10; V = 1287; M = 1517\nSampling 600 iterations!\nIteration 25 ...\nIteration 50 ...\nIteration 75 ...\nIteration 100 ...\nIteration 125 ...\nIteration 150 ...\nIteration 175 ...\nIteration 200 ...\nIteration 225 ...\nIteration 250 ...\nIteration 275 ...\nIteration 300 ...\nIteration 325 ...\nIteration 350 ...\nIteration 375 ...\nIteration 400 ...\nIteration 425 ...\nIteration 450 ...\nIteration 475 ...\nIteration 500 ...\nIteration 525 ...\nIteration 550 ...\nIteration 575 ...\nIteration 600 ...\nGibbs sampling completed!\n\n\nIn this code: - alpha = 50/K and delta = 0.1 are hyperparameters that control the distribution of topics in documents and words in topics, respectively. - seed = 123 ensures reproducibility. - burnin = 100 and iter = 500 control the number of Gibbs sampling iterations.\n\n\n17.8.2 Extract Posterior Distributions\nThe posterior function extracts the posterior distributions from the trained LDA model, giving us insight into the topics and terms.\n\n# Obtain the posterior distributions of the LDA model\ntmResult &lt;- posterior(lda_model)\n\n# Check the attributes of the result\nattributes(tmResult)\n\n$names\n[1] \"terms\"  \"topics\"\n\n# Save the document-topic distribution to a CSV file\nwrite.csv(tmResult$topics, \"tmResult.csv\")\n\nThe posterior function returns a list with: - tmResult$topics: a document-topic distribution matrix (theta), where each row represents a document and each column represents a topic. - tmResult$terms: a topic-term distribution matrix (beta), where each row represents a topic and each column represents a term.\n\n\n17.8.3 Analyze the Topic-Term Distribution (beta)\nThe beta matrix shows the probability of each word appearing in each topic. This information can help us interpret the topics by identifying the most likely words for each one.\n\n# Extract the topic-term distribution matrix\nbeta &lt;- tmResult$terms\n\n# Save the beta matrix to a CSV file\nwrite.csv(beta, 'beta.csv')\n\n# Check the dimensions of the beta matrix\ndim(beta)  # Expected output: K x ncol(DTM)\n\n[1]   10 1287\n\n\nTo verify that each row of beta represents a probability distribution (i.e., sums to 1):\n\n# Verify that each row in beta sums to 1\nrowSums(beta)\n\n 1  2  3  4  5  6  7  8  9 10 \n 1  1  1  1  1  1  1  1  1  1 \n\n\n\n\n17.8.4 Analyze the Document-Topic Distribution (theta)\nEach row in the theta matrix represents a document, and each column represents a topic. The values indicate the probability of each topic in each document.\n\n# Extract the document-topic distribution matrix\ntheta &lt;- tmResult$topics\n\n# Save the theta matrix to a CSV file\nwrite.csv(theta, 'theta.csv')\n\n\n\n17.8.5 Create and Save a Matrix of Top Words for Each Topic\nTo facilitate interpretation, it’s often helpful to list the top words for each topic in a more human-readable format. We can transpose the beta matrix and round values to simplify the results.\n\n# Round the beta values to three decimal places for readability\nphi_matrix &lt;- round(beta, 3)\n\n# Transpose and convert to data frame for saving\nT_phi_matrix &lt;- as.data.frame(t(phi_matrix))\nnames(T_phi_matrix) &lt;- paste(\"Topic\", 1:K)\n\n# Save the transposed matrix with topic words\nwrite.csv(T_phi_matrix, \"lda_topic_words.csv\")\n\n# Display the first few words for each topic\nT_phi_matrix[1:K, ]\n\n               Topic 1 Topic 2 Topic 3 Topic 4 Topic 5 Topic 6 Topic 7 Topic 8\nEnglish          0.001   0.000   0.013   0.000   0.058   0.000   0.000   0.094\nKorean           0.000   0.000   0.000   0.002   0.000   0.000   0.000   0.000\nable             0.000   0.000   0.000   0.002   0.000   0.002   0.000   0.000\nacquire          0.000   0.000   0.000   0.000   0.000   0.004   0.000   0.000\narise            0.000   0.000   0.000   0.000   0.000   0.000   0.000   0.001\nbasis            0.000   0.000   0.000   0.000   0.000   0.001   0.006   0.000\ncharacteristic   0.000   0.000   0.006   0.000   0.000   0.000   0.004   0.000\ncorrect          0.000   0.000   0.000   0.000   0.000   0.000   0.000   0.000\ndifference       0.033   0.002   0.000   0.000   0.001   0.000   0.000   0.000\neducation        0.000   0.000   0.000   0.000   0.000   0.000   0.000   0.048\n               Topic 9 Topic 10\nEnglish          0.005    0.002\nKorean           0.014    0.000\nable             0.001    0.000\nacquire          0.000    0.000\narise            0.000    0.000\nbasis            0.000    0.000\ncharacteristic   0.000    0.000\ncorrect          0.003    0.000\ndifference       0.000    0.000\neducation        0.000    0.000\n\n\nIn this transposed matrix, each row corresponds to a word, and each column represents a topic, showing the top words for each topic.\nThis analysis yields several matrices:\n\nbeta: Topic-term distribution, showing the most probable words per topic.\ntheta: Document-topic distribution, showing topic proportions for each document.\nTransposed beta: A matrix of top words per topic, which can be saved for easy inspection.\n\nThese outputs allow us to interpret and label topics meaningfully, bringing us closer to understanding the thematic structure in our dataset.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Topic Modeling with R (Example)</span>"
    ]
  },
  {
    "objectID": "17.Topic_Modeling_Example.html#extracting-and-visualizing-topics",
    "href": "17.Topic_Modeling_Example.html#extracting-and-visualizing-topics",
    "title": "17  Topic Modeling with R (Example)",
    "section": "17.9 Extracting and Visualizing Topics",
    "text": "17.9 Extracting and Visualizing Topics\n\n17.9.1 Extract the Top 15 Words for Each Topic\nThe terms function can be used to identify the top terms in each topic. In this example, we extract the 15 most probable words for each topic and save the results to a CSV file for easy reference.\n\n# Extract the top 15 words for each topic\ntop_words &lt;- terms(lda_model, 15)\n\n# Save the top words to a CSV file\nwrite.csv(top_words, \"lda_topic_15words.csv\")\n\nThis code produces a table where each column represents a topic, and each row lists one of the top 15 words associated with that topic. This table can be helpful for interpreting and labeling topics.\n\n\n17.9.2 Visualize the LDA Model with LDAvis\nVisualizing the LDA model provides a dynamic way to explore topics. The LDAvis package, combined with the topicmodels2LDAvis function below, generates an interactive visualization that displays: - The distance between topics (e.g., similar topics will appear closer together). - The most influential words within each topic.\n\n17.9.2.1 Install and Load Required Libraries\nEnsure that LDAvis and servr are installed, as they are required for creating and serving the visualization.\n\n# Install and load required packages\n# install.packages(\"LDAvis\")\nlibrary(LDAvis)\n\n# install.packages(\"servr\")\nlibrary(servr)\n\n\n\n17.9.2.2 Define a Helper Function for LDAvis\nTo visualize a model created with the topicmodels package, we need a custom function to convert the LDA model to the format required by LDAvis. The following topicmodels2LDAvis function extracts the necessary information from the lda_model.\n\n# Helper function to convert the LDA model to LDAvis format\ntopicmodels2LDAvis &lt;- function(x, ...){\n    post &lt;- topicmodels::posterior(x)\n    if (ncol(post[[\"topics\"]]) &lt; 3) stop(\"The model must contain &gt; 2 topics\")\n    mat &lt;- x@wordassignments\n    LDAvis::createJSON(\n        phi = post[[\"terms\"]], \n        theta = post[[\"topics\"]],\n        vocab = colnames(post[[\"terms\"]]),\n        doc.length = slam::row_sums(mat, na.rm = TRUE),\n        term.frequency = slam::col_sums(mat, na.rm = TRUE)\n    )\n}\n\nThis function extracts:\n\nphi (topic-term distribution matrix).\ntheta (document-topic distribution matrix).\nvocab (list of terms).\ndoc.length (number of terms per document).\nterm.frequency (frequency of each term in the entire corpus).\n\n\n\n17.9.2.3 Generate and Display the Visualization\nFinally, we use the serVis function from LDAvis to create and serve the visualization locally in a web browser.\n\n# Generate and view the LDA visualization\nLDAvis::serVis(topicmodels2LDAvis(lda_model))\n\n\nThis command opens an interactive visualization in your browser, where:\n\nEach circle represents a topic.\nThe size of each circle indicates the prevalence of the topic in the corpus.\nWords within each topic are displayed on the right, ranked by their importance.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Topic Modeling with R (Example)</span>"
    ]
  },
  {
    "objectID": "17.Topic_Modeling_Example.html#topic-ranking-and-trends",
    "href": "17.Topic_Modeling_Example.html#topic-ranking-and-trends",
    "title": "17  Topic Modeling with R (Example)",
    "section": "17.10 Topic Ranking and Trends",
    "text": "17.10 Topic Ranking and Trends\nThis section expands on Topic Ranking and introduces a Hot and Cold Topics Analysis. Topic ranking enables us to identify and name dominant topics across the collection. Hot and cold topic analysis explores topic trends over time, identifying which topics are gaining or losing prominence.\n\n17.10.1 Step 1: Rank Topics by Defining Terms\nTo create meaningful names for each topic, we can identify the top defining words per topic based on specificity rather than mere frequency. The top.topic.words function from the lda package ranks terms by specificity, which highlights unique terms that best describe each topic.\n\n# Install and load the lda package if not installed\n# install.packages('lda')\nlibrary(lda)\n\n# Generate topic names by selecting the top 6 terms for each topic\ntopicNames &lt;- apply(lda::top.topic.words(beta, 6, by.score = TRUE), 2, paste, collapse = \" \")\n\nIn this code: - top.topic.words(beta, 6, by.score = TRUE) selects the top six terms per topic from the beta matrix. - apply(..., 2, paste, collapse = \" \") concatenates these terms into a single name for each topic.\n\n\n17.10.2 Step 2: Calculate Overall Topic Proportions\nTo assess which topics are most prevalent in the corpus, we calculate the average topic proportions across all documents.\n\n# Calculate average topic proportions across all documents\ntopicProportions &lt;- colSums(theta) / nrow(dtm)  # mean probabilities over all documents\n\n# Assign names to the topic proportions\nnames(topicProportions) &lt;- topicNames\n\n# Save topic proportions to a CSV\nwrite.csv(topicProportions, \"topicProportions.csv\")\n\n# Display topics sorted by proportion\nsorted_topic_proportions &lt;- sort(topicProportions, decreasing = TRUE)\nsorted_topic_proportions\n\n    teacher teaching English program education school \n                                           0.10690439 \n  activity school elementary student English textbook \n                                           0.10323217 \n             learn use learning student online course \n                                           0.10148885 \n language classroom practice approach context culture \n                                           0.10127252 \n      student write writing data participant feedback \n                                           0.10016247 \n                  word read use reading strategy text \n                                           0.09944170 \n        develop material child model question purpose \n                                           0.09925692 \n        test speak proficiency listen high assessment \n                                           0.09641813 \nvocabulary effect instruction learner difference type \n                                           0.09600700 \n            learner task self factor interaction role \n                                           0.09581585 \n\n\nThe topicProportions vector indicates the overall importance of each topic in the corpus. Sorting by these values reveals the most prominent topics.\n\n\n17.10.3 Step 3: Count Primary Topics in Each Document\nNext, we can identify the most dominant topic (primary topic) within each document, helping us understand which topics are most frequently the main focus of documents.\n\n# Initialize a counter for primary topics\ncountsOfPrimaryTopics &lt;- rep(0, K)\nnames(countsOfPrimaryTopics) &lt;- topicNames\n\n# Count primary topics across documents\nfor (i in 1:nrow(dtm)) {\n  topicsPerDoc &lt;- theta[i, ]                # Select topic distribution for document i\n  primaryTopic &lt;- order(topicsPerDoc, decreasing = TRUE)[1]  # Identify the primary topic\n  countsOfPrimaryTopics[primaryTopic] &lt;- countsOfPrimaryTopics[primaryTopic] + 1\n}\n\n# Save primary topic counts to CSV\nwrite.csv(countsOfPrimaryTopics, 'countsOfPrimaryTopics.csv')\n\n# Display sorted counts of primary topics\nsorted_counts_primary_topics &lt;- sort(countsOfPrimaryTopics, decreasing = TRUE)\nsorted_counts_primary_topics\n\n    teacher teaching English program education school \n                                                  209 \n  activity school elementary student English textbook \n                                                  178 \n                  word read use reading strategy text \n                                                  173 \n      student write writing data participant feedback \n                                                  165 \n language classroom practice approach context culture \n                                                  163 \n             learn use learning student online course \n                                                  143 \n        test speak proficiency listen high assessment \n                                                  139 \nvocabulary effect instruction learner difference type \n                                                  128 \n            learner task self factor interaction role \n                                                  115 \n        develop material child model question purpose \n                                                  104 \n\n\nThis count indicates how often each topic is the primary focus in documents, providing a measure of topic prominence.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Topic Modeling with R (Example)</span>"
    ]
  },
  {
    "objectID": "17.Topic_Modeling_Example.html#hot-and-cold-topics-analysis",
    "href": "17.Topic_Modeling_Example.html#hot-and-cold-topics-analysis",
    "title": "17  Topic Modeling with R (Example)",
    "section": "17.11 Hot and Cold Topics Analysis",
    "text": "17.11 Hot and Cold Topics Analysis\nTo observe trends over time, we analyze topic proportions across years to identify “hot” (increasing) and “cold” (decreasing) topics.\n\n17.11.1 Step 1: Prepare Data for Temporal Analysis\nWe first convert tokenized data to a “wide” format to facilitate yearly aggregation. This transformation makes each document a single row, with terms as columns, simplifying further analysis.\n\n# Spread tokens to wide format\nlibrary(dplyr)\n\ntokens_year &lt;- tokens %&gt;% \n  group_by(doc_id) %&gt;% \n  mutate(ind = row_number()) %&gt;% \n  tidyr::spread(key = ind, value = lemma)\n\n# Replace NA values with empty strings\ntokens_year[is.na(tokens_year)] &lt;- \"\" \n\n# Unite terms back into a single column for each document\ntokens_year &lt;- tidyr::unite(tokens_year, abstract, -doc_id, sep = \" \")\ntokens_year$abstract &lt;- trimws(tokens_year$abstract)\n\n# Merge with original data to add additional document information (e.g., year)\ntokens_year &lt;- merge(x = tokens_year, y = data, by = \"doc_id\")\n\n\n\n17.11.2 Step 2: Calculate Topic Proportions Over Time\nBy aggregating topic proportions by year, we can observe how each topic’s importance changes over time.\n\n# Install and load reshape if not already installed\n# install.packages('reshape')\nlibrary(reshape)\n\n\nAttaching package: 'reshape'\n\n\nThe following object is masked from 'package:lubridate':\n\n    stamp\n\n\nThe following object is masked from 'package:dplyr':\n\n    rename\n\n\nThe following objects are masked from 'package:tidyr':\n\n    expand, smiths\n\n# Calculate the mean topic proportion per year\ntopic_proportion_per_year &lt;- aggregate(theta, by = list(year = tokens_year$year), mean)\n\n# Save topic proportions per year to a CSV\nwrite.csv(topic_proportion_per_year, 'topic_proportion_per_year.csv')\n\nThis code creates a table where each row represents a year and each column represents the average proportion of a topic for that year.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Topic Modeling with R (Example)</span>"
    ]
  },
  {
    "objectID": "17.Topic_Modeling_Example.html#hot-and-cold-topics-analysis-1",
    "href": "17.Topic_Modeling_Example.html#hot-and-cold-topics-analysis-1",
    "title": "17  Topic Modeling with R (Example)",
    "section": "17.12 Hot and Cold Topics Analysis",
    "text": "17.12 Hot and Cold Topics Analysis\nIn this section, we conduct Hot and Cold Topics Analysis using a time-series approach to identify topics that have increased (hot topics) or decreased (cold topics) in prominence over time. This analysis involves calculating topic trends across years, assessing statistical significance, and visualizing the most prominent hot and cold topics.\n\n17.12.1 Step 1: Calculate Mean Topic Proportions by Year\nTo identify hot and cold topics, we first calculate the mean proportion of each topic per year. This provides a time series of topic prevalence that we can analyze for trends.\n\nlibrary(tm)\n\n# Extract unique years\nyears &lt;- levels(factor(tokens_year$year))  # Check years, e.g., 2000 ~ 2019\n\n# Set the number of topics\ntopics_n &lt;- K    # In this case, 10\n\n# Extract the document-topic distribution matrix\ntheta_2 &lt;- posterior(lda_model)$topics\n\n# Calculate the mean topic proportions by year\ntheta_mean_by_year_by &lt;- by(theta_2, tokens_year$year, colMeans)\ntheta_mean_by_year &lt;- do.call(\"rbind\", theta_mean_by_year_by)  # Combine results into a matrix\n\n# Rename columns and create a time series object\ncolnames(theta_mean_by_year) &lt;- paste(1:topics_n)\ntheta_mean_by_year_ts &lt;- ts(theta_mean_by_year, start = as.integer(years[1]))\ntheta_mean_by_year_time &lt;- time(theta_mean_by_year)\n\n# Save results to CSV\nwrite.csv(theta_mean_by_year, \"theta_mean_by_year.csv\")\ntheta_mean_by_year_time\n\n [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20\nattr(,\"tsp\")\n[1]  1 20  1\n\n\n\n\n17.12.2 Step 2: Identify Hot and Cold Topics with Linear Regression\nTo determine which topics are trending up (hot) or down (cold) over time, we fit a linear regression model for each topic’s time series. The slope of each model indicates the direction and magnitude of change.\n\n# Perform linear regression on topic proportions over time\ntheta_mean_lm &lt;- apply(theta_mean_by_year, 2, function(x) lm(x ~ theta_mean_by_year_time))\n\n# Extract regression coefficients and p-values\ntheta_mean_lm_coef &lt;- lapply(theta_mean_lm, function(x) coef(summary(x)))\n\n# Extract significance (p-values) and slopes\ntheta_mean_lm_coef_sign &lt;- sapply(theta_mean_lm_coef, '[', \"theta_mean_by_year_time\", \"Pr(&gt;|t|)\")\ntheta_mean_lm_coef_slope &lt;- sapply(theta_mean_lm_coef, '[', \"theta_mean_by_year_time\", \"Estimate\")\n\n# Separate positive and negative slopes for analysis\ntheta_mean_lm_coef_slope_pos &lt;- theta_mean_lm_coef_slope[theta_mean_lm_coef_slope &gt;= 0]\ntheta_mean_lm_coef_slope_neg &lt;- theta_mean_lm_coef_slope[theta_mean_lm_coef_slope &lt; 0]\n\n# Save positive and negative slopes to CSV\nwrite.csv(theta_mean_lm_coef_slope_pos, 'theta_mean_lm_coef_slope_positive.csv')\nwrite.csv(theta_mean_lm_coef_slope_neg, 'theta_mean_lm_coef_slope_negative.csv')\n\n\n\n17.12.3 Step 3: Evaluate Significance Levels\nWe test the statistical significance of each trend using various p-value thresholds to determine which trends are likely meaningful.\n\n# Set p-value thresholds\np_level &lt;- c(0.05, 0.01, 0.001, 0.0001)\n\n# Identify topics with significant slopes at each p-level\nsignificance_total &lt;- sapply(p_level, function(x) (theta_mean_lm_coef_sign[theta_mean_lm_coef_sign &lt; x]))\nsignificance_neg &lt;- sapply(1:length(p_level), function(x) intersect(names(theta_mean_lm_coef_slope_neg), names(significance_total[[x]])))\nsignificance_pos &lt;- sapply(1:length(p_level), function(x) intersect(names(theta_mean_lm_coef_slope_pos), names(significance_total[[x]])))\n\n\n\n17.12.4 Step 4: Visualize Hot and Cold Topics\nUsing the lattice package, we can plot the time series of hot and cold topics to observe their changes over time.\n\nlibrary(lattice)\n\n# Select top hot and cold topics for visualization\ntopics_cold &lt;- as.numeric(names(sort(theta_mean_lm_coef_slope[significance_neg[[2]]], decreasing = FALSE)))\ntopics_hot &lt;- as.numeric(names(sort(theta_mean_lm_coef_slope[significance_pos[[2]]], decreasing = TRUE)))\n\n# Combine selected hot and cold topics into a single time series matrix\ncold_and_hot_ts &lt;- cbind(theta_mean_by_year_ts[, topics_cold[1:2]],\n                         theta_mean_by_year_ts[, topics_hot[1:3]], deparse.level = 0)  # Adjust numbers as needed\n\n# Rename columns to indicate topic numbers\ncolnames(cold_and_hot_ts) &lt;- as.character(c(topics_cold[1:2], topics_hot[1:3]))\n\n# Plot hot and cold topics over time\nprint(xyplot(cold_and_hot_ts, layout = c(2, 1), \n             screens = c(rep(\"Cold topics\", 2), rep(\"Hot topics\", 3)),  # Adjust as needed\n             superpose = TRUE, ylim = c(0.05, 0.2), \n             ylab = expression(paste(\"Mean \", theta)), \n             xlab = \"Year\", type = c(\"l\", \"g\"), \n             auto.key = list(space = \"right\"),  \n             scales = list(x = list(alternating = FALSE))\n))\n\n\n\n\n\n\n\n\nThis plot displays the time trends for the top hot and cold topics. Hot topics are increasing over time, while cold topics are decreasing, providing a visual representation of shifting thematic focus in the corpus.\n\n\n17.12.5 Step 5: Summarize Time-Series Analysis Results\nTo further analyze, extract regression summaries for each topic’s time series.\n\n# Extract detailed regression summaries for all topics\nreg_result &lt;- lapply(theta_mean_lm, function(x) summary(x))\nreg_result\n\n$`1`\n\nCall:\nlm(formula = x ~ theta_mean_by_year_time)\n\nResiduals:\n       Min         1Q     Median         3Q        Max \n-0.0106224 -0.0054520 -0.0006585  0.0030597  0.0123526 \n\nCoefficients:\n                         Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)             0.0791519  0.0033328  23.749 4.87e-15 ***\ntheta_mean_by_year_time 0.0014388  0.0002782   5.171 6.41e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.007175 on 18 degrees of freedom\nMultiple R-squared:  0.5977,    Adjusted R-squared:  0.5754 \nF-statistic: 26.74 on 1 and 18 DF,  p-value: 6.415e-05\n\n\n$`2`\n\nCall:\nlm(formula = x ~ theta_mean_by_year_time)\n\nResiduals:\n       Min         1Q     Median         3Q        Max \n-0.0140039 -0.0059298  0.0005148  0.0034152  0.0136802 \n\nCoefficients:\n                         Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)             0.0877861  0.0032552  26.968 5.24e-16 ***\ntheta_mean_by_year_time 0.0007345  0.0002717   2.703   0.0146 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.007008 on 18 degrees of freedom\nMultiple R-squared:  0.2887,    Adjusted R-squared:  0.2492 \nF-statistic: 7.307 on 1 and 18 DF,  p-value: 0.01456\n\n\n$`3`\n\nCall:\nlm(formula = x ~ theta_mean_by_year_time)\n\nResiduals:\n       Min         1Q     Median         3Q        Max \n-0.0133495 -0.0059436  0.0004885  0.0056141  0.0145932 \n\nCoefficients:\n                         Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)             0.0902989  0.0037379  24.158 3.61e-15 ***\ntheta_mean_by_year_time 0.0005222  0.0003120   1.674    0.111    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.008047 on 18 degrees of freedom\nMultiple R-squared:  0.1347,    Adjusted R-squared:  0.08659 \nF-statistic: 2.801 on 1 and 18 DF,  p-value: 0.1115\n\n\n$`4`\n\nCall:\nlm(formula = x ~ theta_mean_by_year_time)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.012520 -0.006555 -0.000802  0.003934  0.021989 \n\nCoefficients:\n                         Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)             0.0786041  0.0045596  17.239 1.23e-12 ***\ntheta_mean_by_year_time 0.0018166  0.0003806   4.773 0.000152 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.009816 on 18 degrees of freedom\nMultiple R-squared:  0.5586,    Adjusted R-squared:  0.5341 \nF-statistic: 22.78 on 1 and 18 DF,  p-value: 0.0001522\n\n\n$`5`\n\nCall:\nlm(formula = x ~ theta_mean_by_year_time)\n\nResiduals:\n       Min         1Q     Median         3Q        Max \n-0.0182790 -0.0030607 -0.0003752  0.0042530  0.0218266 \n\nCoefficients:\n                          Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)              0.1112232  0.0040568  27.416 3.92e-16 ***\ntheta_mean_by_year_time -0.0006941  0.0003387  -2.049   0.0553 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.008733 on 18 degrees of freedom\nMultiple R-squared:  0.1892,    Adjusted R-squared:  0.1442 \nF-statistic:   4.2 on 1 and 18 DF,  p-value: 0.05528\n\n\n$`6`\n\nCall:\nlm(formula = x ~ theta_mean_by_year_time)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.013077 -0.005287 -0.002604  0.005887  0.015043 \n\nCoefficients:\n                          Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)              0.1244954  0.0040789  30.522  &lt; 2e-16 ***\ntheta_mean_by_year_time -0.0019584  0.0003405  -5.751 1.88e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.008781 on 18 degrees of freedom\nMultiple R-squared:  0.6476,    Adjusted R-squared:  0.628 \nF-statistic: 33.08 on 1 and 18 DF,  p-value: 1.882e-05\n\n\n$`7`\n\nCall:\nlm(formula = x ~ theta_mean_by_year_time)\n\nResiduals:\n       Min         1Q     Median         3Q        Max \n-0.0132064 -0.0029839 -0.0000425  0.0017775  0.0147170 \n\nCoefficients:\n                          Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)              0.1081636  0.0031731  34.087   &lt;2e-16 ***\ntheta_mean_by_year_time -0.0007511  0.0002649  -2.836    0.011 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.006831 on 18 degrees of freedom\nMultiple R-squared:  0.3088,    Adjusted R-squared:  0.2704 \nF-statistic:  8.04 on 1 and 18 DF,  p-value: 0.01097\n\n\n$`8`\n\nCall:\nlm(formula = x ~ theta_mean_by_year_time)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.015552 -0.009472 -0.005539  0.006255  0.040635 \n\nCoefficients:\n                         Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)              0.113451   0.006720  16.883 1.76e-12 ***\ntheta_mean_by_year_time -0.000597   0.000561  -1.064    0.301    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.01447 on 18 degrees of freedom\nMultiple R-squared:  0.0592,    Adjusted R-squared:  0.006931 \nF-statistic: 1.133 on 1 and 18 DF,  p-value: 0.3013\n\n\n$`9`\n\nCall:\nlm(formula = x ~ theta_mean_by_year_time)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.010320 -0.006048 -0.002329  0.004951  0.013002 \n\nCoefficients:\n                         Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)             0.0887010  0.0033473   26.50 7.14e-16 ***\ntheta_mean_by_year_time 0.0009250  0.0002794    3.31  0.00389 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.007206 on 18 degrees of freedom\nMultiple R-squared:  0.3784,    Adjusted R-squared:  0.3439 \nF-statistic: 10.96 on 1 and 18 DF,  p-value: 0.003891\n\n\n$`10`\n\nCall:\nlm(formula = x ~ theta_mean_by_year_time)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.015222 -0.005836  0.001027  0.004734  0.021663 \n\nCoefficients:\n                          Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)              0.1181249  0.0041647  28.364  &lt; 2e-16 ***\ntheta_mean_by_year_time -0.0014367  0.0003477  -4.132 0.000625 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.008965 on 18 degrees of freedom\nMultiple R-squared:  0.4868,    Adjusted R-squared:  0.4583 \nF-statistic: 17.08 on 1 and 18 DF,  p-value: 0.0006254\n\n\nThe reg_result object contains detailed regression summaries for each topic, including coefficients, p-values, and R-squared values.\nSummary\nThis hot and cold topics analysis highlights the changing prominence of topics over time:\n\nHot Topics: Topics with significantly positive trends.\nCold Topics: Topics with significantly negative trends.\n\nThese trends provide insights into the evolution of topics, revealing which areas of discussion have gained or lost interest. This analysis is especially valuable for diachronic studies and can uncover shifts in focus across different time periods in the dataset.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Topic Modeling with R (Example)</span>"
    ]
  },
  {
    "objectID": "C.assignment3_topic.html",
    "href": "C.assignment3_topic.html",
    "title": "20  Assignment #03:Topic Modeling",
    "section": "",
    "text": "20.1 Instructions:\nIn this assignment, you’ll perform a topic modeling analysis on a selected corpus to explore and interpret themes. Follow the steps below to complete your work, and submit your results as instructed. Provide clear explanations and visualizations to support your analysis.",
    "crumbs": [
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Assignment #03:Topic Modeling</span>"
    ]
  },
  {
    "objectID": "C.assignment3_topic.html#steps-for-the-assignment",
    "href": "C.assignment3_topic.html#steps-for-the-assignment",
    "title": "20  Assignment #03:Topic Modeling",
    "section": "20.2 Steps for the Assignment",
    "text": "20.2 Steps for the Assignment\nStep 1. Select Your Corpus (5 points)\nCorpus Selection:\n\nChoose a text corpus that interests you. Potential sources include:\n\nProject Gutenberg: a collection of public domain books.\nKaggle Datasets: various datasets that may include text.\nOther text-based datasets are also acceptable.\n\n\nCorpus Details:\n\nProvide a brief description of your chosen corpus (1-2 sentences).\nMention the source and why you selected this corpus (1-2 sentences).\n\n\nStep 2. Conduct Topic Modeling Analysis (15 points)\nPreprocessing (5 points):\n\nPrepare the text for analysis by following standard preprocessing steps:\n\nTokenization (splitting text into words or phrases).\nStop word removal.\nLemmatization or stemming.\nVectorization (transforming text into numerical form, if needed).\n\nDescribe these preprocessing steps in your final submission.\n\nTopic Modeling (5 points):\n\nImplement a topic modeling technique, such as Latent Dirichlet Allocation (LDA) to identify topics within your corpus.\nProvide a summary of each identified topic, including representative keywords or phrases.\nInclude a visualization\n\nTopic Interpretation and Analysis (5 points):\n\nInterpret the themes or patterns within the identified topics. Explain any notable or recurring themes and how they relate to the overall structure of your corpus.",
    "crumbs": [
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Assignment #03:Topic Modeling</span>"
    ]
  },
  {
    "objectID": "C.assignment3_topic.html#submission-guidelines",
    "href": "C.assignment3_topic.html#submission-guidelines",
    "title": "20  Assignment #03:Topic Modeling",
    "section": "20.3 Submission Guidelines",
    "text": "20.3 Submission Guidelines\n\nReport Format: Submit a PDF (or MS-Word) report (2 pages, approximately 500 words), including screenshots of visualizations.\nPython or R Code\n\nDue Date: Submit by November 28, 2024, via the course portal.\nTotal: 20 points",
    "crumbs": [
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Assignment #03:Topic Modeling</span>"
    ]
  }
]