<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.57">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>11&nbsp; Text Representation Based on Counts – Basic AI Programming</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./12.Word_Embedding.html" rel="next">
<link href="./10.Word_Cloud.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./11.Text_Representation.html"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Text Representation Based on Counts</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Basic AI Programming</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./01.setup.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Setting Up Environment</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./02.intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Constants and Variables in Python</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./03.control.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Control Structures in Python</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./04.function.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Functions and Packages in Python</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./05.pandas.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Data Analysis in Python</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./06.arithmetic.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Basic Arithmetic Operations in Python</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./07.Regular_Expression.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Regular Expressions (RegEx) in Python</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./08.Custom_Corpus.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Building a Custom Corpus</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./09.Preprocessing.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Text Preprocessing</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./10.Word_Cloud.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Visualizing Word Frequencies with Graphs and Word Clouds</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./11.Text_Representation.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Text Representation Based on Counts</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./12.Word_Embedding.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Word Embedding and Relational Similarity</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./13.NLP_toolkit.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Word Embedding Activities</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./14.Semantic_Network.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Semantic Network Analysis</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./15.SNA_Example.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Semantic Network Analysis (Examples)</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./16.Topic_Modeling.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Topic Modeling in Python</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./17.Topic_Modeling_Example.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Topic Modeling with R (Example)</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./A.assignment1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Assignment #01</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./B.assignment2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">Assignment #02: Semantic Network Analysis</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C.assignment3_topic.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">Assignment #03:Topic Modeling</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#what-is-a-vector" id="toc-what-is-a-vector" class="nav-link active" data-scroll-target="#what-is-a-vector"><span class="header-section-number">11.0.1</span> What is a vector?</a></li>
  <li><a href="#examples" id="toc-examples" class="nav-link" data-scroll-target="#examples"><span class="header-section-number">11.0.2</span> Examples:</a></li>
  <li><a href="#embedding-how-do-machines-understand-natural-language" id="toc-embedding-how-do-machines-understand-natural-language" class="nav-link" data-scroll-target="#embedding-how-do-machines-understand-natural-language"><span class="header-section-number">11.0.3</span> Embedding: How do machines understand natural language?</a></li>
  <li><a href="#history-of-the-embedding-concept" id="toc-history-of-the-embedding-concept" class="nav-link" data-scroll-target="#history-of-the-embedding-concept"><span class="header-section-number">11.0.4</span> History of the Embedding Concept</a></li>
  <li><a href="#the-neural-network-era" id="toc-the-neural-network-era" class="nav-link" data-scroll-target="#the-neural-network-era"><span class="header-section-number">11.0.5</span> The Neural Network Era</a></li>
  <li><a href="#embedding-representation-methods" id="toc-embedding-representation-methods" class="nav-link" data-scroll-target="#embedding-representation-methods"><span class="header-section-number">11.1</span> Embedding Representation Methods</a>
  <ul class="collapse">
  <li><a href="#local-representation" id="toc-local-representation" class="nav-link" data-scroll-target="#local-representation"><span class="header-section-number">11.1.1</span> <strong>Local Representation:</strong></a></li>
  <li><a href="#continuous-representation" id="toc-continuous-representation" class="nav-link" data-scroll-target="#continuous-representation"><span class="header-section-number">11.1.2</span> <strong>Continuous Representation:</strong></a></li>
  <li><a href="#sparse-representation-one-hot-encoding" id="toc-sparse-representation-one-hot-encoding" class="nav-link" data-scroll-target="#sparse-representation-one-hot-encoding"><span class="header-section-number">11.1.3</span> <strong>Sparse Representation: One-Hot Encoding</strong></a></li>
  <li><a href="#distributed-representation" id="toc-distributed-representation" class="nav-link" data-scroll-target="#distributed-representation"><span class="header-section-number">11.1.4</span> <strong>Distributed Representation</strong></a></li>
  </ul></li>
  <li><a href="#examples-of-text-representation-methods" id="toc-examples-of-text-representation-methods" class="nav-link" data-scroll-target="#examples-of-text-representation-methods"><span class="header-section-number">11.2</span> Examples of Text Representation Methods</a>
  <ul class="collapse">
  <li><a href="#document-term-matrix-dtm" id="toc-document-term-matrix-dtm" class="nav-link" data-scroll-target="#document-term-matrix-dtm"><span class="header-section-number">11.2.1</span> Document-Term Matrix (DTM)</a></li>
  <li><a href="#concept-of-document-term-matrix" id="toc-concept-of-document-term-matrix" class="nav-link" data-scroll-target="#concept-of-document-term-matrix"><span class="header-section-number">11.2.2</span> Concept of Document-Term Matrix</a></li>
  <li><a href="#understanding-the-components" id="toc-understanding-the-components" class="nav-link" data-scroll-target="#understanding-the-components"><span class="header-section-number">11.2.3</span> Understanding the Components:</a></li>
  </ul></li>
  <li><a href="#tf-idf-term-frequency-inverse-document-frequency" id="toc-tf-idf-term-frequency-inverse-document-frequency" class="nav-link" data-scroll-target="#tf-idf-term-frequency-inverse-document-frequency"><span class="header-section-number">11.3</span> TF-IDF (Term Frequency-Inverse Document Frequency)</a>
  <ul class="collapse">
  <li><a href="#concept-of-tf-idf" id="toc-concept-of-tf-idf" class="nav-link" data-scroll-target="#concept-of-tf-idf"><span class="header-section-number">11.3.1</span> Concept of TF-IDF</a></li>
  <li><a href="#term-frequency-tf" id="toc-term-frequency-tf" class="nav-link" data-scroll-target="#term-frequency-tf"><span class="header-section-number">11.3.2</span> <strong>Term Frequency (TF)</strong></a></li>
  <li><a href="#inverse-document-frequency-idf" id="toc-inverse-document-frequency-idf" class="nav-link" data-scroll-target="#inverse-document-frequency-idf"><span class="header-section-number">11.3.3</span> <strong>Inverse Document Frequency (IDF)</strong></a></li>
  <li><a href="#tf-idf-score" id="toc-tf-idf-score" class="nav-link" data-scroll-target="#tf-idf-score"><span class="header-section-number">11.3.4</span> <strong>TF-IDF Score</strong></a></li>
  <li><a href="#example-python-code-using-sklearn-for-tf-idf" id="toc-example-python-code-using-sklearn-for-tf-idf" class="nav-link" data-scroll-target="#example-python-code-using-sklearn-for-tf-idf"><span class="header-section-number">11.3.5</span> Example Python Code Using <code>sklearn</code> for TF-IDF</a></li>
  </ul></li>
  <li><a href="#comparison-of-dtm-and-tf-idf" id="toc-comparison-of-dtm-and-tf-idf" class="nav-link" data-scroll-target="#comparison-of-dtm-and-tf-idf"><span class="header-section-number">11.4</span> Comparison of DTM and TF-IDF</a></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion"><span class="header-section-number">11.5</span> Conclusion</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Text Representation Based on Counts</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<section id="what-is-a-vector" class="level3" data-number="11.0.1">
<h3 data-number="11.0.1" class="anchored" data-anchor-id="what-is-a-vector"><span class="header-section-number">11.0.1</span> What is a vector?</h3>
<p>A <strong>vector</strong> is explained in relation to scalars, matrices, and tensors. These concepts are important for understanding various mathematical and physical quantities.</p>
<ul>
<li>A <strong>scalar</strong> is a real number that represents the magnitude (e.g., size or weight) of an object.</li>
<li>A <strong>vector</strong> has both direction and magnitude. It is composed of multiple scalar values.</li>
<li>A <strong>matrix</strong> is a 2-dimensional array of numbers, which is essentially a collection of vectors organized in rows and columns.</li>
<li>A <strong>tensor</strong> is a higher-dimensional generalization of matrices, consisting of numbers arranged in more than two dimensions.</li>
</ul>
<p>Vectors and these structures are also applied in natural language processing to represent characteristics of words, phrases, and sentences.</p>
</section>
<section id="examples" class="level3" data-number="11.0.2">
<h3 data-number="11.0.2" class="anchored" data-anchor-id="examples"><span class="header-section-number">11.0.2</span> Examples:</h3>
<ul>
<li><p><strong>Scalar</strong>: A single number like <code>1</code></p></li>
<li><p><strong>Vector</strong>: A list of numbers like <span class="math inline">\(\begin{bmatrix} 1 \\ 2 \end{bmatrix}\)</span></p></li>
<li><p><strong>Matrix</strong>: A 2x2 array like <span class="math inline">\(\begin{bmatrix} 1 &amp; 2 \\ 3 &amp; 4 \end{bmatrix}\)</span></p></li>
<li><p><strong>Tensor</strong>: A 3-dimensional array like <span class="math inline">\(\begin{bmatrix} \begin{bmatrix} 1 &amp; 2 \\ 3 &amp; 4 \end{bmatrix} \\ \begin{bmatrix} 5 &amp; 6 \\ 7 &amp; 8 \end{bmatrix} \end{bmatrix}\)</span></p></li>
</ul>
</section>
<section id="embedding-how-do-machines-understand-natural-language" class="level3" data-number="11.0.3">
<h3 data-number="11.0.3" class="anchored" data-anchor-id="embedding-how-do-machines-understand-natural-language"><span class="header-section-number">11.0.3</span> Embedding: How do machines understand natural language?</h3>
<p>To understand how machines process human language, it is essential to grasp the concept of embedding. Computers fundamentally process data by switching electric signals on and off, meaning they work with binary states (on and off). Therefore, any form of input data must be converted into numbers for the computer to process it. Consequently, for a computer to understand natural language, the process of converting language into numerical data must happen first.</p>
<p>This is similar to how students learn a foreign language—by associating new words with their meaning in their native language through various methods such as context or dictionary lookups. However, for computers to understand human language, they must convert language elements like words and sentences into numbers, which correspond to the processing units. These units (words, sentences, etc.) are represented as vectors of numbers.</p>
<p>In other words, to enable computers to understand natural language, human language must be transformed into the computer’s language—numbers. The process of converting language into numbers for machine processing is known as <strong>embedding</strong>. When words are converted into vectors, it is called <strong>word embedding</strong>, and when sentences are converted into vectors, it is called <strong>sentence embedding</strong>.</p>
</section>
<section id="history-of-the-embedding-concept" class="level3" data-number="11.0.4">
<h3 data-number="11.0.4" class="anchored" data-anchor-id="history-of-the-embedding-concept"><span class="header-section-number">11.0.4</span> History of the Embedding Concept</h3>
<p>The idea of <strong>embedding</strong> originates from the concept of capturing the meaning of language units (words, sentences, etc.) based only on contextual information. This concept can be traced back to the structuralist linguists of the 1950s, including <strong>Zellig Harris</strong>, <strong>John Firth</strong>, and others. In the 1960s, <strong>Charles Osgood</strong> introduced the <strong>semantic differential method</strong>, which showed an early model of embedding by expressing psychological meanings of words in three-dimensional semantic space.</p>
<p><img src="images/clipboard-3032486285.png" class="img-fluid" width="471"></p>
<p>(Source:Lavrakas (2008). Encyclopedia of survey research methods. Sage.)</p>
<p>For example, as shown in the figure, the word “energy” is represented in 10 dimensions, with each axis representing different semantic traits. This method of representing words in multi-dimensional space laid the foundation for embedding.</p>
<p>In the 1990s, automatic methods were developed to extract <strong>features</strong> from text, using models like <strong>Latent Semantic Analysis (LSA)</strong> and <strong>Latent Dirichlet Allocation (LDA)</strong>. These were some of the key approaches to embedding that relied on statistical techniques.</p>
</section>
<section id="the-neural-network-era" class="level3" data-number="11.0.5">
<h3 data-number="11.0.5" class="anchored" data-anchor-id="the-neural-network-era"><span class="header-section-number">11.0.5</span> The Neural Network Era</h3>
<p>In 2003, <strong>Yoshua Bengio</strong> and other researchers introduced the <strong>neural probabilistic language model</strong>, marking the beginning of the use of neural networks in NLP. Since then, neural network-based embeddings have been widely used. Neural models are based on predicting which word is likely to come next by learning embeddings during the prediction process.</p>
<p>For instance, <strong>dense embeddings</strong> are used in deep learning-based natural language processing to convert input data into vector representations. This form of embedding is now common in deep learning frameworks.</p>
<p>The content in the image discusses different methods for representing vectors, particularly focusing on <strong>sparse representation</strong> and <strong>distributed representation</strong>. Below is the English translation and summary:</p>
<hr>
</section>
<section id="embedding-representation-methods" class="level2" data-number="11.1">
<h2 data-number="11.1" class="anchored" data-anchor-id="embedding-representation-methods"><span class="header-section-number">11.1</span> Embedding Representation Methods</h2>
<p>To understand embedding, it is necessary first to understand how vectors are represented and generated. Vector representation can broadly be classified into <strong>sparse representation</strong> and <strong>distributed representation</strong>.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/clipboard-4032977184.png" class="img-fluid figure-img" width="485"></p>
<figcaption>(Image Source: https://www.oreilly.com/content/how-neural-networks-learn-distributed-representations/)</figcaption>
</figure>
</div>
<p>To illustrate the concepts of sparse and distributed representations, we can use a simple shape example. In the above <strong>Figure</strong>, four shapes and their names are shown. The target shape’s name is represented in black, and the other shapes’ names are left blank. In this method, the number of shapes corresponds to the number of features (i.e., dimensions), and each feature is independent.</p>
<p>In sparse representation, features are encoded using 1s and 0s. The black mark is represented as 1, and the white marks as 0s. Most of the values are 0, with only the corresponding feature being marked as 1. This method is called <strong>sparse representation</strong> because only a few features are active (non-zero).</p>
<p>In the figure, the first shape is a horizontally long rectangle, and the second shape is a vertically long rectangle. These two shapes share no similarities in sparse representation. Each shape is represented independently, with no overlap in features.</p>
<hr>
<p>In contrast, <strong>distributed representation</strong> captures the similarity between shapes. In the following <strong>Figure</strong>, both the long horizontal and long vertical rectangles share a common feature, i.e., the fact that they are rectangles, but they differ in their specific directions (horizontal or vertical). This allows for more compact and meaningful vector representations by capturing shared properties between different objects.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/clipboard-1197708767.png" class="img-fluid figure-img" alt="https://www.oreilly.com/content/how-neural-networks-learn-distributed-representations/" width="437"></p>
<figcaption>(Image Soruce: https://www.oreilly.com/content/how-neural-networks-learn-distributed-representations/)</figcaption>
</figure>
</div>
<p>For instance, the first and second shapes share the property of being rectangles, while the third and fourth shapes share the property of being towers. In distributed representation, the shapes are described based on their common characteristics, rather than independent one-hot features as in sparse representation.</p>
<hr>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/clipboard-2879015786.png" class="img-fluid figure-img"></p>
<figcaption>(Image Source: https://wikidocs.net/31767)</figcaption>
</figure>
</div>
<section id="local-representation" class="level3" data-number="11.1.1">
<h3 data-number="11.1.1" class="anchored" data-anchor-id="local-representation"><span class="header-section-number">11.1.1</span> <strong>Local Representation:</strong></h3>
<ul>
<li><strong>One-hot Vector</strong>: A simple count-based method where each word is represented by a vector where only one position is marked as 1 (the word’s position in the vocabulary), while all other positions are 0.</li>
<li><strong>N-gram</strong>: A technique that considers sequences of words (or characters) of length N to capture local context in text.</li>
</ul>
</section>
<section id="continuous-representation" class="level3" data-number="11.1.2">
<h3 data-number="11.1.2" class="anchored" data-anchor-id="continuous-representation"><span class="header-section-number">11.1.2</span> <strong>Continuous Representation:</strong></h3>
<p>This is more advanced than local representation and is generally better at capturing the semantic relationships between words.</p>
<ul>
<li><strong>Count-Based Methods:</strong>
<ul>
<li><strong>Bag of Words (DTM)</strong>: A representation that counts word occurrences in documents without considering word order. This includes methods like <strong>document-term matrices (DTM)</strong>.</li>
<li><strong>LSA (Latent Semantic Analysis)</strong>: A method based on singular value decomposition of the document-term matrix, often used for capturing the global meaning of words across full documents.</li>
<li><strong>GloVe (Global Vectors for Word Representation)</strong>: A word representation that combines the benefits of global matrix factorization and local context window methods.</li>
</ul></li>
<li><strong>Prediction-Based Methods:</strong>
<ul>
<li><strong>Word2Vec (FastText)</strong>: A continuous bag-of-words (CBOW) and skip-gram model that predicts either a word from its surrounding context or the context from a given word. <strong>FastText</strong> is an extension of Word2Vec that also considers subword information.</li>
</ul></li>
</ul>
<hr>
</section>
<section id="sparse-representation-one-hot-encoding" class="level3" data-number="11.1.3">
<h3 data-number="11.1.3" class="anchored" data-anchor-id="sparse-representation-one-hot-encoding"><span class="header-section-number">11.1.3</span> <strong>Sparse Representation: One-Hot Encoding</strong></h3>
<p>One of the simplest forms of embedding is <strong>one-hot encoding</strong>, a type of sparse representation. In this method, each element (such as a word) in a collection (e.g., sentence, document) is represented as a vector where the target element is assigned a value of 1, and all other elements are assigned a value of 0.</p>
<p>For example, if we have the sentence <strong>“all is well”</strong> with the words “all,” “is,” and “well,” the one-hot encoding representation for these words would be:</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Word</th>
<th>all</th>
<th>is</th>
<th>well</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>all</strong></td>
<td>1</td>
<td>0</td>
<td>0</td>
</tr>
<tr class="even">
<td><strong>is</strong></td>
<td>0</td>
<td>1</td>
<td>0</td>
</tr>
<tr class="odd">
<td><strong>well</strong></td>
<td>0</td>
<td>0</td>
<td>1</td>
</tr>
</tbody>
</table>
<p><strong>[Figure] One-Hot Encoding Method of Embedding</strong></p>
<p>This table represents the <strong>one-hot encoding</strong> embedding of the words “all,” “is,” and “well,” where each word is assigned a unique vector. Only the position corresponding to the word has a value of 1, and all other positions are 0.</p>
<section id="advantages" class="level4" data-number="11.1.3.1">
<h4 data-number="11.1.3.1" class="anchored" data-anchor-id="advantages"><span class="header-section-number">11.1.3.1</span> Advantages:</h4>
<ul>
<li>It is easy to understand which words are being used, as the representation is simple.</li>
</ul>
</section>
<section id="disadvantages" class="level4" data-number="11.1.3.2">
<h4 data-number="11.1.3.2" class="anchored" data-anchor-id="disadvantages"><span class="header-section-number">11.1.3.2</span> Disadvantages:</h4>
<ul>
<li>As the number of words increases, the size of the vector grows, leading to high memory usage.</li>
<li>One-hot encoding does not capture any semantic similarity between words; for example, “king” and “queen” would be completely different.</li>
</ul>
<p>The <strong>document-term matrix (DTM)</strong> is an extension of one-hot encoding at the document level. The image also mentions the <strong>term frequency-inverse document frequency (TF-IDF)</strong>, which is another approach that assigns importance to words based on how often they appear across multiple documents.</p>
</section>
</section>
<section id="distributed-representation" class="level3" data-number="11.1.4">
<h3 data-number="11.1.4" class="anchored" data-anchor-id="distributed-representation"><span class="header-section-number">11.1.4</span> <strong>Distributed Representation</strong></h3>
<p>The second major embedding type described is <strong>distributed representation</strong>, where words are represented as vectors with real-numbered values. Unlike one-hot encoding, distributed representation captures semantic relationships between words. For example, the word <strong>“all”</strong> could be represented by a vector like [-0.0031, -0.0421, 0.0125, 0.0062].</p>
<p>Here is the translation of the table from the image to English:</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Word</th>
<th>Dimension 1</th>
<th>Dimension 2</th>
<th>Dimension 3</th>
<th>Dimension 4</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>all</strong></td>
<td>-0.0031</td>
<td>-0.0421</td>
<td>0.0125</td>
<td>0.0062</td>
</tr>
<tr class="even">
<td><strong>is</strong></td>
<td>0.0212</td>
<td>0.0125</td>
<td>-0.0089</td>
<td>0.0376</td>
</tr>
<tr class="odd">
<td><strong>well</strong></td>
<td>-0.0543</td>
<td>0.0684</td>
<td>-0.0023</td>
<td>0.0181</td>
</tr>
</tbody>
</table>
<p><strong>[Figure] Distributed Representation Method of Embedding</strong></p>
<p>This table shows an example of distributed embeddings for the words “all,” “is,” and “well,” where each word is represented as a vector in a 4-dimensional space. Each dimension has a real number value, capturing more information about the relationships between words compared to one-hot encoding.</p>
<section id="advantages-1" class="level4" data-number="11.1.4.1">
<h4 data-number="11.1.4.1" class="anchored" data-anchor-id="advantages-1"><span class="header-section-number">11.1.4.1</span> Advantages:</h4>
<ul>
<li>Distributed representations can capture the semantic meaning of words.</li>
<li>The dimensionality of vectors does not increase as more words are added.</li>
</ul>
<p>This method is widely used in natural language processing today because it can efficiently capture word meanings and relationships. Techniques like <strong>Word2Vec</strong> calculate these word embeddings. Two popular models for Word2Vec are <strong>CBOW (Continuous Bag of Words Model)</strong> and <strong>Skip-Gram</strong> (see Chapter 12. Word Embedding and Similarity)</p>
</section>
</section>
</section>
<section id="examples-of-text-representation-methods" class="level2" data-number="11.2">
<h2 data-number="11.2" class="anchored" data-anchor-id="examples-of-text-representation-methods"><span class="header-section-number">11.2</span> Examples of Text Representation Methods</h2>
<section id="document-term-matrix-dtm" class="level3" data-number="11.2.1">
<h3 data-number="11.2.1" class="anchored" data-anchor-id="document-term-matrix-dtm"><span class="header-section-number">11.2.1</span> Document-Term Matrix (DTM)</h3>
</section>
<section id="concept-of-document-term-matrix" class="level3" data-number="11.2.2">
<h3 data-number="11.2.2" class="anchored" data-anchor-id="concept-of-document-term-matrix"><span class="header-section-number">11.2.2</span> Concept of Document-Term Matrix</h3>
<p>A <strong>Document-Term Matrix (DTM)</strong> is a matrix that represents the frequency of terms occurring in a set of documents. This matrix is widely used in natural language processing (NLP) for tasks such as text mining, information retrieval, and topic modeling.</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Example Document-Term Matrix (DTM) data</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> {</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>    <span class="st">'word1'</span>: [<span class="dv">4</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">3</span>, <span class="dv">0</span>, <span class="dv">1</span>],</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>    <span class="st">'word2'</span>: [<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">5</span>, <span class="dv">0</span>, <span class="dv">2</span>, <span class="dv">0</span>],</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>    <span class="st">'word3'</span>: [<span class="dv">0</span>, <span class="dv">7</span>, <span class="dv">0</span>, <span class="dv">8</span>, <span class="dv">1</span>, <span class="dv">4</span>],</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>    <span class="st">'word4'</span>: [<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">3</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>],</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>    <span class="st">'word5'</span>: [<span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">4</span>],</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>    <span class="st">'word6'</span>: [<span class="dv">2</span>, <span class="dv">1</span>, <span class="dv">9</span>, <span class="dv">0</span>, <span class="dv">12</span>, <span class="dv">0</span>],</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>    <span class="st">'wordN'</span>: [<span class="dv">0</span>, <span class="dv">3</span>, <span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">4</span>, <span class="dv">2</span>],</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Creating the Document-Term Matrix DataFrame</span></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>dtm_df <span class="op">=</span> pd.DataFrame(data, index<span class="op">=</span>[<span class="ss">f'doc</span><span class="sc">{</span>i<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">'</span> <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">6</span>)])</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>dtm_df</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><img src="images/clipboard-3316802906.png" class="img-fluid"></p>
</section>
<section id="understanding-the-components" class="level3" data-number="11.2.3">
<h3 data-number="11.2.3" class="anchored" data-anchor-id="understanding-the-components"><span class="header-section-number">11.2.3</span> Understanding the Components:</h3>
<ul>
<li><strong>Rows</strong> (doc1, doc, … ,doc6): Each row represents a document in the corpus.</li>
<li><strong>Columns</strong> (word1, word2, … , wordN): Each column represents a unique word (term) in the corpus.</li>
<li><strong>Values</strong>: The numbers in the matrix represent the frequency of each term in the corresponding document. For instance, in document ( doc1 ), term ( word1 ) appears 4 times, while term ( word1 ) appears once.</li>
</ul>
<p>Here is an example of how to create a Document-Term Matrix using Python.</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Import necessary libraries</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.feature_extraction.text <span class="im">import</span> CountVectorizer</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Define sample documents</span></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>documents <span class="op">=</span> [ </span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Sir Walter has resented it.  As the head of the house, he felt that he ought to have been consulted, especially after taking the young man so publicly by the hand"</span>,</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Of Man's first disobedience, and the fruit Of that forbidden tree whose mortal taste Brought death into the World, and all our woe,  With loss of Eden, till one greater Man  Restore us, and regain the blissful seat, "</span>,</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Come, said my soul, Such verses for my Body let us write, (for we are one,) That should I after return, Or, long, long hence, in other spheres, There to some group of mates the chants resuming, (Tallying Earth's soil, trees, winds, tumultuous waves,) Ever with pleas'd smile I may keep on, Ever and ever yet the verses owning--as, first, I here and now Signing for Soul and Body, set to them my name,"</span></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize CountVectorizer to count word frequencies</span></span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>vectorizer <span class="op">=</span> CountVectorizer()</span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit and transform the documents to generate a word count vector</span></span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> vectorizer.fit_transform(documents)</span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Get the list of words</span></span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a>words <span class="op">=</span> vectorizer.get_feature_names_out()</span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert the result to a DataFrame for better visualization</span></span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a>df_bow <span class="op">=</span> pd.DataFrame(X.toarray(), columns<span class="op">=</span>words)</span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a><span class="co"># Display the result</span></span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a>df_bow</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>This code creates a Document-Term Matrix from three sample documents. The matrix counts the occurrences of each word in the documents and presents it in the following format:</p>
<p><img src="images/clipboard-1940150482.png" class="img-fluid"></p>
<p>This structure allows for further analysis such as identifying important words in a document or across documents, performing clustering, or calculating term importance metrics like <strong>TF-IDF</strong> (Term Frequency-Inverse Document Frequency).</p>
</section>
</section>
<section id="tf-idf-term-frequency-inverse-document-frequency" class="level2" data-number="11.3">
<h2 data-number="11.3" class="anchored" data-anchor-id="tf-idf-term-frequency-inverse-document-frequency"><span class="header-section-number">11.3</span> TF-IDF (Term Frequency-Inverse Document Frequency)</h2>
<section id="concept-of-tf-idf" class="level3" data-number="11.3.1">
<h3 data-number="11.3.1" class="anchored" data-anchor-id="concept-of-tf-idf"><span class="header-section-number">11.3.1</span> Concept of TF-IDF</h3>
<p>TF-IDF is a method used to evaluate the importance of a word in a document relative to a collection of documents. <strong>TF (Term Frequency)</strong> measures how frequently a term appears in a document, while <strong>IDF (Inverse Document Frequency)</strong> quantifies how rare the term is across all documents. A higher TF-IDF score indicates that the term is important in the document but appears infrequently in other documents.</p>
</section>
<section id="term-frequency-tf" class="level3" data-number="11.3.2">
<h3 data-number="11.3.2" class="anchored" data-anchor-id="term-frequency-tf"><span class="header-section-number">11.3.2</span> <strong>Term Frequency (TF)</strong></h3>
<p>The <strong>TF</strong> of a term measures how frequently a term occurs in a document. It is calculated as:</p>
<p><span class="math display">\[TF(t, d) = \frac{\text{Number of times term } t \text{ appears in document } d}{\text{Total number of terms in document } d}\]</span></p>
</section>
<section id="inverse-document-frequency-idf" class="level3" data-number="11.3.3">
<h3 data-number="11.3.3" class="anchored" data-anchor-id="inverse-document-frequency-idf"><span class="header-section-number">11.3.3</span> <strong>Inverse Document Frequency (IDF)</strong></h3>
<p>The <strong>IDF</strong> of a term measures how important a term is in the whole corpus. Words that are common across many documents (e.g., “the”, “is”, etc.) receive a low score. The formula is:</p>
<p><span class="math display">\[
IDF(t) = \log\left(\frac{N}{1 + \text{Number of documents containing term } t}\right)
\]</span> Where:</p>
<ul>
<li><p><em>N</em> is the total number of documents.</p></li>
<li><p>The “+1” in the denominator is added to prevent division by zero.</p></li>
</ul>
</section>
<section id="tf-idf-score" class="level3" data-number="11.3.4">
<h3 data-number="11.3.4" class="anchored" data-anchor-id="tf-idf-score"><span class="header-section-number">11.3.4</span> <strong>TF-IDF Score</strong></h3>
<p>The TF-IDF score is computed by multiplying TF and IDF:</p>
<p><span class="math display">\[ TF\text{-}IDF(t, d) = TF(t, d) \times IDF(t) \]</span></p>
</section>
<section id="example-python-code-using-sklearn-for-tf-idf" class="level3" data-number="11.3.5">
<h3 data-number="11.3.5" class="anchored" data-anchor-id="example-python-code-using-sklearn-for-tf-idf"><span class="header-section-number">11.3.5</span> Example Python Code Using <code>sklearn</code> for TF-IDF</h3>
<p>Let’s calculate the TF-IDF scores for a small example corpus using Python.</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.feature_extraction.text <span class="im">import</span> TfidfVectorizer</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Sample corpus of documents</span></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>documents <span class="op">=</span> [</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>    <span class="st">"The cat sat on the mat"</span>,</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>    <span class="st">"The dog sat on the log"</span>,</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Cats and dogs are pets"</span></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the TF-IDF vectorizer</span></span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>vectorizer <span class="op">=</span> TfidfVectorizer()</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit the vectorizer on the sample documents</span></span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>tfidf_matrix <span class="op">=</span> vectorizer.fit_transform(documents)</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Get the feature names (terms)</span></span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a>terms <span class="op">=</span> vectorizer.get_feature_names_out()</span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert the TF-IDF matrix into a Pandas DataFrame for better readability</span></span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a>tfidf_df <span class="op">=</span> pd.DataFrame(tfidf_matrix.toarray(), columns<span class="op">=</span>terms)</span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-23"><a href="#cb3-23" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(tfidf_df)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><img src="images/clipboard-1421312929.png" class="img-fluid"></p>
<p>This approach helps measure the relative importance of words in the documents and can be used for various NLP tasks like text classification, document clustering, etc.</p>
</section>
</section>
<section id="comparison-of-dtm-and-tf-idf" class="level2" data-number="11.4">
<h2 data-number="11.4" class="anchored" data-anchor-id="comparison-of-dtm-and-tf-idf"><span class="header-section-number">11.4</span> Comparison of DTM and TF-IDF</h2>
<ul>
<li><strong>DTM</strong> simply counts how many times each word appears in the document, without considering how common the word is in the entire document collection.</li>
<li><strong>TF-IDF</strong> combines frequency information with the rarity of the word across all documents, thus highlighting words that are important in one document but uncommon in the overall corpus.</li>
</ul>
<p>Both methods are useful for tasks like calculating document similarity, extracting keywords, and analyzing the importance of specific terms in a corpus.</p>
</section>
<section id="conclusion" class="level2" data-number="11.5">
<h2 data-number="11.5" class="anchored" data-anchor-id="conclusion"><span class="header-section-number">11.5</span> Conclusion</h2>
<p>Count-based word representation methods like DTM and TF-IDF are fundamental tools for converting text into numerical formats, making it easier to apply statistical approaches to natural language data. In this chapter, we explored the concepts and Python implementations of both methods, providing a foundation for more complex NLP tasks.</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./10.Word_Cloud.html" class="pagination-link" aria-label="Visualizing Word Frequencies with Graphs and Word Clouds">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Visualizing Word Frequencies with Graphs and Word Clouds</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./12.Word_Embedding.html" class="pagination-link" aria-label="Word Embedding and Relational Similarity">
        <span class="nav-page-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Word Embedding and Relational Similarity</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>