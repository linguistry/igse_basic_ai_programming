<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.57">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>16&nbsp; Topic Modeling in Python – Basic AI Programming</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./17.Topic_Modeling_Example.html" rel="next">
<link href="./15.SNA_Example.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./16.Topic_Modeling.html"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Topic Modeling in Python</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Basic AI Programming</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./01.setup.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Setting Up Environment</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./02.intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Constants and Variables in Python</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./03.control.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Control Structures in Python</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./04.function.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Functions and Packages in Python</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./05.pandas.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Data Analysis in Python</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./06.arithmetic.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Basic Arithmetic Operations in Python</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./07.Regular_Expression.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Regular Expressions (RegEx) in Python</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./08.Custom_Corpus.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Building a Custom Corpus</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./09.Preprocessing.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Text Preprocessing</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./10.Word_Cloud.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Visualizing Word Frequencies with Graphs and Word Clouds</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./11.Text_Representation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Text Representation Based on Counts</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./12.Word_Embedding.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Word Embedding and Relational Similarity</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./13.NLP_toolkit.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Word Embedding Activities</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./14.Semantic_Network.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Semantic Network Analysis</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./15.SNA_Example.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Semantic Network Analysis (Examples)</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./16.Topic_Modeling.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Topic Modeling in Python</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./17.Topic_Modeling_Example.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Topic Modeling with R (Example)</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./19.Document_Classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Document Classification</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./A.assignment1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">Assignment #01</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./B.assignment2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">Assignment #02: Semantic Network Analysis</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C.assignment3_topic.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">21</span>&nbsp; <span class="chapter-title">Assignment #03:Topic Modeling</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#topic-modeling" id="toc-topic-modeling" class="nav-link active" data-scroll-target="#topic-modeling"><span class="header-section-number">16.1</span> Topic Modeling</a></li>
  <li><a href="#background" id="toc-background" class="nav-link" data-scroll-target="#background"><span class="header-section-number">16.2</span> Background</a>
  <ul class="collapse">
  <li><a href="#key-concepts-of-topic-modeling" id="toc-key-concepts-of-topic-modeling" class="nav-link" data-scroll-target="#key-concepts-of-topic-modeling"><span class="header-section-number">16.2.1</span> Key Concepts of Topic Modeling</a></li>
  <li><a href="#how-topic-modeling-works" id="toc-how-topic-modeling-works" class="nav-link" data-scroll-target="#how-topic-modeling-works"><span class="header-section-number">16.2.2</span> How Topic Modeling Works</a></li>
  <li><a href="#common-algorithms" id="toc-common-algorithms" class="nav-link" data-scroll-target="#common-algorithms"><span class="header-section-number">16.2.3</span> Common Algorithms</a></li>
  <li><a href="#applications-of-topic-modeling" id="toc-applications-of-topic-modeling" class="nav-link" data-scroll-target="#applications-of-topic-modeling"><span class="header-section-number">16.2.4</span> Applications of Topic Modeling</a></li>
  <li><a href="#advantages" id="toc-advantages" class="nav-link" data-scroll-target="#advantages"><span class="header-section-number">16.2.5</span> Advantages</a></li>
  <li><a href="#challenges" id="toc-challenges" class="nav-link" data-scroll-target="#challenges"><span class="header-section-number">16.2.6</span> Challenges</a></li>
  <li><a href="#tools-and-libraries" id="toc-tools-and-libraries" class="nav-link" data-scroll-target="#tools-and-libraries"><span class="header-section-number">16.2.7</span> Tools and Libraries</a></li>
  <li><a href="#example-use-case" id="toc-example-use-case" class="nav-link" data-scroll-target="#example-use-case"><span class="header-section-number">16.2.8</span> Example Use Case</a></li>
  </ul></li>
  <li><a href="#python-code" id="toc-python-code" class="nav-link" data-scroll-target="#python-code"><span class="header-section-number">16.3</span> Python Code</a>
  <ul class="collapse">
  <li><a href="#data-preprocessing-techniques" id="toc-data-preprocessing-techniques" class="nav-link" data-scroll-target="#data-preprocessing-techniques"><span class="header-section-number">16.3.1</span> Data Preprocessing Techniques</a></li>
  <li><a href="#introduction-to-latent-dirichlet-allocation-lda" id="toc-introduction-to-latent-dirichlet-allocation-lda" class="nav-link" data-scroll-target="#introduction-to-latent-dirichlet-allocation-lda"><span class="header-section-number">16.3.2</span> Introduction to Latent Dirichlet Allocation (LDA)</a></li>
  <li><a href="#building-bigram-models-for-text-analysis" id="toc-building-bigram-models-for-text-analysis" class="nav-link" data-scroll-target="#building-bigram-models-for-text-analysis"><span class="header-section-number">16.3.3</span> Building Bigram Models for Text Analysis</a></li>
  <li><a href="#defining-custom-functions" id="toc-defining-custom-functions" class="nav-link" data-scroll-target="#defining-custom-functions"><span class="header-section-number">16.3.4</span> Defining Custom Functions</a></li>
  <li><a href="#step-by-step-preprocessing-workflow" id="toc-step-by-step-preprocessing-workflow" class="nav-link" data-scroll-target="#step-by-step-preprocessing-workflow"><span class="header-section-number">16.3.5</span> Step-by-Step Preprocessing Workflow</a></li>
  <li><a href="#transforming-data-into-corpus-and-dictionary-formats" id="toc-transforming-data-into-corpus-and-dictionary-formats" class="nav-link" data-scroll-target="#transforming-data-into-corpus-and-dictionary-formats"><span class="header-section-number">16.3.6</span> Transforming Data into Corpus and Dictionary Formats</a></li>
  <li><a href="#creating-a-corpus" id="toc-creating-a-corpus" class="nav-link" data-scroll-target="#creating-a-corpus"><span class="header-section-number">16.3.7</span> Creating a Corpus</a></li>
  <li><a href="#building-the-base-topic-model-with-lda" id="toc-building-the-base-topic-model-with-lda" class="nav-link" data-scroll-target="#building-the-base-topic-model-with-lda"><span class="header-section-number">16.3.8</span> Building the Base Topic Model with LDA</a></li>
  <li><a href="#building-the-lda-model" id="toc-building-the-lda-model" class="nav-link" data-scroll-target="#building-the-lda-model"><span class="header-section-number">16.3.9</span> Building the LDA Model</a></li>
  <li><a href="#evaluating-the-lda-model-with-perplexity-and-coherence" id="toc-evaluating-the-lda-model-with-perplexity-and-coherence" class="nav-link" data-scroll-target="#evaluating-the-lda-model-with-perplexity-and-coherence"><span class="header-section-number">16.3.10</span> Evaluating the LDA Model with Perplexity and Coherence</a></li>
  <li><a href="#computing-coherence-score" id="toc-computing-coherence-score" class="nav-link" data-scroll-target="#computing-coherence-score"><span class="header-section-number">16.3.11</span> Computing Coherence Score</a></li>
  <li><a href="#visualizing-topics-and-keywords" id="toc-visualizing-topics-and-keywords" class="nav-link" data-scroll-target="#visualizing-topics-and-keywords"><span class="header-section-number">16.3.12</span> Visualizing Topics and Keywords</a></li>
  <li><a href="#document-topic-distribution" id="toc-document-topic-distribution" class="nav-link" data-scroll-target="#document-topic-distribution"><span class="header-section-number">16.3.13</span> Document-Topic Distribution</a></li>
  </ul></li>
  <li><a href="#python-code-that-calculates-perplexity-and-coherence-scores-for-different-topic-numbers" id="toc-python-code-that-calculates-perplexity-and-coherence-scores-for-different-topic-numbers" class="nav-link" data-scroll-target="#python-code-that-calculates-perplexity-and-coherence-scores-for-different-topic-numbers"><span class="header-section-number">16.4</span> Python code that calculates perplexity and coherence scores for different topic numbers</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Topic Modeling in Python</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<section id="topic-modeling" class="level2" data-number="16.1">
<h2 data-number="16.1" class="anchored" data-anchor-id="topic-modeling"><span class="header-section-number">16.1</span> Topic Modeling</h2>
<p><strong>Topic modeling</strong> is a type of statistical modeling used in natural language processing (NLP) and text mining to discover abstract topics that occur within a collection of documents. It helps in organizing, understanding, and summarizing large sets of textual data by identifying patterns of word usage and grouping related words into topics.</p>
</section>
<section id="background" class="level2" data-number="16.2">
<h2 data-number="16.2" class="anchored" data-anchor-id="background"><span class="header-section-number">16.2</span> Background</h2>
<section id="key-concepts-of-topic-modeling" class="level3" data-number="16.2.1">
<h3 data-number="16.2.1" class="anchored" data-anchor-id="key-concepts-of-topic-modeling"><span class="header-section-number">16.2.1</span> Key Concepts of Topic Modeling</h3>
<ol type="1">
<li><strong>Documents and Corpus</strong>:
<ul>
<li><strong>Document</strong>: A single piece of text (e.g., an article, a tweet, a book chapter).</li>
<li><strong>Corpus</strong>: A collection of documents.</li>
</ul></li>
<li><strong>Topics</strong>:
<ul>
<li>A topic is characterized by a distribution of words that frequently occur together. For example, a “sports” topic might include words like “game,” “team,” “score,” and “player.”</li>
</ul></li>
<li><strong>Word Distribution</strong>:
<ul>
<li>Each topic is represented by a probability distribution over a fixed vocabulary of words.</li>
<li>Each document is represented by a mixture of topics, indicating the presence and proportion of each topic within the document.</li>
</ul></li>
</ol>
</section>
<section id="how-topic-modeling-works" class="level3" data-number="16.2.2">
<h3 data-number="16.2.2" class="anchored" data-anchor-id="how-topic-modeling-works"><span class="header-section-number">16.2.2</span> How Topic Modeling Works</h3>
<ol type="1">
<li><strong>Preprocessing</strong>:
<ul>
<li><strong>Tokenization</strong>: Breaking text into words or tokens.</li>
<li><strong>Stop-word Removal</strong>: Eliminating common words that carry little meaning (e.g., “the,” “and”).</li>
<li><strong>Stemming/Lemmatization</strong>: Reducing words to their base or root form.</li>
<li><strong>Vectorization</strong>: Converting text into numerical representations, such as term frequency-inverse document frequency (TF-IDF).</li>
</ul></li>
<li><strong>Algorithm Application</strong>:
<ul>
<li><strong>Latent Dirichlet Allocation (LDA)</strong>: The most popular topic modeling technique. It assumes that documents are a mixture of topics and that each topic is a mixture of words.</li>
<li><strong>Non-negative Matrix Factorization (NMF)</strong>: Decomposes the document-term matrix into two lower-dimensional matrices, representing topics and their association with words.</li>
<li><strong>Latent Semantic Analysis (LSA)</strong>: Uses singular value decomposition to identify patterns in the relationships between terms and documents.</li>
</ul></li>
<li><strong>Parameter Estimation</strong>:
<ul>
<li>Determining the number of topics.</li>
<li>Estimating the probability distributions for topics and words using statistical methods.</li>
</ul></li>
<li><strong>Interpretation</strong>:
<ul>
<li>Analyzing the top words in each topic to assign meaningful labels.</li>
<li>Understanding the distribution of topics across the corpus to gain insights.</li>
</ul></li>
</ol>
</section>
<section id="common-algorithms" class="level3" data-number="16.2.3">
<h3 data-number="16.2.3" class="anchored" data-anchor-id="common-algorithms"><span class="header-section-number">16.2.3</span> Common Algorithms</h3>
<ol type="1">
<li><strong>Latent Dirichlet Allocation (LDA)</strong>:
<ul>
<li>A generative probabilistic model.</li>
<li>Assumes documents are generated from a mixture of topics, each of which is a distribution over words.</li>
<li>Widely used due to its effectiveness and interpretability.</li>
</ul></li>
<li><strong>Non-negative Matrix Factorization (NMF)</strong>:
<ul>
<li>Decomposes the document-term matrix into non-negative factors.</li>
<li>Useful for extracting additive, parts-based representations of data.</li>
</ul></li>
<li><strong>Latent Semantic Analysis (LSA)</strong>:
<ul>
<li>Focuses on capturing the underlying structure in the data by reducing dimensionality.</li>
<li>Less probabilistic compared to LDA and NMF.</li>
</ul></li>
</ol>
</section>
<section id="applications-of-topic-modeling" class="level3" data-number="16.2.4">
<h3 data-number="16.2.4" class="anchored" data-anchor-id="applications-of-topic-modeling"><span class="header-section-number">16.2.4</span> Applications of Topic Modeling</h3>
<ul>
<li><strong>Information Retrieval</strong>: Enhancing search engines by identifying relevant topics.</li>
<li><strong>Document Classification</strong>: Organizing and categorizing large sets of documents.</li>
<li><strong>Trend Analysis</strong>: Monitoring changes in topics over time.</li>
<li><strong>Recommendation Systems</strong>: Suggesting content based on topic similarity.</li>
<li><strong>Content Summarization</strong>: Generating summaries by highlighting key topics.</li>
<li><strong>Social Media Analysis</strong>: Understanding prevalent themes and discussions.</li>
</ul>
</section>
<section id="advantages" class="level3" data-number="16.2.5">
<h3 data-number="16.2.5" class="anchored" data-anchor-id="advantages"><span class="header-section-number">16.2.5</span> Advantages</h3>
<ul>
<li><strong>Scalability</strong>: Can handle large volumes of text data.</li>
<li><strong>Unsupervised Learning</strong>: Does not require labeled data.</li>
<li><strong>Discovering Hidden Structures</strong>: Reveals underlying themes that may not be immediately apparent.</li>
</ul>
</section>
<section id="challenges" class="level3" data-number="16.2.6">
<h3 data-number="16.2.6" class="anchored" data-anchor-id="challenges"><span class="header-section-number">16.2.6</span> Challenges</h3>
<ul>
<li><strong>Choosing the Number of Topics</strong>: Selecting the optimal number of topics can be subjective and may require experimentation.</li>
<li><strong>Interpretability</strong>: Topics may sometimes include ambiguous or overlapping words, making them hard to label clearly.</li>
<li><strong>Quality of Preprocessing</strong>: Effective preprocessing is crucial for meaningful topic extraction.</li>
<li><strong>Scalability with Very Large Corpora</strong>: While scalable, extremely large datasets may require significant computational resources.</li>
</ul>
</section>
<section id="tools-and-libraries" class="level3" data-number="16.2.7">
<h3 data-number="16.2.7" class="anchored" data-anchor-id="tools-and-libraries"><span class="header-section-number">16.2.7</span> Tools and Libraries</h3>
<ul>
<li><strong>Gensim</strong>: A Python library for topic modeling, particularly known for its efficient implementation of LDA.</li>
<li><strong>Scikit-learn</strong>: Offers implementations of NMF and LDA for topic modeling.</li>
<li><strong>MALLET</strong>: A Java-based package that provides efficient LDA and other topic modeling tools.</li>
<li><strong>Topic Modeling Toolkits in R</strong>: Such as <code>topicmodels</code> and <code>stm</code> packages.</li>
</ul>
</section>
<section id="example-use-case" class="level3" data-number="16.2.8">
<h3 data-number="16.2.8" class="anchored" data-anchor-id="example-use-case"><span class="header-section-number">16.2.8</span> Example Use Case</h3>
<p>Suppose a company has thousands of customer reviews about its products. Topic modeling can help identify common themes, such as “product quality,” “customer service,” “pricing,” and “usability.” By understanding these topics, the company can make informed decisions to improve its offerings and address customer concerns more effectively.</p>
</section>
</section>
<section id="python-code" class="level2" data-number="16.3">
<h2 data-number="16.3" class="anchored" data-anchor-id="python-code"><span class="header-section-number">16.3</span> Python Code</h2>
<div class="sourceCode" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Importing essential libraries for data analysis</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd  <span class="co"># pandas is used for data manipulation and analysis</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np   <span class="co"># numpy is used for numerical operations and handling arrays</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Reading data from a CSV file</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> pd.read_csv(<span class="st">'data_topicmodeling.csv'</span>, encoding<span class="op">=</span><span class="st">'utf-8'</span>)</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>data.head()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><img src="images/clipboard-2338318362.png" class="img-fluid"></p>
<ul>
<li><strong><code>pd.read_csv()</code></strong>: This function reads a CSV file and loads it into a <code>DataFrame</code> object, which can then be manipulated using various <code>pandas</code> methods.</li>
<li><strong><code>encoding='utf-8'</code></strong>: Specifies the character encoding to handle different types of text correctly. UTF-8 is widely used because it supports a broad range of characters.</li>
</ul>
<section id="data-preprocessing-techniques" class="level3" data-number="16.3.1">
<h3 data-number="16.3.1" class="anchored" data-anchor-id="data-preprocessing-techniques"><span class="header-section-number">16.3.1</span> Data Preprocessing Techniques</h3>
<p><em>Replacing Empty Cells with <code>NaN</code></em></p>
<p>In many datasets, there are empty cells that need to be identified and treated appropriately. An empty cell can be represented by an empty string (<code>''</code>). The code below shows how to replace these empty cells with <code>NaN</code> (Not a Number), which <code>pandas</code> recognizes as a missing value.</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Replace empty cells in the 'abstract' column with NaN</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>data[<span class="st">'abstract'</span>] <span class="op">=</span> data[<span class="st">'abstract'</span>].replace(<span class="st">''</span>, np.nan)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><em>Dropping Rows with NaN Values</em></p>
<div class="sourceCode" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Drop rows where 'abstract' column has NaN values</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>data.dropna(subset<span class="op">=</span>[<span class="st">'abstract'</span>], inplace<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><em>Applying Lowercase Transformation</em></p>
<div class="sourceCode" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert the text in the 'abstract' column to lowercase</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>data[<span class="st">'lower_abstract'</span>] <span class="op">=</span> data[<span class="st">'abstract'</span>].<span class="bu">apply</span>(<span class="kw">lambda</span> x: x.lower())</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>data.head()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><img src="images/clipboard-81693117.png" class="img-fluid"></p>
</section>
<section id="introduction-to-latent-dirichlet-allocation-lda" class="level3" data-number="16.3.2">
<h3 data-number="16.3.2" class="anchored" data-anchor-id="introduction-to-latent-dirichlet-allocation-lda"><span class="header-section-number">16.3.2</span> Introduction to Latent Dirichlet Allocation (LDA)</h3>
<p>Latent Dirichlet Allocation (LDA) is a popular algorithm used for topic modeling, which uncovers hidden thematic structures in a corpus of text. It works by assuming that each document is a mixture of various topics and that each topic is a mixture of words. Before applying LDA, it is essential to preprocess the text data, including tokenization.</p>
<section id="tokenization-using-gensim" class="level4" data-number="16.3.2.1">
<h4 data-number="16.3.2.1" class="anchored" data-anchor-id="tokenization-using-gensim"><span class="header-section-number">16.3.2.1</span> Tokenization Using <code>gensim</code></h4>
</section>
<section id="what-is-tokenization" class="level4" data-number="16.3.2.2">
<h4 data-number="16.3.2.2" class="anchored" data-anchor-id="what-is-tokenization"><span class="header-section-number">16.3.2.2</span> What is Tokenization?</h4>
<p>Tokenization is the process of breaking down text into smaller units, known as tokens. These tokens can be words, phrases, or even characters. For LDA, tokenization helps convert text into a format that can be analyzed by the algorithm.</p>
</section>
<section id="using-gensim-for-tokenization" class="level4" data-number="16.3.2.3">
<h4 data-number="16.3.2.3" class="anchored" data-anchor-id="using-gensim-for-tokenization"><span class="header-section-number">16.3.2.3</span> Using <code>gensim</code> for Tokenization</h4>
<p><code>gensim</code> is a robust Python library for topic modeling and natural language processing. The <code>gensim.utils.simple_preprocess</code> function provides an effective way to tokenize text.</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Importing the gensim library and simple_preprocess function</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> gensim</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> gensim.utils <span class="im">import</span> simple_preprocess</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Function to tokenize a list of sentences</span></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> sent_to_words(sentences):</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> sentence <span class="kw">in</span> sentences:</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>        <span class="cf">yield</span>(gensim.utils.simple_preprocess(sentence, deacc<span class="op">=</span><span class="va">True</span>))  <span class="co"># Tokenizes and removes punctuations with deacc=True</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="inspecting-the-preprocessed-text" class="level4" data-number="16.3.2.4">
<h4 data-number="16.3.2.4" class="anchored" data-anchor-id="inspecting-the-preprocessed-text"><span class="header-section-number">16.3.2.4</span> Inspecting the Preprocessed Text</h4>
<p>Before proceeding with tokenization, we review the current state of the preprocessed text data, stored in a column called <code>lower_abstract</code>:</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Extracting the 'lower_abstract' column</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>lower_abstract <span class="op">=</span> data[<span class="st">'lower_abstract'</span>]</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>lower_abstract.head()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<ul>
<li><strong><code>lower_abstract.head()</code></strong>: Displays the first few entries of the <code>lower_abstract</code> column, showing the lowercase version of the <code>abstract</code> text data.</li>
</ul>
<div class="sourceCode" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Applying the tokenization function to the 'lower_abstract' column</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>data[<span class="st">'words'</span>] <span class="op">=</span> <span class="bu">list</span>(sent_to_words(lower_abstract))</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>data.head()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<ul>
<li><strong><code>data['words']</code></strong>: Contains lists of tokens for each <code>abstract</code>, effectively converting text data into a list of words that can be used for LDA.</li>
</ul>
<div class="sourceCode" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Creating a list of word tokens from the 'lower_abstract' column</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>data_words <span class="op">=</span> <span class="bu">list</span>(sent_to_words(lower_abstract))</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Displaying the first two tokenized entries</span></span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>data_words[:<span class="dv">2</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><strong>Expected Output:</strong></p>
<pre><code>[['word1', 'word2', 'word3', ...], ['word1', 'word2', 'word3', ...]]</code></pre>
</section>
<section id="removing-stopwords-for-effective-text-analysis" class="level4" data-number="16.3.2.5">
<h4 data-number="16.3.2.5" class="anchored" data-anchor-id="removing-stopwords-for-effective-text-analysis"><span class="header-section-number">16.3.2.5</span> Removing Stopwords for Effective Text Analysis</h4>
<p><strong>Stopwords</strong> are common words in a language (e.g., “is”, “the”, “and”) that often carry little meaningful information in the context of text analysis. Removing these words helps improve the efficiency of text processing and enhances the quality of insights drawn from tasks such as topic modeling.</p>
<section id="importing-and-downloading-stopwords-with-nltk" class="level5" data-number="16.3.2.5.1">
<h5 data-number="16.3.2.5.1" class="anchored" data-anchor-id="importing-and-downloading-stopwords-with-nltk"><span class="header-section-number">16.3.2.5.1</span> Importing and Downloading Stopwords with <code>nltk</code></h5>
<div class="sourceCode" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Importing the nltk library and downloading the stopwords package</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> nltk</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>nltk.download(<span class="st">'stopwords'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<ul>
<li><strong><code>nltk.download('stopwords')</code></strong>: Downloads the stopwords dataset if it is not already present in your environment.</li>
</ul>
<div class="sourceCode" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> nltk.corpus <span class="im">import</span> stopwords</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Creating a custom set of stopwords and adding more words</span></span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>stop_words_final <span class="op">=</span> <span class="bu">set</span>(stopwords.words(<span class="st">"english"</span>))</span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(stop_words_final)</span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a>additional_words <span class="op">=</span> {<span class="st">"study"</span>, <span class="st">"research"</span>, <span class="st">"analysis"</span>, <span class="st">"experiment"</span>}  <span class="co"># Add more words here</span></span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a>stop_words_final.update(additional_words)</span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Display the final set of stopwords</span></span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a>stop_words_final</span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(stop_words_final)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<ul>
<li><strong><code>stopwords.words("english")</code></strong>: Returns a list of common English stopwords.</li>
<li><strong><code>set()</code></strong>: Converts the list to a set for efficient lookup operations.</li>
<li><strong>Customization</strong>: Adding words like <code>"study"</code> ensures that domain-specific terms that are not useful for the analysis are also removed.</li>
</ul>
<p><strong>Example Output:</strong></p>
<p><img src="images/clipboard-2586914852.png" class="img-fluid"></p>
</section>
<section id="why-remove-stopwords" class="level5" data-number="16.3.2.5.2">
<h5 data-number="16.3.2.5.2" class="anchored" data-anchor-id="why-remove-stopwords"><span class="header-section-number">16.3.2.5.2</span> Why Remove Stopwords?</h5>
<ul>
<li><strong>Enhances Analysis</strong>: Removing stopwords reduces noise in the text data, allowing models to focus on more meaningful words.</li>
<li><strong>Improves Model Coherence</strong>: Reduces the dimensionality of the data and enhances the interpretability of topic models and other NLP algorithms.</li>
</ul>
</section>
</section>
</section>
<section id="building-bigram-models-for-text-analysis" class="level3" data-number="16.3.3">
<h3 data-number="16.3.3" class="anchored" data-anchor-id="building-bigram-models-for-text-analysis"><span class="header-section-number">16.3.3</span> Building Bigram Models for Text Analysis</h3>
<p>In natural language processing (NLP), a bigram refers to a sequence of two consecutive words in a sentence. Bigrams capture context that single-word tokens (unigrams) might miss, which can be beneficial for tasks such as topic modeling, language modeling, and more.</p>
<section id="introduction-to-gensims-phrases-model" class="level4" data-number="16.3.3.1">
<h4 data-number="16.3.3.1" class="anchored" data-anchor-id="introduction-to-gensims-phrases-model"><span class="header-section-number">16.3.3.1</span> Introduction to <code>gensim</code>’s Phrases Model</h4>
<p><code>gensim</code> provides a <code>Phrases</code> class that can be used to detect and build bigrams in text. The <code>Phrases</code> model identifies word pairs that appear together frequently enough to be treated as a single phrase.</p>
<section id="key-parameters" class="level5" data-number="16.3.3.1.1">
<h5 data-number="16.3.3.1.1" class="anchored" data-anchor-id="key-parameters"><span class="header-section-number">16.3.3.1.1</span> Key Parameters:</h5>
<ul>
<li><strong><code>min_count</code></strong>: Ignores all words and bigrams with a total collected count lower than this value. It helps filter out infrequent word pairs.</li>
<li><strong><code>threshold</code></strong>: Sets the score threshold for forming a bigram. A higher threshold results in fewer detected phrases, which ensures only the most significant pairs are identified.</li>
</ul>
</section>
<section id="formula-for-phrasing" class="level5" data-number="16.3.3.1.2">
<h5 data-number="16.3.3.1.2" class="anchored" data-anchor-id="formula-for-phrasing"><span class="header-section-number">16.3.3.1.2</span> Formula for Phrasing:</h5>
<p>A bigram is accepted if:</p>
<p><span class="math display">\[{(count(a, b) - min\_count) * N / (count(a) * count(b)) &gt; threshold}\]</span></p>
<p>where (N) is the total vocabulary size.</p>
</section>
</section>
<section id="building-and-applying-the-bigram-model" class="level4" data-number="16.3.3.2">
<h4 data-number="16.3.3.2" class="anchored" data-anchor-id="building-and-applying-the-bigram-model"><span class="header-section-number">16.3.3.2</span> Building and Applying the Bigram Model</h4>
<p>The first step is to create a <code>Phrases</code> object to detect word pairs in the tokenized text data.</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Importing the Phrases class from gensim</span></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> gensim.models.phrases <span class="im">import</span> Phrases, Phraser</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Building the bigram model with custom min_count and threshold values</span></span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Ensure data_words is a list of tokenized sentences</span></span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a><span class="co"># For example: data_words = [['the', 'purpose', 'of', 'this', 'paper'], ['is', 'to', 'explain', ...]]</span></span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a>bigram <span class="op">=</span> gensim.models.Phrases(data_words, min_count<span class="op">=</span><span class="dv">1</span>, threshold<span class="op">=</span><span class="dv">10</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<ul>
<li><strong><code>min_count=1</code></strong>: Only considers word pairs that appear at least once in the text corpus.</li>
<li><strong><code>threshold=10</code></strong>: A higher threshold results in fewer, more significant bigrams.</li>
</ul>
</section>
<section id="creating-a-faster-bigram-model-using-phraser" class="level4" data-number="16.3.3.3">
<h4 data-number="16.3.3.3" class="anchored" data-anchor-id="creating-a-faster-bigram-model-using-phraser"><span class="header-section-number">16.3.3.3</span> Creating a Faster Bigram Model Using <code>Phraser</code></h4>
<p>Once the <code>Phrases</code> model is built, it can be converted to a <code>Phraser</code> object, which allows for more efficient transformation of tokenized text.</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Converting the bigram model to a Phraser for efficient use</span></span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>bigram_mod <span class="op">=</span> Phraser(bigram)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="applying-the-bigram-model" class="level4" data-number="16.3.3.4">
<h4 data-number="16.3.3.4" class="anchored" data-anchor-id="applying-the-bigram-model"><span class="header-section-number">16.3.3.4</span> Applying the Bigram Model</h4>
<p>To view the results of the bigram model, apply it to tokenized text data:</p>
<div class="sourceCode" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Printing the first tokenized sentence with bigrams</span></span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>bigram_sentences <span class="op">=</span> [bigram_mod[sentence] <span class="cf">for</span> sentence <span class="kw">in</span> data_words]</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(bigram_sentences[<span class="dv">0</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><strong>Expected Outcome:</strong></p>
<p>The output shows a list of tokens where frequent word pairs are combined into a single token:</p>
<p><img src="images/clipboard-2410213701.png" class="img-fluid"></p>
</section>
<section id="benefits-of-using-bigrams" class="level4" data-number="16.3.3.5">
<h4 data-number="16.3.3.5" class="anchored" data-anchor-id="benefits-of-using-bigrams"><span class="header-section-number">16.3.3.5</span> Benefits of Using Bigrams:</h4>
<ul>
<li><strong>Improves Context</strong>: Bigrams provide more context than individual words, making text analysis richer and more meaningful.</li>
<li><strong>Enhances Topic Modeling</strong>: Combining common word pairs helps LDA and other models identify topics more accurately.</li>
</ul>
</section>
</section>
<section id="defining-custom-functions" class="level3" data-number="16.3.4">
<h3 data-number="16.3.4" class="anchored" data-anchor-id="defining-custom-functions"><span class="header-section-number">16.3.4</span> Defining Custom Functions</h3>
<section id="function-for-removing-stopwords" class="level4" data-number="16.3.4.1">
<h4 data-number="16.3.4.1" class="anchored" data-anchor-id="function-for-removing-stopwords"><span class="header-section-number">16.3.4.1</span> Function for Removing Stopwords</h4>
<div class="sourceCode" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> remove_stopwords(texts):</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> [[word <span class="cf">for</span> word <span class="kw">in</span> simple_preprocess(<span class="bu">str</span>(doc)) <span class="cf">if</span> word <span class="kw">not</span> <span class="kw">in</span> stop_words_final] <span class="cf">for</span> doc <span class="kw">in</span> texts]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<ul>
<li><strong>Input</strong>: A list of tokenized sentences (<code>texts</code>).</li>
<li><strong>Process</strong>: Iterates through each word in a document and removes those present in the <code>stop_words_final</code> set.</li>
<li><strong>Output</strong>: A list of tokenized sentences without stopwords.</li>
</ul>
</section>
<section id="function-for-creating-bigrams" class="level4" data-number="16.3.4.2">
<h4 data-number="16.3.4.2" class="anchored" data-anchor-id="function-for-creating-bigrams"><span class="header-section-number">16.3.4.2</span> Function for Creating Bigrams</h4>
<p>This function applies a pre-trained bigram model to generate bigrams from tokenized sentences.</p>
<div class="sourceCode" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> make_bigrams(texts):</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> [bigram_mod[doc] <span class="cf">for</span> doc <span class="kw">in</span> texts]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<ul>
<li><strong>Input</strong>: A list of tokenized sentences.</li>
<li><strong>Process</strong>: Uses <code>bigram_mod</code>, a <code>Phraser</code> object, to transform sentences and add bigrams where appropriate.</li>
<li><strong>Output</strong>: A list of tokenized sentences where frequent word pairs are combined into bigrams.</li>
</ul>
</section>
<section id="function-for-lemmatization" class="level4" data-number="16.3.4.3">
<h4 data-number="16.3.4.3" class="anchored" data-anchor-id="function-for-lemmatization"><span class="header-section-number">16.3.4.3</span> Function for Lemmatization</h4>
<p>Lemmatization reduces words to their base or dictionary form, known as the lemma. This helps group different forms of a word (e.g., “running” and “ran”) into a single term (“run”).</p>
<div class="sourceCode" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> lemmatization(texts, allowed_postags<span class="op">=</span>[<span class="st">'NOUN'</span>, <span class="st">'ADJ'</span>, <span class="st">'VERB'</span>, <span class="st">'ADV'</span>]):</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>    texts_out <span class="op">=</span> []</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> sent <span class="kw">in</span> texts:</span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a>        doc <span class="op">=</span> nlp(<span class="st">" "</span>.join(sent))</span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a>        texts_out.append([token.lemma_ <span class="cf">for</span> token <span class="kw">in</span> doc <span class="cf">if</span> token.pos_ <span class="kw">in</span> allowed_postags])</span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> texts_out</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<ul>
<li><strong>Input</strong>: A list of tokenized sentences and a list of allowed part-of-speech (POS) tags (default is <code>['NOUN', 'ADJ', 'VERB', 'ADV']</code>).</li>
<li><strong>Process</strong>:
<ul>
<li>Uses the <code>spacy</code> NLP pipeline (<code>nlp</code>) to process sentences.</li>
<li>Extracts the lemma of each token if its POS tag matches the allowed POS tags.</li>
</ul></li>
<li><strong>Output</strong>: A list of tokenized sentences with words lemmatized and filtered by POS tags.</li>
</ul>
</section>
</section>
<section id="step-by-step-preprocessing-workflow" class="level3" data-number="16.3.5">
<h3 data-number="16.3.5" class="anchored" data-anchor-id="step-by-step-preprocessing-workflow"><span class="header-section-number">16.3.5</span> Step-by-Step Preprocessing Workflow</h3>
<section id="removing-stopwords" class="level4" data-number="16.3.5.1">
<h4 data-number="16.3.5.1" class="anchored" data-anchor-id="removing-stopwords"><span class="header-section-number">16.3.5.1</span> Removing Stopwords</h4>
<p>The first step is to remove common stopwords from the tokenized text using the <code>remove_stopwords()</code> function:</p>
<div class="sourceCode" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Remove stopwords from the tokenized data</span></span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>data_noStopWords <span class="op">=</span> remove_stopwords(data_words)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<ul>
<li><strong><code>data_words</code></strong>: The tokenized text data.</li>
<li><strong><code>data_noStopWords</code></strong>: The output is a list of tokenized sentences with stopwords removed.</li>
</ul>
</section>
<section id="forming-bigrams" class="level4" data-number="16.3.5.2">
<h4 data-number="16.3.5.2" class="anchored" data-anchor-id="forming-bigrams"><span class="header-section-number">16.3.5.2</span> Forming Bigrams</h4>
<p>Next, we apply the <code>make_bigrams()</code> function to create bigrams in the text data:</p>
<div class="sourceCode" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Form bigrams from the data without stopwords</span></span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>data_bgrams <span class="op">=</span> make_bigrams(data_noStopWords)</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(data_bgrams[<span class="dv">0</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><img src="images/clipboard-2862194758.png" class="img-fluid"></p>
<ul>
<li><strong><code>data_noStopWords</code></strong>: The tokenized sentences without stopwords.</li>
<li><strong><code>data_bgrams</code></strong>: The output is a list of tokenized sentences where common word pairs have been combined into bigrams.</li>
</ul>
</section>
<section id="initializing-spacy-for-lemmatization" class="level4" data-number="16.3.5.3">
<h4 data-number="16.3.5.3" class="anchored" data-anchor-id="initializing-spacy-for-lemmatization"><span class="header-section-number">16.3.5.3</span> Initializing <code>spaCy</code> for Lemmatization</h4>
<p>Before performing lemmatization, we need to initialize the <code>spaCy</code> language model. For efficiency, only the <code>tagger</code> component is kept active:</p>
<div class="sourceCode" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize spaCy English model with limited components for efficiency</span></span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> spacy</span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a>nlp <span class="op">=</span> spacy.load(<span class="st">"en_core_web_sm"</span>, disable<span class="op">=</span>[<span class="st">'parser'</span>, <span class="st">'ner'</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<ul>
<li><strong><code>en_core_web_sm</code></strong>: A lightweight English language model suitable for basic NLP tasks.</li>
<li><strong><code>disable=['parser', 'ner']</code></strong>: Disables the parser and named entity recognition to optimize performance during lemmatization.</li>
</ul>
</section>
<section id="lemmatization" class="level4" data-number="16.3.5.4">
<h4 data-number="16.3.5.4" class="anchored" data-anchor-id="lemmatization"><span class="header-section-number">16.3.5.4</span> Lemmatization</h4>
<p>The final step is to apply the <code>lemmatization()</code> function, which reduces words to their base forms while keeping only specified parts of speech:</p>
<div class="sourceCode" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Perform lemmatization on the bigram data, keeping only nouns and verbs</span></span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>bgram_lemmatized <span class="op">=</span> lemmatization(data_bgrams, allowed_postags<span class="op">=</span>[<span class="st">'NOUN'</span>, <span class="st">'VERB'</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<ul>
<li><strong><code>data_bgrams</code></strong>: The tokenized sentences with bigrams.</li>
<li><strong><code>allowed_postags=['NOUN', 'VERB']</code></strong>: Filters the tokens to keep only nouns and verbs during lemmatization.</li>
<li><strong><code>bgram_lemmatized</code></strong>: The output is a list of sentences with lemmatized tokens.</li>
</ul>
<div class="sourceCode" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(bgram_lemmatized[<span class="dv">0</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
</section>
<section id="transforming-data-into-corpus-and-dictionary-formats" class="level3" data-number="16.3.6">
<h3 data-number="16.3.6" class="anchored" data-anchor-id="transforming-data-into-corpus-and-dictionary-formats"><span class="header-section-number">16.3.6</span> Transforming Data into Corpus and Dictionary Formats</h3>
<p>In order to train topic models such as Latent Dirichlet Allocation (LDA), text data must be converted into a numerical format that represents the words and their frequencies. <code>gensim</code> provides tools for transforming text data into a <strong>dictionary</strong> and a <strong>corpus</strong> format, which are essential for topic modeling.</p>
<section id="understanding-corpus-and-dictionary" class="level4" data-number="16.3.6.1">
<h4 data-number="16.3.6.1" class="anchored" data-anchor-id="understanding-corpus-and-dictionary"><span class="header-section-number">16.3.6.1</span> Understanding Corpus and Dictionary</h4>
<ul>
<li><strong>Dictionary</strong>: Maps each unique word in the text data to a unique integer ID. This dictionary acts as a reference for word encoding.</li>
<li><strong>Corpus</strong>: Represents the text data in the form of <code>(word_id, word_frequency)</code> tuples, where <code>word_id</code> corresponds to the unique ID from the dictionary and <code>word_frequency</code> indicates the number of times the word appears in the document.</li>
</ul>
</section>
<section id="creating-a-dictionary" class="level4" data-number="16.3.6.2">
<h4 data-number="16.3.6.2" class="anchored" data-anchor-id="creating-a-dictionary"><span class="header-section-number">16.3.6.2</span> Creating a Dictionary</h4>
</section>
<section id="using-gensim-to-create-a-dictionary" class="level4" data-number="16.3.6.3">
<h4 data-number="16.3.6.3" class="anchored" data-anchor-id="using-gensim-to-create-a-dictionary"><span class="header-section-number">16.3.6.3</span> Using <code>gensim</code> to Create a Dictionary</h4>
<p>The <code>gensim.corpora.Dictionary</code> class is used to create a dictionary from the preprocessed text data:</p>
<div class="sourceCode" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Importing the corpora module from gensim</span></span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> gensim.corpora <span class="im">as</span> corpora</span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Creating a dictionary from the lemmatized text data</span></span>
<span id="cb23-5"><a href="#cb23-5" aria-hidden="true" tabindex="-1"></a>dictionary_bgram <span class="op">=</span> corpora.Dictionary(bgram_lemmatized)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<ul>
<li><strong><code>corpora.Dictionary()</code></strong>: Takes a list of tokenized sentences and creates a mapping of words to unique integer IDs.</li>
<li><strong><code>bgram_lemmatized</code></strong>: The lemmatized and preprocessed text data.</li>
</ul>
<p><strong>Example:</strong></p>
<div class="sourceCode" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(dictionary_bgram.token2id)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>The output will display a dictionary with words as keys and their corresponding integer IDs as values.</p>
</section>
</section>
<section id="creating-a-corpus" class="level3" data-number="16.3.7">
<h3 data-number="16.3.7" class="anchored" data-anchor-id="creating-a-corpus"><span class="header-section-number">16.3.7</span> Creating a Corpus</h3>
<section id="transforming-text-data-into-a-corpus" class="level4" data-number="16.3.7.1">
<h4 data-number="16.3.7.1" class="anchored" data-anchor-id="transforming-text-data-into-a-corpus"><span class="header-section-number">16.3.7.1</span> Transforming Text Data into a Corpus</h4>
<p>The corpus is created by converting each document into a bag-of-words (BoW) format, where each document is represented as a list of <code>(word_id, word_frequency)</code> tuples.</p>
<div class="sourceCode" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Assigning the preprocessed data to a variable for clarity</span></span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a>texts_bgram <span class="op">=</span> bgram_lemmatized</span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Creating the corpus using the doc2bow method</span></span>
<span id="cb25-5"><a href="#cb25-5" aria-hidden="true" tabindex="-1"></a>corpus_bgram <span class="op">=</span> [dictionary_bgram.doc2bow(text) <span class="cf">for</span> text <span class="kw">in</span> texts_bgram]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<ul>
<li><strong><code>doc2bow()</code></strong>: Converts a document (a list of words) into a BoW representation, counting the frequency of each word in the document and returning it as a list of <code>(word_id, word_frequency)</code> tuples.</li>
</ul>
<p><strong>Example:</strong></p>
<div class="sourceCode" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Displaying the first few elements of the corpus</span></span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(texts_bgram[<span class="dv">1</span>])</span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(corpus_bgram[<span class="dv">1</span>:<span class="dv">3</span>])  <span class="co"># View the second and third documents</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<ul>
<li><strong><code>texts_bgram[1]</code></strong>: Displays the list of words in the second document.</li>
<li><strong><code>corpus_bgram[1:3]</code></strong>: Displays the BoW format for the second and third documents, e.g., <code>[(word_id, frequency), ...]</code>.</li>
</ul>
</section>
<section id="explanation-of-the-corpus-format" class="level4" data-number="16.3.7.2">
<h4 data-number="16.3.7.2" class="anchored" data-anchor-id="explanation-of-the-corpus-format"><span class="header-section-number">16.3.7.2</span> Explanation of the Corpus Format</h4>
<p>Each document in the corpus contains tuples where:</p>
<ul>
<li><strong><code>word_id</code></strong>: The unique integer ID assigned to the word in the dictionary.</li>
<li><strong><code>word_frequency</code></strong>: The number of times the word appears in the document.</li>
</ul>
<p>Example interpretation:</p>
<pre><code>[(0, 2), (3, 1), (5, 1)]</code></pre>
<p>This output indicates that:</p>
<ul>
<li>The word with ID <code>0</code> appears twice.</li>
<li>The word with ID <code>3</code> appears once.</li>
<li>The word with ID <code>5</code> appears once.</li>
</ul>
</section>
<section id="benefits-of-using-corpus-and-dictionary" class="level4" data-number="16.3.7.3">
<h4 data-number="16.3.7.3" class="anchored" data-anchor-id="benefits-of-using-corpus-and-dictionary"><span class="header-section-number">16.3.7.3</span> Benefits of Using Corpus and Dictionary</h4>
<ul>
<li><strong>Efficient Storage</strong>: The BoW format is a sparse representation that saves space.</li>
<li><strong>Compatibility</strong>: Most topic modeling algorithms, including LDA, use a corpus and dictionary as input.</li>
<li><strong>Flexibility</strong>: The dictionary can be further modified to remove low-frequency or high-frequency words, which can help refine the analysis.</li>
</ul>
</section>
</section>
<section id="building-the-base-topic-model-with-lda" class="level3" data-number="16.3.8">
<h3 data-number="16.3.8" class="anchored" data-anchor-id="building-the-base-topic-model-with-lda"><span class="header-section-number">16.3.8</span> Building the Base Topic Model with LDA</h3>
<section id="important-parameters-for-building-an-lda-model" class="level4" data-number="16.3.8.1">
<h4 data-number="16.3.8.1" class="anchored" data-anchor-id="important-parameters-for-building-an-lda-model"><span class="header-section-number">16.3.8.1</span> Important Parameters for Building an LDA Model</h4>
<p>When building an LDA model, several parameters influence the outcome and quality of the generated topics:</p>
<p><strong>Number of Topics (<code>num_topics</code>)</strong></p>
<ul>
<li><strong><code>k</code></strong>: The number of topics the model should find in the corpus. This parameter requires careful tuning based on the data and the goals of the analysis.</li>
<li><strong>Example</strong>: <code>k = 4</code>, meaning the model will attempt to identify 4 distinct topics.</li>
</ul>
<p><strong><code>id2word</code></strong></p>
<ul>
<li><strong><code>id2word</code></strong>: A dictionary that maps word IDs to words (strings). This is crucial for determining vocabulary size and understanding the topics the model generates.</li>
<li><strong>Source</strong>: Created from the <code>gensim.corpora.Dictionary</code> object.</li>
</ul>
<p><strong>Hyperparameters: <code>alpha</code> and <code>eta</code></strong></p>
<ul>
<li><strong><code>alpha</code></strong> (document-topic distribution):
<ul>
<li>Affects the sparsity of the topic representation within documents.</li>
<li>A lower value (close to 0) indicates fewer topics per document, while a higher value results in more topics per document.</li>
<li><strong>Example</strong>: <code>alpha = 1.0/k</code>, making it symmetric across topics.</li>
</ul></li>
<li><strong><code>eta</code></strong> (topic-word distribution):
<ul>
<li>Affects the sparsity of the word distribution within topics.</li>
<li>A lower value results in fewer words per topic, while a higher value includes more words per topic.</li>
<li><strong>Example</strong>: <code>eta = 1.0/k</code>, creating a symmetric distribution.</li>
</ul></li>
</ul>
<p><strong><code>chunksize</code></strong></p>
<ul>
<li>Controls the number of documents processed at a time during training.</li>
<li><strong>Impact</strong>: A larger <code>chunksize</code> can speed up training but requires more memory.</li>
<li><strong>Example</strong>: <code>chunksize=100</code>.</li>
</ul>
<p><strong><code>passes</code></strong></p>
<ul>
<li>The number of times the model iterates over the entire corpus (also known as “epochs”).</li>
<li><strong>Impact</strong>: More passes generally improve the model’s quality but increase training time.</li>
<li><strong>Example</strong>: <code>passes=10</code>.</li>
</ul>
</section>
</section>
<section id="building-the-lda-model" class="level3" data-number="16.3.9">
<h3 data-number="16.3.9" class="anchored" data-anchor-id="building-the-lda-model"><span class="header-section-number">16.3.9</span> Building the LDA Model</h3>
<p>Below is the code to build an LDA model using <code>gensim</code>’s <code>LdaMulticore</code> class:</p>
<div class="sourceCode" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Importing the necessary module</span></span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> gensim.models</span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-4"><a href="#cb28-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Setting parameters for the model</span></span>
<span id="cb28-5"><a href="#cb28-5" aria-hidden="true" tabindex="-1"></a>k <span class="op">=</span> <span class="dv">4</span>  <span class="co"># Number of topics</span></span>
<span id="cb28-6"><a href="#cb28-6" aria-hidden="true" tabindex="-1"></a>a <span class="op">=</span> <span class="fl">1.0</span> <span class="op">/</span> k  <span class="co"># Alpha value for symmetric topic distribution</span></span>
<span id="cb28-7"><a href="#cb28-7" aria-hidden="true" tabindex="-1"></a>b <span class="op">=</span> <span class="fl">1.0</span> <span class="op">/</span> k  <span class="co"># Eta value for symmetric word distribution</span></span>
<span id="cb28-8"><a href="#cb28-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-9"><a href="#cb28-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Building the LDA model</span></span>
<span id="cb28-10"><a href="#cb28-10" aria-hidden="true" tabindex="-1"></a>lda_model <span class="op">=</span> gensim.models.LdaMulticore(</span>
<span id="cb28-11"><a href="#cb28-11" aria-hidden="true" tabindex="-1"></a>    corpus<span class="op">=</span>corpus_bgram,          <span class="co"># Corpus in BoW format</span></span>
<span id="cb28-12"><a href="#cb28-12" aria-hidden="true" tabindex="-1"></a>    id2word<span class="op">=</span>dictionary_bgram,     <span class="co"># Dictionary for word-ID mapping</span></span>
<span id="cb28-13"><a href="#cb28-13" aria-hidden="true" tabindex="-1"></a>    random_state<span class="op">=</span><span class="dv">100</span>,             <span class="co"># Seed for reproducibility</span></span>
<span id="cb28-14"><a href="#cb28-14" aria-hidden="true" tabindex="-1"></a>    chunksize<span class="op">=</span><span class="dv">100</span>,                <span class="co"># Number of documents to process at once</span></span>
<span id="cb28-15"><a href="#cb28-15" aria-hidden="true" tabindex="-1"></a>    passes<span class="op">=</span><span class="dv">10</span>,                    <span class="co"># Number of iterations over the entire corpus</span></span>
<span id="cb28-16"><a href="#cb28-16" aria-hidden="true" tabindex="-1"></a>    num_topics<span class="op">=</span>k,                 <span class="co"># Number of topics</span></span>
<span id="cb28-17"><a href="#cb28-17" aria-hidden="true" tabindex="-1"></a>    alpha<span class="op">=</span>a,                      <span class="co"># Document-topic distribution hyperparameter</span></span>
<span id="cb28-18"><a href="#cb28-18" aria-hidden="true" tabindex="-1"></a>    eta<span class="op">=</span>b                         <span class="co"># Topic-word distribution hyperparameter</span></span>
<span id="cb28-19"><a href="#cb28-19" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<ul>
<li><strong><code>gensim.models.LdaMulticore</code></strong>:
<ul>
<li>Utilizes multiple CPU cores to speed up training.</li>
<li>Takes <code>corpus</code> and <code>id2word</code> as mandatory arguments.</li>
</ul></li>
<li><strong>Hyperparameters</strong>:
<ul>
<li><code>alpha</code> and <code>eta</code> were set to <code>1.0/k</code> for a symmetric prior, evenly distributing the importance of topics and words.</li>
</ul></li>
<li><strong><code>random_state</code></strong>:
<ul>
<li>Ensures reproducibility by seeding the random number generator.</li>
</ul></li>
</ul>
<p>To inspect the topics discovered by the model, use:</p>
<div class="sourceCode" id="cb29"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Printing the topics found by the model</span></span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> idx, topic <span class="kw">in</span> lda_model.print_topics(<span class="op">-</span><span class="dv">1</span>):</span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Topic </span><span class="sc">{</span>idx<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>topic<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><strong>Example Output:</strong></p>
<pre><code>Topic 0: '0.025*"data" + 0.020*"analysis" + 0.018*"model" + ...'
Topic 1: '0.030*"machine" + 0.025*"learning" + 0.015*"algorithm" + ...'
...</code></pre>
</section>
<section id="evaluating-the-lda-model-with-perplexity-and-coherence" class="level3" data-number="16.3.10">
<h3 data-number="16.3.10" class="anchored" data-anchor-id="evaluating-the-lda-model-with-perplexity-and-coherence"><span class="header-section-number">16.3.10</span> Evaluating the LDA Model with Perplexity and Coherence</h3>
<p>Evaluating the quality of an LDA model is crucial to ensure that it generates meaningful topics. Two common metrics used for this purpose are <strong>Perplexity</strong> and <strong>Coherence Score</strong>.</p>
<ul>
<li><strong>Perplexity</strong>: Measures how well a model predicts a sample. A <strong>lower perplexity score indicates a better generalization</strong> of the model to unseen data.</li>
<li><strong>Coherence Score</strong>: Assesses the degree of semantic similarity between high-scoring words in a topic. Higher coherence indicates more interpretable and relevant topics.</li>
</ul>
<section id="code-to-compute-perplexity" class="level4" data-number="16.3.10.1">
<h4 data-number="16.3.10.1" class="anchored" data-anchor-id="code-to-compute-perplexity"><span class="header-section-number">16.3.10.1</span> Code to Compute Perplexity</h4>
<div class="sourceCode" id="cb31"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Importing the necessary module for CoherenceModel</span></span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> gensim.models.coherencemodel <span class="im">import</span> CoherenceModel</span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-4"><a href="#cb31-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Computing the Perplexity of the LDA model</span></span>
<span id="cb31-5"><a href="#cb31-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'</span><span class="ch">\n</span><span class="st">Perplexity: '</span>, lda_model.log_perplexity(corpus_bgram))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<ul>
<li><strong><code>lda_model.log_perplexity(corpus_bgram)</code></strong>:
<ul>
<li>Evaluates the model’s perplexity on the given corpus.</li>
<li>The lower the perplexity score, the better the model is at predicting unseen data.</li>
</ul></li>
</ul>
<pre><code>Perplexity: -7.123456789 (example value)</code></pre>
</section>
</section>
<section id="computing-coherence-score" class="level3" data-number="16.3.11">
<h3 data-number="16.3.11" class="anchored" data-anchor-id="computing-coherence-score"><span class="header-section-number">16.3.11</span> Computing Coherence Score</h3>
<p>The Coherence Score measures the degree of semantic similarity between the high-scoring words in a topic. There are different types of coherence measures, with <strong>‘c_v’</strong> being one of the most commonly used as it correlates well with human interpretation.</p>
<section id="code-to-compute-coherence-score" class="level4" data-number="16.3.11.1">
<h4 data-number="16.3.11.1" class="anchored" data-anchor-id="code-to-compute-coherence-score"><span class="header-section-number">16.3.11.1</span> Code to Compute Coherence Score</h4>
<div class="sourceCode" id="cb33"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Computing the Coherence Score</span></span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a>coherence_model_lda <span class="op">=</span> CoherenceModel(</span>
<span id="cb33-3"><a href="#cb33-3" aria-hidden="true" tabindex="-1"></a>    model<span class="op">=</span>lda_model,</span>
<span id="cb33-4"><a href="#cb33-4" aria-hidden="true" tabindex="-1"></a>    texts<span class="op">=</span>bgram_lemmatized,</span>
<span id="cb33-5"><a href="#cb33-5" aria-hidden="true" tabindex="-1"></a>    dictionary<span class="op">=</span>dictionary_bgram,</span>
<span id="cb33-6"><a href="#cb33-6" aria-hidden="true" tabindex="-1"></a>    coherence<span class="op">=</span><span class="st">'c_v'</span></span>
<span id="cb33-7"><a href="#cb33-7" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb33-8"><a href="#cb33-8" aria-hidden="true" tabindex="-1"></a>coherence_lda <span class="op">=</span> coherence_model_lda.get_coherence()</span>
<span id="cb33-9"><a href="#cb33-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-10"><a href="#cb33-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Printing the Coherence Score</span></span>
<span id="cb33-11"><a href="#cb33-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'</span><span class="ch">\n</span><span class="st">Coherence Score: '</span>, coherence_lda)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<ul>
<li><strong><code>CoherenceModel()</code></strong>:</li>
<li>Takes the trained LDA model, the tokenized texts, the dictionary, and the type of coherence measure as inputs.</li>
<li><strong><code>coherence='c_v'</code></strong>: A coherence measure that evaluates the semantic similarity between words.</li>
<li><strong><code>get_coherence()</code></strong>:</li>
<li>Computes and returns the coherence score for the model.</li>
<li><strong>Interpretation</strong>: A higher coherence score indicates that the topics are more interpretable and relevant.</li>
</ul>
<pre><code>Coherence Score: 0.45 (example value)</code></pre>
</section>
<section id="importance-of-model-evaluation" class="level4" data-number="16.3.11.2">
<h4 data-number="16.3.11.2" class="anchored" data-anchor-id="importance-of-model-evaluation"><span class="header-section-number">16.3.11.2</span> Importance of Model Evaluation</h4>
<ul>
<li><strong>Perplexity</strong>:
<ul>
<li>While perplexity is a useful metric, it does not always correlate well with human interpretation. It is best used alongside coherence to assess the model’s quality.</li>
</ul></li>
<li><strong>Coherence</strong>:
<ul>
<li>The coherence score provides a direct indication of how interpretable the topics are. It helps in fine-tuning the model for producing more meaningful and human-readable topics.</li>
</ul></li>
</ul>
</section>
</section>
<section id="visualizing-topics-and-keywords" class="level3" data-number="16.3.12">
<h3 data-number="16.3.12" class="anchored" data-anchor-id="visualizing-topics-and-keywords"><span class="header-section-number">16.3.12</span> Visualizing Topics and Keywords</h3>
<p>To better understand the topics generated by an LDA model, visualization tools can be invaluable. <strong>PyLDAvis</strong> is a popular library that provides an interactive way to examine topics and the associated keywords. This chapter will guide you through setting up PyLDAvis and using it to visualize LDA model results.</p>
<section id="installing-and-importing-pyldavis" class="level4" data-number="16.3.12.1">
<h4 data-number="16.3.12.1" class="anchored" data-anchor-id="installing-and-importing-pyldavis"><span class="header-section-number">16.3.12.1</span> Installing and Importing PyLDAvis</h4>
<p>Ensure that <strong>PyLDAvis</strong> is installed in your environment:</p>
<div class="sourceCode" id="cb35"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a><span class="ex">!pip</span> install pyLDAvis</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Import the necessary modules:</p>
<div class="sourceCode" id="cb36"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pyLDAvis</span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pyLDAvis.gensim_models</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="preparing-and-displaying-the-visualization" class="level4" data-number="16.3.12.2">
<h4 data-number="16.3.12.2" class="anchored" data-anchor-id="preparing-and-displaying-the-visualization"><span class="header-section-number">16.3.12.2</span> Preparing and Displaying the Visualization</h4>
<p>To visualize the topics and their key terms, prepare the data using PyLDAvis:</p>
<div class="sourceCode" id="cb37"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Enable PyLDAvis to work in a notebook environment</span></span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true" tabindex="-1"></a>pyLDAvis.enable_notebook()</span>
<span id="cb37-3"><a href="#cb37-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-4"><a href="#cb37-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Prepare the visualization data</span></span>
<span id="cb37-5"><a href="#cb37-5" aria-hidden="true" tabindex="-1"></a>LDAvis_prepared <span class="op">=</span> pyLDAvis.gensim_models.prepare(lda_model, corpus_bgram, dictionary_bgram)</span>
<span id="cb37-6"><a href="#cb37-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-7"><a href="#cb37-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Save the visualization as an HTML file</span></span>
<span id="cb37-8"><a href="#cb37-8" aria-hidden="true" tabindex="-1"></a>pyLDAvis.save_html(LDAvis_prepared, <span class="st">'lda_model_visualization.html'</span>)</span>
<span id="cb37-9"><a href="#cb37-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-10"><a href="#cb37-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Display the visualization</span></span>
<span id="cb37-11"><a href="#cb37-11" aria-hidden="true" tabindex="-1"></a>pyLDAvis.display(LDAvis_prepared)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><img src="images/clipboard-3103001348.png" class="img-fluid"></p>
<p>The PyLDAvis visualization consists of the following components:</p>
<ul>
<li><strong>Circles</strong>: Represent topics. The size of the circle indicates the proportion of the corpus that the topic covers.</li>
<li><strong>Distance Between Circles</strong>: Indicates how different the topics are from one another. Topics that are more similar are closer together.</li>
<li><strong>λ (Lambda) Parameter</strong>:
<ul>
<li>Controls the relevance of the words shown for each topic.</li>
<li><strong>λ close to 1</strong>: Displays more frequently appearing words within each topic.</li>
<li><strong>λ close to 0</strong>: Shows words that distinguish one topic from another.</li>
</ul></li>
</ul>
</section>
<section id="how-to-interpret" class="level4" data-number="16.3.12.3">
<h4 data-number="16.3.12.3" class="anchored" data-anchor-id="how-to-interpret"><span class="header-section-number">16.3.12.3</span> How to Interpret:</h4>
<ul>
<li><strong>Explore topics by clicking on them</strong> to see the most relevant keywords.</li>
<li>Adjust the <strong>λ slider</strong> to view words based on their frequency and distinguishing power.</li>
</ul>
</section>
</section>
<section id="document-topic-distribution" class="level3" data-number="16.3.13">
<h3 data-number="16.3.13" class="anchored" data-anchor-id="document-topic-distribution"><span class="header-section-number">16.3.13</span> Document-Topic Distribution</h3>
<p>Understanding how documents are distributed across topics is essential for interpreting LDA results. The <strong>document-topic distribution</strong> shows the proportion of each topic within a document, indicating which topics are most prevalent.</p>
<section id="extracting-document-topic-distribution" class="level4" data-number="16.3.13.1">
<h4 data-number="16.3.13.1" class="anchored" data-anchor-id="extracting-document-topic-distribution"><span class="header-section-number">16.3.13.1</span> Extracting Document-Topic Distribution</h4>
<p>The following code snippet iterates through the documents in the corpus and prints the topic distribution for each:</p>
<div class="sourceCode" id="cb38"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Iterate through the document-topic distribution and print for the first few documents</span></span>
<span id="cb38-2"><a href="#cb38-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, topic_list <span class="kw">in</span> <span class="bu">enumerate</span>(lda_model[corpus_bgram]):</span>
<span id="cb38-3"><a href="#cb38-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> i <span class="op">==</span> <span class="dv">5</span>:  <span class="co"># Limit to the first 5 documents for demonstration</span></span>
<span id="cb38-4"><a href="#cb38-4" aria-hidden="true" tabindex="-1"></a>        <span class="cf">break</span></span>
<span id="cb38-5"><a href="#cb38-5" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>i<span class="sc">}</span><span class="ss">th document consists of the following topics (proportions):"</span>, topic_list)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<ul>
<li><strong><code>lda_model[corpus_bgram]</code></strong>:
<ul>
<li>Applies the trained LDA model to the corpus and returns a list of tuples for each document. Each tuple contains a topic ID and its proportion within the document.</li>
</ul></li>
<li><strong><code>topic_list</code></strong>:
<ul>
<li>A list of tuples representing the topic distribution in a document. Each tuple follows the format <code>(topic_id, proportion)</code>, where <code>proportion</code> indicates how much of the document is represented by <code>topic_id</code>.</li>
</ul></li>
</ul>
<p><strong>Example Output:</strong></p>
<pre><code>0th document consists of the following topics (proportions): [(0, 0.25), (2, 0.50), (3, 0.25)]
1th document consists of the following topics (proportions): [(1, 0.60), (2, 0.40)]
...</code></pre>
<ul>
<li>In the first document, <strong>Topic 0</strong> accounts for 25%, <strong>Topic 2</strong> for 50%, and <strong>Topic 3</strong> for 25% of the content.</li>
<li>These proportions help identify the dominant topic(s) in each document and how each document contributes to the overall topic model.</li>
</ul>
</section>
<section id="use-cases-for-document-topic-distributions" class="level4" data-number="16.3.13.2">
<h4 data-number="16.3.13.2" class="anchored" data-anchor-id="use-cases-for-document-topic-distributions"><span class="header-section-number">16.3.13.2</span> Use Cases for Document-Topic Distributions</h4>
<ul>
<li><strong>Topic-Based Document Classification</strong>: Documents can be categorized based on the most prevalent topic.</li>
<li><strong>Content Analysis</strong>: Understanding the spread of topics across a corpus helps in content segmentation and thematic analysis.</li>
<li><strong>Model Refinement</strong>: Identifying documents with mixed topics can indicate whether further preprocessing or model tuning is needed.</li>
</ul>
</section>
<section id="saving-document-topic-distributions-to-csv" class="level4" data-number="16.3.13.3">
<h4 data-number="16.3.13.3" class="anchored" data-anchor-id="saving-document-topic-distributions-to-csv"><span class="header-section-number">16.3.13.3</span> Saving Document-Topic Distributions to CSV</h4>
<p>After computing the document-topic distributions, it is often useful to export this data for further analysis or reporting. This chapter will guide you through the process of storing the document-topic distribution as a DataFrame and exporting it to a CSV file for easy access.</p>
<div class="sourceCode" id="cb40"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize an empty list to store document-topic distributions</span></span>
<span id="cb40-2"><a href="#cb40-2" aria-hidden="true" tabindex="-1"></a>doc_topics <span class="op">=</span> []</span>
<span id="cb40-3"><a href="#cb40-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-4"><a href="#cb40-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Iterate over each document in the corpus to extract topic proportions</span></span>
<span id="cb40-5"><a href="#cb40-5" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, topic_list <span class="kw">in</span> <span class="bu">enumerate</span>(lda_model[corpus_bgram]):</span>
<span id="cb40-6"><a href="#cb40-6" aria-hidden="true" tabindex="-1"></a>    doc_topics.append(topic_list)</span>
<span id="cb40-7"><a href="#cb40-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-8"><a href="#cb40-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert the list of document-topic distributions to a DataFrame</span></span>
<span id="cb40-9"><a href="#cb40-9" aria-hidden="true" tabindex="-1"></a>lda_model_topics <span class="op">=</span> pd.DataFrame(doc_topics)</span>
<span id="cb40-10"><a href="#cb40-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-11"><a href="#cb40-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Export the DataFrame to a CSV file</span></span>
<span id="cb40-12"><a href="#cb40-12" aria-hidden="true" tabindex="-1"></a>lda_model_topics.to_csv(<span class="st">'doc_topics.csv'</span>, index<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb40-13"><a href="#cb40-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-14"><a href="#cb40-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Display the DataFrame to verify the results</span></span>
<span id="cb40-15"><a href="#cb40-15" aria-hidden="true" tabindex="-1"></a>lda_model_topics</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<ul>
<li><strong><code>doc_topics.append(topic_list)</code></strong>: Appends the topic distribution of each document as a list of tuples to <code>doc_topics</code>.</li>
<li><strong><code>pd.DataFrame(doc_topics)</code></strong>: Converts the list of topic distributions into a Pandas DataFrame.</li>
<li><strong><code>to_csv('doc_topics.csv')</code></strong>: Exports the DataFrame to a CSV file named <code>doc_topics.csv</code> for further analysis.</li>
</ul>
</section>
<section id="structure-of-the-csv-file" class="level4" data-number="16.3.13.4">
<h4 data-number="16.3.13.4" class="anchored" data-anchor-id="structure-of-the-csv-file"><span class="header-section-number">16.3.13.4</span> Structure of the CSV File:</h4>
<p>The CSV file will have the following structure: - Each row represents a document. - Each cell contains a tuple <code>(topic_id, proportion)</code> indicating the proportion of the document assigned to that topic.</p>
<p><strong>Example CSV Output:</strong></p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Document</th>
<th>Topic 0</th>
<th>Topic 1</th>
<th>Topic 2</th>
<th>…</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0</td>
<td>(0, 0.25)</td>
<td>(1, 0.50)</td>
<td>(3, 0.25)</td>
<td>…</td>
</tr>
<tr class="even">
<td>1</td>
<td>(1, 0.60)</td>
<td>(2, 0.40)</td>
<td>NaN</td>
<td>…</td>
</tr>
<tr class="odd">
<td>2</td>
<td>(0, 0.15)</td>
<td>(1, 0.35)</td>
<td>(3, 0.50)</td>
<td>…</td>
</tr>
</tbody>
</table>
</section>
<section id="benefits-of-saving-document-topic-distributions" class="level4" data-number="16.3.13.5">
<h4 data-number="16.3.13.5" class="anchored" data-anchor-id="benefits-of-saving-document-topic-distributions"><span class="header-section-number">16.3.13.5</span> Benefits of Saving Document-Topic Distributions</h4>
<ul>
<li><strong>Further Analysis</strong>: The exported CSV can be used in various data analysis tools or software for deeper examination.</li>
<li><strong>Reporting and Visualization</strong>: The CSV format makes it easy to create visual reports and dashboards.</li>
<li><strong>Integration with Other Applications</strong>: Data stored in CSV format can be used for integrations with business intelligence tools like Tableau or Power BI.</li>
</ul>
<hr>
</section>
</section>
</section>
<section id="python-code-that-calculates-perplexity-and-coherence-scores-for-different-topic-numbers" class="level2" data-number="16.4">
<h2 data-number="16.4" class="anchored" data-anchor-id="python-code-that-calculates-perplexity-and-coherence-scores-for-different-topic-numbers"><span class="header-section-number">16.4</span> Python code that calculates perplexity and coherence scores for different topic numbers</h2>
<p>Below is Python code that calculates perplexity and coherence scores for different topic numbers between specified limits <code>a</code> and <code>b</code>. This code helps identify the topic number with the best scores:</p>
<div class="sourceCode" id="cb41"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> gensim</span>
<span id="cb41-2"><a href="#cb41-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> gensim.models.coherencemodel <span class="im">import</span> CoherenceModel</span>
<span id="cb41-3"><a href="#cb41-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-4"><a href="#cb41-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Function to compute perplexity and coherence for different numbers of topics</span></span>
<span id="cb41-5"><a href="#cb41-5" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> evaluate_topic_models(corpus, dictionary, texts, start, end):</span>
<span id="cb41-6"><a href="#cb41-6" aria-hidden="true" tabindex="-1"></a>    results <span class="op">=</span> []</span>
<span id="cb41-7"><a href="#cb41-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-8"><a href="#cb41-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> k <span class="kw">in</span> <span class="bu">range</span>(start, end <span class="op">+</span> <span class="dv">1</span>):</span>
<span id="cb41-9"><a href="#cb41-9" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"Training LDA model with </span><span class="sc">{</span>k<span class="sc">}</span><span class="ss"> topics..."</span>)</span>
<span id="cb41-10"><a href="#cb41-10" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb41-11"><a href="#cb41-11" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Set alpha and eta to be symmetric for simplicity</span></span>
<span id="cb41-12"><a href="#cb41-12" aria-hidden="true" tabindex="-1"></a>        alpha <span class="op">=</span> <span class="fl">1.0</span> <span class="op">/</span> k</span>
<span id="cb41-13"><a href="#cb41-13" aria-hidden="true" tabindex="-1"></a>        eta <span class="op">=</span> <span class="fl">1.0</span> <span class="op">/</span> k</span>
<span id="cb41-14"><a href="#cb41-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-15"><a href="#cb41-15" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Build LDA model</span></span>
<span id="cb41-16"><a href="#cb41-16" aria-hidden="true" tabindex="-1"></a>        lda_model <span class="op">=</span> gensim.models.LdaMulticore(</span>
<span id="cb41-17"><a href="#cb41-17" aria-hidden="true" tabindex="-1"></a>            corpus<span class="op">=</span>corpus,</span>
<span id="cb41-18"><a href="#cb41-18" aria-hidden="true" tabindex="-1"></a>            id2word<span class="op">=</span>dictionary,</span>
<span id="cb41-19"><a href="#cb41-19" aria-hidden="true" tabindex="-1"></a>            num_topics<span class="op">=</span>k,</span>
<span id="cb41-20"><a href="#cb41-20" aria-hidden="true" tabindex="-1"></a>            random_state<span class="op">=</span><span class="dv">100</span>,</span>
<span id="cb41-21"><a href="#cb41-21" aria-hidden="true" tabindex="-1"></a>            chunksize<span class="op">=</span><span class="dv">100</span>,</span>
<span id="cb41-22"><a href="#cb41-22" aria-hidden="true" tabindex="-1"></a>            passes<span class="op">=</span><span class="dv">10</span>,</span>
<span id="cb41-23"><a href="#cb41-23" aria-hidden="true" tabindex="-1"></a>            alpha<span class="op">=</span>alpha,</span>
<span id="cb41-24"><a href="#cb41-24" aria-hidden="true" tabindex="-1"></a>            eta<span class="op">=</span>eta</span>
<span id="cb41-25"><a href="#cb41-25" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb41-26"><a href="#cb41-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-27"><a href="#cb41-27" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Calculate perplexity</span></span>
<span id="cb41-28"><a href="#cb41-28" aria-hidden="true" tabindex="-1"></a>        perplexity <span class="op">=</span> lda_model.log_perplexity(corpus)</span>
<span id="cb41-29"><a href="#cb41-29" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"Perplexity for </span><span class="sc">{</span>k<span class="sc">}</span><span class="ss"> topics: </span><span class="sc">{</span>perplexity<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb41-30"><a href="#cb41-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-31"><a href="#cb41-31" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Calculate coherence score</span></span>
<span id="cb41-32"><a href="#cb41-32" aria-hidden="true" tabindex="-1"></a>        coherence_model_lda <span class="op">=</span> CoherenceModel(</span>
<span id="cb41-33"><a href="#cb41-33" aria-hidden="true" tabindex="-1"></a>            model<span class="op">=</span>lda_model,</span>
<span id="cb41-34"><a href="#cb41-34" aria-hidden="true" tabindex="-1"></a>            texts<span class="op">=</span>texts,</span>
<span id="cb41-35"><a href="#cb41-35" aria-hidden="true" tabindex="-1"></a>            dictionary<span class="op">=</span>dictionary,</span>
<span id="cb41-36"><a href="#cb41-36" aria-hidden="true" tabindex="-1"></a>            coherence<span class="op">=</span><span class="st">'c_v'</span></span>
<span id="cb41-37"><a href="#cb41-37" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb41-38"><a href="#cb41-38" aria-hidden="true" tabindex="-1"></a>        coherence <span class="op">=</span> coherence_model_lda.get_coherence()</span>
<span id="cb41-39"><a href="#cb41-39" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"Coherence Score for </span><span class="sc">{</span>k<span class="sc">}</span><span class="ss"> topics: </span><span class="sc">{</span>coherence<span class="sc">}</span><span class="ch">\n</span><span class="ss">"</span>)</span>
<span id="cb41-40"><a href="#cb41-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-41"><a href="#cb41-41" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Append results</span></span>
<span id="cb41-42"><a href="#cb41-42" aria-hidden="true" tabindex="-1"></a>        results.append((k, perplexity, coherence))</span>
<span id="cb41-43"><a href="#cb41-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-44"><a href="#cb41-44" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> results</span>
<span id="cb41-45"><a href="#cb41-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-46"><a href="#cb41-46" aria-hidden="true" tabindex="-1"></a><span class="co"># Set the range for the number of topics</span></span>
<span id="cb41-47"><a href="#cb41-47" aria-hidden="true" tabindex="-1"></a>a <span class="op">=</span> <span class="dv">2</span>  <span class="co"># Minimum number of topics</span></span>
<span id="cb41-48"><a href="#cb41-48" aria-hidden="true" tabindex="-1"></a>b <span class="op">=</span> <span class="dv">10</span>  <span class="co"># Maximum number of topics</span></span>
<span id="cb41-49"><a href="#cb41-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-50"><a href="#cb41-50" aria-hidden="true" tabindex="-1"></a><span class="co"># Run the evaluation</span></span>
<span id="cb41-51"><a href="#cb41-51" aria-hidden="true" tabindex="-1"></a>results <span class="op">=</span> evaluate_topic_models(corpus_bgram, dictionary_bgram, bgram_lemmatized, a, b)</span>
<span id="cb41-52"><a href="#cb41-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-53"><a href="#cb41-53" aria-hidden="true" tabindex="-1"></a><span class="co"># Find the best number of topics based on coherence and perplexity</span></span>
<span id="cb41-54"><a href="#cb41-54" aria-hidden="true" tabindex="-1"></a>best_coherence <span class="op">=</span> <span class="bu">max</span>(results, key<span class="op">=</span><span class="kw">lambda</span> x: x[<span class="dv">2</span>])</span>
<span id="cb41-55"><a href="#cb41-55" aria-hidden="true" tabindex="-1"></a>best_perplexity <span class="op">=</span> <span class="bu">min</span>(results, key<span class="op">=</span><span class="kw">lambda</span> x: x[<span class="dv">1</span>])</span>
<span id="cb41-56"><a href="#cb41-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-57"><a href="#cb41-57" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Best coherence score obtained with </span><span class="sc">{</span>best_coherence[<span class="dv">0</span>]<span class="sc">}</span><span class="ss"> topics: Coherence = </span><span class="sc">{</span>best_coherence[<span class="dv">2</span>]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb41-58"><a href="#cb41-58" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Best perplexity obtained with </span><span class="sc">{</span>best_perplexity[<span class="dv">0</span>]<span class="sc">}</span><span class="ss"> topics: Perplexity = </span><span class="sc">{</span>best_perplexity[<span class="dv">1</span>]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb41-59"><a href="#cb41-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-60"><a href="#cb41-60" aria-hidden="true" tabindex="-1"></a><span class="co"># Print all results for comparison</span></span>
<span id="cb41-61"><a href="#cb41-61" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Detailed Results (Topics, Perplexity, Coherence):"</span>)</span>
<span id="cb41-62"><a href="#cb41-62" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> res <span class="kw">in</span> results:</span>
<span id="cb41-63"><a href="#cb41-63" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Topics: </span><span class="sc">{</span>res[<span class="dv">0</span>]<span class="sc">}</span><span class="ss">, Perplexity: </span><span class="sc">{</span>res[<span class="dv">1</span>]<span class="sc">}</span><span class="ss">, Coherence: </span><span class="sc">{</span>res[<span class="dv">2</span>]<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<ul>
<li><strong><code>evaluate_topic_models()</code></strong>: A function that trains LDA models for each number of topics between <code>start</code> and <code>end</code> and calculates both perplexity and coherence scores.</li>
<li><strong>Parameters</strong>:
<ul>
<li><code>corpus</code>: The bag-of-words corpus.</li>
<li><code>dictionary</code>: The <code>gensim</code> dictionary object.</li>
<li><code>texts</code>: The tokenized, preprocessed texts.</li>
<li><code>start</code>, <code>end</code>: The range for the number of topics (<code>k</code>).</li>
</ul></li>
<li><strong>Output</strong>:
<ul>
<li>A list of tuples containing <code>(number of topics, perplexity, coherence score)</code>.</li>
<li>The best number of topics based on maximum coherence and minimum perplexity is printed.</li>
</ul></li>
<li><strong>Key Metrics</strong>:
<ul>
<li><strong>Perplexity</strong>: Lower values are better.</li>
<li><strong>Coherence</strong>: Higher values are better.</li>
</ul></li>
</ul>
<p>This code allows you to identify the topic number that optimizes both perplexity and coherence for your dataset.</p>
<p><strong>Example Output:</strong></p>
<p><img src="images/clipboard-2285204010.png" class="img-fluid"></p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./15.SNA_Example.html" class="pagination-link" aria-label="Semantic Network Analysis (Examples)">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Semantic Network Analysis (Examples)</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./17.Topic_Modeling_Example.html" class="pagination-link" aria-label="Topic Modeling with R (Example)">
        <span class="nav-page-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Topic Modeling with R (Example)</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>